** DONE Make "punch-list"
   SCHEDULED: <2020-01-28 Tue>
** TODO follow up on ideas from Clark Bernier
   SCHEDULED: <2020-01-29 Wed>

Nate, since I'm standing at your desk today, I figured it's only fair that I finally get back to you.

The first thing that comes to mind is social identity theory, which focuses on how comparisons with out-groups are central to defining and valuing group identities.  Tajfel and Turner (1986) is the canonical piece; Hogg and Terry (2000) extend it into organizational contexts.  Brubaker and Cooper (2000) place social identity theory in the context of other ways people talk about identity and is a helpful companion to any dive into identity theories.  Though this would require specifying why intensified monitoring and enforcement leads to increased attention to social identities at all.  Perhaps social identity is a tool supporting the monitoring and enforcement regimes you have in mind?  

Another potential mechanism is that intensifying monitoring and enforcement leads to a greater need to coordinate decisions with others.  For instance, enforcement actions with greater sanctioning may increase the importance that any given enforcer believes that their peers will approve of their enforcement decisions.  A moderator may not worry so much about their peers' perceptions when enforcement is limited to rolling back changes, but much moreso when banning someone.  Correll et al. (2017) argue that the need to coordinate decisions will lead people to rely on conventional indicators of quality, as these give the best guess at what their peers are likely to believe.  Whereas they focus on status symbols becoming more important, I could see an argument for in-group/out-group distinctions becoming more relevant under such conditions.  That is, I have stronger enforcement options, so I worry more about what my peers think, so I am more likely to incorporate obvious signals, like group membership, into my decisions because I believe my peers will, too.

Finally, maybe group membership becomes more important because increased monitoring/enforcement gives the impression that group members have already been vetted by trusted others?  Group membership under more stringent regimes might act as a specific status characteristic that signals quality to monitors.  Under this set of mechanisms, it's not the enforcers become biased against out-groups but biased towards in-group members.  I'm not sure how you would distinguish this empirically.  If you can get a copy, Cecilia Ridgeway's new book Status is the best statement of the status theory underlying the argument.  Otherwise, the canonical statement of expectations state theory, and the role played by diffuse and specific status characteristics is Berger et al.'s (1980) review piece.

Hope that helps!  Happy to chat more if it'd be helpful.

Cheers!
-Clark
 

    Berger, Joseph, Susan J. Rosenholtz, and Morris Zelditch. 1980. “Status Organizing Processes.” Annual Review of Sociology 6: 479–508.
    Brubaker, Rogers, and Frederick Cooper. 2000. “Beyond ‘Identity.’” Theory and Society 29 (1): 1–47.
    Correll, Shelley J., Cecilia L. Ridgeway, Ezra W. Zuckerman, Sharon Jank, Sara Jordan-Bloch, and Sandra Nakagawa. 2017. “It’s the Conventional Thought That Counts: How Third-Order Inference Produces Status Advantage.” American Sociological Review, 0003122417691503.
    Hogg, Michael A., and Deborah J. Terry. 2000. “Social Identity and Self-Categorization Processes in Organizational Contexts.” The Academy of Management Review 25 (1): 121–40. https://doi.org/10.2307/259266.
    Tajfel, Henri, and John C Turner. 1986. “The Social Identity Theory of Intergroup Behavior.” In Psychology of Intergroup Relations, edited by Stephen Worchel and William G. Austin. Chicago: Nelson-Hall.

** DONE look at Morey's paper on the fallacy of confidence intervals <- ** TODO Report the sample size by Wiki for each of our models.
** TODO look again for citations suggesting "nudging" may lead to errors
   SCHEDULED: <2020-01-29 Wed>
** TODO is there an alternative to "identity based" as a category of signal like registration status. 
   SCHEDULED: <2020-01-30 Thu>
If we argue that "identity" matters because of in-group membership (i.e. being Wikipedian) then this may not be so important

** TODO improve high level takeaways for designers / builders
   SCHEDULED: <2020-02-10 Mon>

** TODO improve citations to types of regulation / governance systems
   SCHEDULED: <2020-01-31 Fri>

** TODO use social identity theory to argue that measures like anonymity and having a user page constitute "identity based signals"

** TODO reog background to introduce concepts in as intuitive and compact manner as possible
   SCHEDULED: <2020-02-04 Tue>

** TODO report summary statistics, dates RCfilters was introduced for each wiki. 
   SCHEDULED: <2020-02-06 Thu>

** TODO make perma.cc links
   SCHEDULED: <2020-04-13 Mon>
** TODO do placebo tests
   SCHEDULED: <2020-02-04 Tue>
** TODO consider other kinds of hypothesis tests based on adoption check.
   SCHEDULED: <2020-02-05 Wed>
e.g. if only "very likely bad" is significantly adopted, then we can just test our hypotheses by comparing coefficients at that threshold.

** TODO Find another solution to the problem of low N in H2 and H3 
   SCHEDULED: <2020-01-29 Wed>
   - is there another measure we can use for controversial sanctioning?
   - is there another measure we can use for has user page?
   - maybe raising the 20k strata limit will help. 
   - also double check for bugs. 

** TODO rewrite abstract with better transition and results
   SCHEDULED: <2020-02-03 Mon>

** TODO use social identity theory to argue for taste-based discrimination on the basis of group membership as a plausible intuition for null findings
   SCHEDULED: <2020-01-30 Thu>

** TODO proofread bibliography

** DONE draft limitations section 
   SCHEDULED: <2020-01-07 Tue>
** Knit remaining pieces of data in data section (date ranges, sample sizes)
   SCHEDULED: <2020-01-29 Wed>
** Write discussion and results sections
   SCHEDULED: <2020-02-03 Mon>
** Why do "likely bad" flags have a negative effect?
   SCHEDULED: <2020-01-29 Wed>
** Fix rounding of x axis in plots
   SCHEDULED: <2020-01-29 Wed>
** DONE Increase the sample size so we have more non-reverted edits around the very likely damaging threhsold.
   SCHEDULED: <2020-01-28 Tue>

   SCHEDULED: <2020-01-28 Tue>
** TODO Re-run archaeologist until missingness rate is low, alternatively, debug the missing data issue. 
   SCHEDULED: <2020-01-29 Wed>
** TODO Fit models with shorter bandwidth
   SCHEDULED: <2020-01-29 Wed>
** TODO Run analysis on more targeted measures of reversion. 
   SCHEDULED: <2020-01-29 Wed>

** WONT DO Use use the same date range for all wikis and exclude those without data for the entire range?
   SCHEDULED: <2020-01-29 Wed>
I don't think this matters very much. We're just trying to get the broadest sample possible.

** Ask Halfak if we can log the scores DB automatically since this lets us stratify by score which is more convenient.
   SCHEDULED: <2020-01-29 Wed>



** DONE Consider measuring warnings as sanctions.
   SCHEDULED: <2019-12-28 Sat>
** DONE cite Grimmelmann virtues of moderation for definition of moderation
   SCHEDULED: <2020-01-06 Mon>
** DONE cite nora's chi paper on anonymity
   SCHEDULED: <2020-01-06 Mon>
** TODO cite haiyi's work on algorithms.
   SCHEDULED: <2020-01-29 Wed>
** TODO cite any other CSCW about algorithms.
   SCHEDULED: <2020-01-29 Wed>
** TODO figure out my subjective / normative take on this. is this a good thing or a bad thing?

** TODO Why would we show algorithmic flags and identity-based signals in the same interface?
   SCHEDULED: <2020-01-29 Wed>
** TODO think of a better term than "conservative" or "liberal" to describe strictness of moderation.
** TODO build more intuition that moderation actions can be in error / controversial and why this is bad.
   SCHEDULED: <2020-01-30 Thu>
** TODO cite all the papers about the importance of studying Wikipedia in many languages. then we can cite the reading time paper maybe.
   SCHEDULED: <2020-01-29 Wed>
** TODO build argument that moderation is fast paced and stressful more to help with the above, it's as easy as citing Sarah Roberts and Seering more.
   SCHEDULED: <2020-01-30 Thu>
** TODO find someone to cite for salient signals in cscw
** TODO Emphasize visibility  and monitoring as a useful concept for thinking about governance.  Visibility and salient signals are two different mechanisms that our two hypotheses try to tease apart.
** DONE Define flagging.

** TODO Make it clear what our results demonstrate directly and indirectly. 
** DONE Email Bo Cowgill and ask for updates. I drafted an email in outlook. Send on Friday.
** TODO Create table of strata sample sizes and weights for the appendix.
** DONE Get scores from https://quarry.wmflabs.org/query/40712 if the missing data is bad.
   SCHEDULED: <2020-01-08 Wed>
** TODO add controversial revert varaiable to dataset.
** DONE update data set with scores from quarry and reverted-reverts.
   SCHEDULED: <2020-01-02 Thu>
** DONE compare re-scored missingness to old missingness
   SCHEDULED: <2020-01-02 Thu>
** TODO run revscoring with feature injection (don't do it for now)
** TODO robustness check in the local linear regression where we include wikis with the live site issue.
** DONE simplify wiki_weeks generation
   SCHEDULED: <2019-12-03 Tue>
** DONE Check revscoring results and thresholds
   SCHEDULED: <2019-12-03 Tue>
** DONE Add scores to sample with the revert in 24 var
   SCHEDULED: <2019-11-27 Wed>
* DONE Try a less restricted time series model: see if a long-run spline and a short-run spline (or a lagged dv) are stationary according to the Breush-Godfrey test. (Do this after i'm done with other things I can do first while the stan models run)S
  SCHEDULED: <2019-12-07 Sat>

for week of year with fixed effects for month instead of fixed effects for week.
** DONE [#B] Score huge sample
   SCHEDULED: <2019-12-16 Mon>
** DONE model selection for panel models of different spline degrees of freedom using LOO
   SCHEDULED: <2019-11-23 Sat>
** DONE rerun panel models (also using using p_reverted)
   SCHEDULED: <2019-11-23 Sat>
** DONE [#A] Collect thresholds for each deployment
** TODO Use latest model for scoring when we are pre-cutoff
** DONE convert to cscw template
   SCHEDULED: <2019-12-31 Tue>
** TODO fix missing data in revscoring (deleted revisions, zhwiki, this is fucking up the weights!)
** DONE [#A] knit bias analysis
   SCHEDULED: <2020-01-09 Thu>
** DONE run bias analysis on static model version.
   SCHEDULED: <2019-12-07 Sat>
   This actually isn't that important and we probably don't have to do it unless reviewers ask. 
   It's probably enough to keep it up to date with the new wikis. 
   Also, it's a bit of a hassle.
** DONE backup joal's wikidata snapshot (at least for the records that I use). 
** DONE [#B] plot model proto wiki
   SCHEDULED: <2020-01-09 Thu>
** DONE [#B] create pooled bias analysis
   SCHEDULED: <2019-12-11 Wed>
** DONE integrate bias analysis with main repo
** DONE label rdd reverts only if they are damaging
   SCHEDULED: <2019-11-27 Wed>
** TODO Run analysis using a makefile
   SCHEDULED: <2020-02-10 Mon>
** DONE create dependent variable p_reverted (prortion of anon/newcomer edits that reverted)
** TODO [#B] Inspect the messagewalls-style models
   SCHEDULED: <2020-01-29 Wed>
** DONE [#B] RDD data points using data from multiple wikis (get the N big enough to convince mako :) ?)
   SCHEDULED: <2019-11-21 Thu>
** DONE Prep for CGSA meeting (reply to Salt's email)
   SCHEDULED: <2019-11-20 Wed>
** DONE [#A] run revscoring on new sample.
   SCHEDULED: <2019-11-19 Tue>
** DONE [#A] regenerate wikiweeks
   SCHEDULED: <2019-11-19 Tue>
** DONE make a new outline
   SCHEDULED: <2019-11-15 Fri> DEADLINE: <2019-11-13 Wed>

** DONE make it so I never have to run revscoring again
   SCHEDULED: <2019-11-18 Mon>
** DONE regenerate the commit cutoff db to include euwiki
   SCHEDULED: <2019-11-16 Sat>
** DONE [#A] Model anon and newcomers seperately. 
** DONE Drop wikis without enough observations. 
   SCHEDULED: <2019-11-18 Mon>
** TODO [#A] Submit to CSCW
   SCHEDULED: <2020-04-13 Mon> DEADLINE: <2020-04-15 Wed>
** DONE [#A] Model with estimates for average wiki
   SCHEDULED: <2019-11-18 Mon>
   This is somewhat fraught. Seems like between wiki-heterogeneity makes it difficult to estiamte a pooling effect. 
   So let's hold off on that and either present an average-edit model or seperate models for each wiki. But which?

   What's the right way to do this? Have equal sized samples from each wiki and don't weight. 
** DONE [#B] assign thresholds to edits! (there seems to be a bug in getting defaults
   
   SCHEDULED: <2019-11-27 Wed>
** TODO Score pre-treatment edits using latest model versions (instead of earliest model versions)
** WONT DO [#C] Model pooling estimates across thresholds
   SCHEDULED: <2020-01-28 Tue>
** TODO [#A] Robustness Check: Run on a sample of much earlier edits and different cutoffs.
   SCHEDULED: <2020-01-29 Wed>
** TODO [#C] RDD: Plot density conditional on outcomes to test for control over assignment.
   SCHEDULED: <2020-01-29 Wed>
** DONE Compare models using LOO or LRT
** DONE [#A] Investigate spikes in wiki-weeks data. 
   SCHEDULED: <2019-11-22 Fri>
   I didn't find a good explanation, but I noticed that I wasn't removing bots. Also we should model p.reverted instead of n.reverted. I'll try again later.
** TODO [#C] Try fitting models using MLE
   We don't need to do this since we'll want to compare estimates and so have a need for bayes.
** TODO [#A] Fit time series models with splines for time and loo-based model selection.
** DONE [#A] Visualize reversion rates in buckets. 
   SCHEDULED: <2019-11-19 Tue>
** DONE [#A] Debug newcomer panel data model.
   SCHEDULED: <2019-11-21 Thu>
   probably should be fitting binomial models predicting proportion reverted instead
   it fits ok when we don't do QR decomposition. 
** TODO make time-series plots with data and model predicted values. 
** DONE [#A] (fit and interpret) time series models for new hypotheses
   SCHEDULED: <2019-11-22 Fri> DEADLINE: <2019-11-23 Sat>
** BLOCKED [#B] Do seperate RDD analyses for each wiki
   SCHEDULED: <2020-01-29 Wed>
** TODO Robustness checks with varying neighborhood sizes.
** DONE Fit RDD models on newcomer and anonymous editors.
   SCHEDULED: <2019-11-19 Tue>
** DONE [#A] Run RDDs
   SCHEDULED: <2020-01-30 Thu>
** DONE Make pretty discontinuity plots for every wiki. 
   SCHEDULED: <2019-11-13 Wed>
** TODO [#C] Model with time to revert as outcome
** DONE figure out best way to model multiple cutoffs (with missing data)
   Maybe it's one cutoff per model but we exclude data on the other sides of the other cutoffs.
   Or we don't. Mako might be helpful with that. 
** DONE (preliminary) threshhold analysis.
** DONE Fit model from litschig_impact_2013
   SCHEDULED: <2019-11-11 Mon>
** DONE Fit per-wiki models. 
** DONE Why did we lose user ids?
   SCHEDULED: <2019-11-03 Sun>
** DONE [#A] Fit kink model to check that funny cutoffs aren't due to mispecification. 
** DONE Make it so I never have to score edits again.
   SCHEDULED: <2019-11-03 Sun>
** DONE move to git-annex from git-lfs
   SCHEDULED: <2019-11-03 Sun>
** DONE rerun archaeologist on new sample
   SCHEDULED: <2019-10-26 Sat>
** DONE make plots for threshhold analysis. 
** DONE fix error handling in archaeologist
** DONE Make new sample
   SCHEDULED: <2019-10-26 Sat>
** DONE fix remaining bug in archaeologist.
** DONE rebase from github to code.communitydata
   SCHEDULED: <2019-10-18 Fri>
** DONE get default cutoffs
   SCHEDULED: <2019-10-30 Wed>
** DONE Send halfak sample of edits around the threshold.
   SCHEDULED: <2019-11-03 Sun>
** DONE fix error handling in archaeologist
** DONE Make new sample
   SCHEDULED: <2019-11-05 Tue>
** DONE rerun archaeologist on new sample
   SCHEDULED: <2019-11-06 Wed>

** DONE fix remaining bug in archaeologist.
** DONE (preliminary) threshhold analysis.
   SCHEDULED: <2019-10-30 Wed>
** DONE make plots for threshhold analysis. 
   SCHEDULED: <2019-10-30 Wed>
** DONE see if they will install git-annex on the wmf machines.
   SCHEDULED: <2019-11-04 Mon>
   Git-annex isn't installed on wmf machines. So I need to ask about it.
   SCHEDULED: <2019-11-03 Sun>
** DONE rebase from github to code.communitydata
   SCHEDULED: <2019-10-18 Fri>

*** Future project
- two different extreme assumptions could be: the same damage gets
  reverted, it takes more work. 2. Stuff doesn't get reverted at all,
  the cost of debiasing is more damage getting through.
- 
** DONE make code for making threshold ME plots
   SCHEDULED: <2019-11-04 Mon>
