{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# conf.getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/srv/home/nathante/ores_bias_project\n"
     ]
    }
   ],
   "source": [
    "cd ores_bias_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timezone\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Window\n",
    "from functools import reduce\n",
    "from pyspark.sql.functions import udf\n",
    "from dateutil import parser\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.addPyFile(\".local/lib/python3.5/site-packages/mwcomments-0.2.0-py3.5.egg\")\n",
    "spark.sparkContext.addPyFile(\".local/lib/python3.5/site-packages/sortedcontainers-2.1.0-py3.5.egg\")\n",
    "spark.sparkContext.addPyFile(\".local/lib/python3.5/site-packages/python_dateutil-2.8.0-py3.5.egg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mwcomments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wtm = mwcomments.WikiToolMap.load_WikiToolMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "broad_wtm = sc.broadcast(wtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "treated_wikis = pd.read_csv(\"ores_bias_data/rcfilters_enabled.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select canonical cutoffs\n",
    "cutoffs = treated_wikis.groupby(treated_wikis.Wiki).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoffs.timestamp = pd.to_datetime(cutoffs.timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoffs = cutoffs.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ok we're ready to fire up spark and make a stratified sample\n",
    "wmhist = spark.read.table(\"wmf.mediawiki_history\")\n",
    "# we only need the latest snapshot\n",
    "wmhist = wmhist.filter(f.col(\"snapshot\") == \"2019-04\")\n",
    "wmhist = wmhist.filter((f.col(\"event_entity\") == \"revision\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "wmhist = wmhist.select(['wiki_db','event_timestamp','event_comment','revision_id','revision_parent_id','revision_text_bytes','revision_text_bytes_diff','revision_text_sha1','revision_is_identity_reverted','revision_first_identity_reverting_revision_id','revision_is_identity_revert','revision_tags','event_user_id','event_user_text','event_user_is_anonymous','event_user_creation_timestamp','event_user_first_edit_timestamp','event_user_revision_count','event_user_seconds_since_previous_revision','page_id','page_title_historical','page_title',                'page_namespace','page_is_redirect','page_is_deleted','page_revision_count','page_seconds_since_previous_revision',\"event_user_groups\",\"event_user_is_bot_by\",\"revision_deleted_parts\",'revision_deleted_parts_are_suppressed'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "treated_wikis = set(treated_wikis.Wiki)\n",
    "\n",
    "wmhist = wmhist.withColumn(\"treated\",f.col(\"wiki_db\").isin(treated_wikis))\n",
    "\n",
    "wmhist = wmhist.filter(f.col(\"treated\") == True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o196.collectToPython.\n: org.apache.spark.SparkException: Job 1 cancelled \n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1602)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1542)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1789)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1772)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1761)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:938)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:297)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3195)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3192)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3254)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3253)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3192)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-1b3aa3f51fab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmissing_configs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwmhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"page_title\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%Undo-summary%\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"page_title\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%Revertpage%\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"page_is_deleted\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wiki_db'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'revision_id'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'page_id'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'page_title'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmissing_configs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"deleted_config_revisions_treated.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark2/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mtoPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1966\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s\\n%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_exception_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1967\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1968\u001b[0;31m             \u001b[0mpdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1969\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1970\u001b[0m             \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark2/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    464\u001b[0m         \"\"\"\n\u001b[1;32m    465\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark2/python/lib/py4j-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark2/python/lib/py4j-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o196.collectToPython.\n: org.apache.spark.SparkException: Job 1 cancelled \n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1602)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1542)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1789)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1772)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1761)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:938)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:297)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3195)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3192)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3254)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3253)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3192)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "missing_configs = wmhist.filter( (f.col(\"page_title\").like(\"%Undo-summary%\")) | (f.col(\"page_title\").like(\"%Revertpage%\"))).filter(f.col(\"page_is_deleted\")).select('wiki_db','revision_id','page_id','page_title').toPandas()\n",
    "\n",
    "missing_configs.to_csv(\"deleted_config_revisions_treated.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spark_functions import build_wmhist_step1\n",
    "wmhist = build_wmhist_step1(wmhist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spark_functions import process_reverts\n",
    "reverts = process_reverts(wmhist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o273.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 4 times, most recent failure: Lost task 0.3 in stage 3.0 (TID 1963, analytics1073.eqiad.wmnet, executor 27): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/lib/spark2/python/pyspark/worker.py\", line 219, in main\n    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n  File \"/usr/lib/spark2/python/pyspark/worker.py\", line 139, in read_udfs\n    arg_offsets, udf = read_single_udf(pickleSer, infile, eval_type)\n  File \"/usr/lib/spark2/python/pyspark/worker.py\", line 119, in read_single_udf\n    f, return_type = read_command(pickleSer, infile)\n  File \"/usr/lib/spark2/python/pyspark/worker.py\", line 59, in read_command\n    command = serializer._read_with_length(file)\n  File \"/usr/lib/spark2/python/pyspark/serializers.py\", line 170, in _read_with_length\n    return self.loads(obj)\n  File \"/usr/lib/spark2/python/pyspark/serializers.py\", line 559, in loads\n    return pickle.loads(obj, encoding=encoding)\nImportError: No module named 'spark_functions'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:83)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:66)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1126)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1132)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:213)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.writeIteratorToStream(PythonUDFRunner.scala:52)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:215)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1991)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:170)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1602)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1590)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1589)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1589)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1823)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1772)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1761)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:363)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3195)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3192)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3254)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3253)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3192)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/lib/spark2/python/pyspark/worker.py\", line 219, in main\n    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n  File \"/usr/lib/spark2/python/pyspark/worker.py\", line 139, in read_udfs\n    arg_offsets, udf = read_single_udf(pickleSer, infile, eval_type)\n  File \"/usr/lib/spark2/python/pyspark/worker.py\", line 119, in read_single_udf\n    f, return_type = read_command(pickleSer, infile)\n  File \"/usr/lib/spark2/python/pyspark/worker.py\", line 59, in read_command\n    command = serializer._read_with_length(file)\n  File \"/usr/lib/spark2/python/pyspark/serializers.py\", line 170, in _read_with_length\n    return self.loads(obj)\n  File \"/usr/lib/spark2/python/pyspark/serializers.py\", line 559, in loads\n    return pickle.loads(obj, encoding=encoding)\nImportError: No module named 'spark_functions'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:83)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:66)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1126)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1132)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:213)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.writeIteratorToStream(PythonUDFRunner.scala:52)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:215)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1991)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:170)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-85a98481a8f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m                                                                              \u001b[0;34m'revision_tags'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                                                                             \u001b[0;34m'revision_id'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                                                                             'event_timestamp']).limit(30).toPandas()\n\u001b[0m",
      "\u001b[0;32m/usr/lib/spark2/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mtoPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1966\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s\\n%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_exception_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1967\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1968\u001b[0;31m             \u001b[0mpdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1969\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1970\u001b[0m             \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark2/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    464\u001b[0m         \"\"\"\n\u001b[1;32m    465\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark2/python/lib/py4j-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark2/python/lib/py4j-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o273.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 4 times, most recent failure: Lost task 0.3 in stage 3.0 (TID 1963, analytics1073.eqiad.wmnet, executor 27): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/lib/spark2/python/pyspark/worker.py\", line 219, in main\n    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n  File \"/usr/lib/spark2/python/pyspark/worker.py\", line 139, in read_udfs\n    arg_offsets, udf = read_single_udf(pickleSer, infile, eval_type)\n  File \"/usr/lib/spark2/python/pyspark/worker.py\", line 119, in read_single_udf\n    f, return_type = read_command(pickleSer, infile)\n  File \"/usr/lib/spark2/python/pyspark/worker.py\", line 59, in read_command\n    command = serializer._read_with_length(file)\n  File \"/usr/lib/spark2/python/pyspark/serializers.py\", line 170, in _read_with_length\n    return self.loads(obj)\n  File \"/usr/lib/spark2/python/pyspark/serializers.py\", line 559, in loads\n    return pickle.loads(obj, encoding=encoding)\nImportError: No module named 'spark_functions'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:83)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:66)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1126)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1132)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:213)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.writeIteratorToStream(PythonUDFRunner.scala:52)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:215)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1991)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:170)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1602)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1590)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1589)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1589)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1823)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1772)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1761)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:363)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3195)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3192)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3254)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3253)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3192)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/lib/spark2/python/pyspark/worker.py\", line 219, in main\n    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n  File \"/usr/lib/spark2/python/pyspark/worker.py\", line 139, in read_udfs\n    arg_offsets, udf = read_single_udf(pickleSer, infile, eval_type)\n  File \"/usr/lib/spark2/python/pyspark/worker.py\", line 119, in read_single_udf\n    f, return_type = read_command(pickleSer, infile)\n  File \"/usr/lib/spark2/python/pyspark/worker.py\", line 59, in read_command\n    command = serializer._read_with_length(file)\n  File \"/usr/lib/spark2/python/pyspark/serializers.py\", line 170, in _read_with_length\n    return self.loads(obj)\n  File \"/usr/lib/spark2/python/pyspark/serializers.py\", line 559, in loads\n    return pickle.loads(obj, encoding=encoding)\nImportError: No module named 'spark_functions'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:83)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:66)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1126)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1132)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:213)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.writeIteratorToStream(PythonUDFRunner.scala:52)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:215)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1991)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:170)\n"
     ]
    }
   ],
   "source": [
    "testpd = wmhist.filter( (f.col(\"wiki_db\") == 'enwiki') & \n",
    "              (f.array_contains(col=\"revision_tags\",value=\"mw-undo\")) & \n",
    "              (f.array_contains(col=\"comment_match\",value=\"undo\"))).select(['event_comment',\n",
    "                                                                            'comment_match',\n",
    "                                                                             'revision_tags',\n",
    "                                                                            'revision_id',\n",
    "                                                                            'event_timestamp']).limit(30).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "wmhist = wmhist.filter(f.col(\"wiki_db\").isin(list(treated_wikis)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for convenience we'll use the start of the week as the time of intervention and ignore the fact that it was actually mid-week in some cases\n",
    "wmhist = wmhist.withColumn(\"week\",f.date_trunc(\"week\",wmhist.event_timestamp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_time = f.lit(\"2007-01-01\")\n",
    "wmhist = wmhist.filter(f.col('week') > min_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "edits_by_user_week = wmhist.filter(f.col(\"page_namespace\")==0).groupBy(['wiki_db','week','event_user_id']).agg(f.count(wmhist.revision_id).alias(\"wiki_week_counts\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o251.collectToPython.\n: org.apache.spark.SparkException: Job 0 cancelled \n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1602)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1542)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1789)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1772)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1761)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:938)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:297)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3195)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3192)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3254)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3253)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3192)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-e727798f0291>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwikis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwmhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"wiki_db\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistinct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mweeks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwmhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"week\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistinct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark2/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mtoPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1966\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s\\n%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_exception_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1967\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1968\u001b[0;31m             \u001b[0mpdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1969\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1970\u001b[0m             \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark2/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    464\u001b[0m         \"\"\"\n\u001b[1;32m    465\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark2/python/lib/py4j-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark2/python/lib/py4j-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o251.collectToPython.\n: org.apache.spark.SparkException: Job 0 cancelled \n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1602)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1542)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1789)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1772)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1761)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:938)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:297)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3195)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3192)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3254)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3253)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3192)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "wikis = wmhist.select(\"wiki_db\").distinct().toPandas()\n",
    "weeks = wmhist.select(\"week\").distinct().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikis.wiki_db.to_csv(\"all_wikis.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikis = wikis.assign(key=1)\n",
    "weeks = weeks.assign(key=1)\n",
    "wiki_weeks = wikis.merge(weeks,on='key')\n",
    "wiki_weeks = wiki_weeks.drop('key',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_weeks = spark.createDataFrame(wiki_weeks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spot check\n",
    "#wmhist.filter((wmhist.event_user_text == \"AaronSw\") & (wmhist.wiki_db == \"enwiki\")).select(['event_user_id','week','wiki_db','event_user_text']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checks out\n",
    "###edits_by_user_week.filter((edits_by_user_week.event_user_id==20842) & (edits_by_user_week.wiki_db==\"enwiki\")).orderBy(edits_by_user_week.week).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "edits_by_user_week = edits_by_user_week.withColumn(\"week_unix\", f.unix_timestamp(edits_by_user_week.week))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmin_d28_in_s = (28+7)*24*60*60\n",
    "tmin_d7_in_s =  (7)*24*60*60\n",
    "\n",
    "active_window = Window.partitionBy(['wiki_db','event_user_id']).orderBy(['week_unix']).rangeBetween(-1 * tmin_d28_in_s,-1*tmin_d7_in_s)\n",
    "\n",
    "edits_by_user_week = edits_by_user_week.withColumn(\"user_edits_last_month\",f.sum(\"wiki_week_counts\").over(active_window))\n",
    "edits_by_user_week = edits_by_user_week.withColumn(\"is_active\", (~ f.isnull(edits_by_user_week.user_edits_last_month)) &   (edits_by_user_week.user_edits_last_month >= 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spot check\n",
    "#edits_by_user_week.filter((edits_by_user_week.event_user_id==20842) & (edits_by_user_week.wiki_db=='enwiki')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "edits_by_week = edits_by_user_week.groupBy(['wiki_db','week']).agg(\n",
    "    f.sum(\"wiki_week_counts\").alias(\"total_edits\"),\n",
    "    f.sum(edits_by_user_week.is_active.cast(IntegerType())).alias(\"active_editors\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# break reverts out into: undo, huggle, bot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wmhist.filter((f.col(\"wiki_db\")==\"aawiki\") & (f.col(\"week\") == \"2007-11-19 00:00:00\") &( wmhist.revision_is_identity_revert == True)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_weeks = wiki_weeks.fillna(0,[\"mean_user_reverts\",\"n_reverts\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wiki_weeks.filter(f.isnull(f.col(\"user_week_sd\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wmhist.filter(~f.isnull(f.col(\"revision_tags\"))).select(f.min(f.col(\"week\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "wmhist = wmhist.withColumn(\n",
    "    'revert_tool_tag',\n",
    "    f.when(\n",
    "        f.array_contains(f.col(\"revision_tags\"),\"mw-undo\"),'undo').otherwise(\n",
    "        f.when(\n",
    "            f.array_contains(f.col(\"revision_tags\"),\"mw-rollback\"),'rollback').otherwise(                             f.when(f.array_contains(f.col(\"revision_tags\"),'huggle'),'huggle').otherwise(\n",
    "            f.when(f.array_contains(f.col(\"revision_tags\"),\"twinkle\"),\"twinkle\").otherwise(\n",
    "            \"otherTool\")))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'match_comment' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-0ba96fe1d999>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwmhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwmhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"revert_tools_match\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatch_comment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"event_comment\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"wiki_db\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'event_timestamp'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'match_comment' is not defined"
     ]
    }
   ],
   "source": [
    "wmhist = wmhist.withColumn(\"revert_tools_match\", match_comment(f.col(\"event_comment\"),f.col(\"wiki_db\"),f.col('event_timestamp')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"cannot resolve '`revert_tools_match`' given input columns: [mediawiki_history.revision_parent_id, mediawiki_history.revision_deleted_parts_are_suppressed, mediawiki_history.revision_deleted_parts, mediawiki_history.page_seconds_since_previous_revision, mediawiki_history.page_revision_count, mediawiki_history.page_title_historical, mediawiki_history.page_namespace, mediawiki_history.event_user_revision_count, event_user_isbot2, mediawiki_history.wiki_db, event_user_isadmin, mediawiki_history.revision_text_bytes_diff, mediawiki_history.event_user_seconds_since_previous_revision, week, comment_match, role_type, mediawiki_history.page_is_deleted, mediawiki_history.event_user_text, anon_new_established, mediawiki_history.event_user_first_edit_timestamp, is_undo, is_rollback, mediawiki_history.event_timestamp, mediawiki_history.event_user_id, mediawiki_history.revision_id, mediawiki_history.event_user_creation_timestamp, event_user_ispatroller, mediawiki_history.revision_is_identity_revert, mediawiki_history.revision_first_identity_reverting_revision_id, mediawiki_history.page_id, event_user_isbot1, mediawiki_history.page_is_redirect, mediawiki_history.event_user_groups, mediawiki_history.event_comment, event_user_is_newcomer, mediawiki_history.revision_text_sha1, mediawiki_history.revision_is_identity_reverted, mediawiki_history.revision_text_bytes, mediawiki_history.revision_tags, mediawiki_history.event_user_is_bot_by, mediawiki_history.event_user_is_anonymous, mediawiki_history.page_title];;\\n'Project [wiki_db#0, event_timestamp#3, event_comment#4, revision_id#52L, revision_parent_id#53L, revision_text_bytes#57L, revision_text_bytes_diff#58L, revision_text_sha1#59, revision_is_identity_reverted#64, revision_first_identity_reverting_revision_id#65L, revision_is_identity_revert#67, revision_tags#69, event_user_id#5L, event_user_text#7, event_user_is_anonymous#17, event_user_creation_timestamp#19, event_user_first_edit_timestamp#20, event_user_revision_count#21L, event_user_seconds_since_previous_revision#22L, page_id#23L, page_title_historical#24, page_title#25, page_namespace#28, page_is_redirect#30, ... 19 more fields]\\n+- AnalysisBarrier\\n      +- Project [wiki_db#0, event_timestamp#3, event_comment#4, revision_id#52L, revision_parent_id#53L, revision_text_bytes#57L, revision_text_bytes_diff#58L, revision_text_sha1#59, revision_is_identity_reverted#64, revision_first_identity_reverting_revision_id#65L, revision_is_identity_revert#67, revision_tags#69, event_user_id#5L, event_user_text#7, event_user_is_anonymous#17, event_user_creation_timestamp#19, event_user_first_edit_timestamp#20, event_user_revision_count#21L, event_user_seconds_since_previous_revision#22L, page_id#23L, page_title_historical#24, page_title#25, page_namespace#28, page_is_redirect#30, ... 18 more fields]\\n         +- Project [wiki_db#0, event_timestamp#3, event_comment#4, revision_id#52L, revision_parent_id#53L, revision_text_bytes#57L, revision_text_bytes_diff#58L, revision_text_sha1#59, revision_is_identity_reverted#64, revision_first_identity_reverting_revision_id#65L, revision_is_identity_revert#67, revision_tags#69, event_user_id#5L, event_user_text#7, event_user_is_anonymous#17, event_user_creation_timestamp#19, event_user_first_edit_timestamp#20, event_user_revision_count#21L, event_user_seconds_since_previous_revision#22L, page_id#23L, page_title_historical#24, page_title#25, page_namespace#28, page_is_redirect#30, ... 17 more fields]\\n            +- Project [wiki_db#0, event_timestamp#3, event_comment#4, revision_id#52L, revision_parent_id#53L, revision_text_bytes#57L, revision_text_bytes_diff#58L, revision_text_sha1#59, revision_is_identity_reverted#64, revision_first_identity_reverting_revision_id#65L, revision_is_identity_revert#67, revision_tags#69, event_user_id#5L, event_user_text#7, event_user_is_anonymous#17, event_user_creation_timestamp#19, event_user_first_edit_timestamp#20, event_user_revision_count#21L, event_user_seconds_since_previous_revision#22L, page_id#23L, page_title_historical#24, page_title#25, page_namespace#28, page_is_redirect#30, ... 16 more fields]\\n               +- Project [wiki_db#0, event_timestamp#3, event_comment#4, revision_id#52L, revision_parent_id#53L, revision_text_bytes#57L, revision_text_bytes_diff#58L, revision_text_sha1#59, revision_is_identity_reverted#64, revision_first_identity_reverting_revision_id#65L, revision_is_identity_revert#67, revision_tags#69, event_user_id#5L, event_user_text#7, event_user_is_anonymous#17, event_user_creation_timestamp#19, event_user_first_edit_timestamp#20, event_user_revision_count#21L, event_user_seconds_since_previous_revision#22L, page_id#23L, page_title_historical#24, page_title#25, page_namespace#28, page_is_redirect#30, ... 15 more fields]\\n                  +- Project [wiki_db#0, event_timestamp#3, event_comment#4, revision_id#52L, revision_parent_id#53L, revision_text_bytes#57L, revision_text_bytes_diff#58L, revision_text_sha1#59, revision_is_identity_reverted#64, revision_first_identity_reverting_revision_id#65L, revision_is_identity_revert#67, revision_tags#69, event_user_id#5L, event_user_text#7, event_user_is_anonymous#17, event_user_creation_timestamp#19, event_user_first_edit_timestamp#20, event_user_revision_count#21L, event_user_seconds_since_previous_revision#22L, page_id#23L, page_title_historical#24, page_title#25, page_namespace#28, page_is_redirect#30, ... 14 more fields]\\n                     +- Project [wiki_db#0, event_timestamp#3, event_comment#4, revision_id#52L, revision_parent_id#53L, revision_text_bytes#57L, revision_text_bytes_diff#58L, revision_text_sha1#59, revision_is_identity_reverted#64, revision_first_identity_reverting_revision_id#65L, revision_is_identity_revert#67, revision_tags#69, event_user_id#5L, event_user_text#7, event_user_is_anonymous#17, event_user_creation_timestamp#19, event_user_first_edit_timestamp#20, event_user_revision_count#21L, event_user_seconds_since_previous_revision#22L, page_id#23L, page_title_historical#24, page_title#25, page_namespace#28, page_is_redirect#30, ... 13 more fields]\\n                        +- Project [wiki_db#0, event_timestamp#3, event_comment#4, revision_id#52L, revision_parent_id#53L, revision_text_bytes#57L, revision_text_bytes_diff#58L, revision_text_sha1#59, revision_is_identity_reverted#64, revision_first_identity_reverting_revision_id#65L, revision_is_identity_revert#67, revision_tags#69, event_user_id#5L, event_user_text#7, event_user_is_anonymous#17, event_user_creation_timestamp#19, event_user_first_edit_timestamp#20, event_user_revision_count#21L, event_user_seconds_since_previous_revision#22L, page_id#23L, page_title_historical#24, page_title#25, page_namespace#28, page_is_redirect#30, ... 12 more fields]\\n                           +- Project [wiki_db#0, event_timestamp#3, event_comment#4, revision_id#52L, revision_parent_id#53L, revision_text_bytes#57L, revision_text_bytes_diff#58L, revision_text_sha1#59, revision_is_identity_reverted#64, revision_first_identity_reverting_revision_id#65L, revision_is_identity_revert#67, revision_tags#69, event_user_id#5L, event_user_text#7, event_user_is_anonymous#17, event_user_creation_timestamp#19, event_user_first_edit_timestamp#20, event_user_revision_count#21L, event_user_seconds_since_previous_revision#22L, page_id#23L, page_title_historical#24, page_title#25, page_namespace#28, page_is_redirect#30, ... 11 more fields]\\n                              +- Project [wiki_db#0, event_timestamp#3, event_comment#4, revision_id#52L, revision_parent_id#53L, revision_text_bytes#57L, revision_text_bytes_diff#58L, revision_text_sha1#59, revision_is_identity_reverted#64, revision_first_identity_reverting_revision_id#65L, revision_is_identity_revert#67, revision_tags#69, event_user_id#5L, event_user_text#7, event_user_is_anonymous#17, event_user_creation_timestamp#19, event_user_first_edit_timestamp#20, event_user_revision_count#21L, event_user_seconds_since_previous_revision#22L, page_id#23L, page_title_historical#24, page_title#25, page_namespace#28, page_is_redirect#30, ... 10 more fields]\\n                                 +- Project [wiki_db#0, event_timestamp#3, event_comment#4, revision_id#52L, revision_parent_id#53L, revision_text_bytes#57L, revision_text_bytes_diff#58L, revision_text_sha1#59, revision_is_identity_reverted#64, revision_first_identity_reverting_revision_id#65L, revision_is_identity_revert#67, revision_tags#69, event_user_id#5L, event_user_text#7, event_user_is_anonymous#17, event_user_creation_timestamp#19, event_user_first_edit_timestamp#20, event_user_revision_count#21L, event_user_seconds_since_previous_revision#22L, page_id#23L, page_title_historical#24, page_title#25, page_namespace#28, page_is_redirect#30, ... 9 more fields]\\n                                    +- Project [wiki_db#0, event_timestamp#3, event_comment#4, revision_id#52L, revision_parent_id#53L, revision_text_bytes#57L, revision_text_bytes_diff#58L, revision_text_sha1#59, revision_is_identity_reverted#64, revision_first_identity_reverting_revision_id#65L, revision_is_identity_revert#67, revision_tags#69, event_user_id#5L, event_user_text#7, event_user_is_anonymous#17, event_user_creation_timestamp#19, event_user_first_edit_timestamp#20, event_user_revision_count#21L, event_user_seconds_since_previous_revision#22L, page_id#23L, page_title_historical#24, page_title#25, page_namespace#28, page_is_redirect#30, ... 8 more fields]\\n                                       +- Project [wiki_db#0, event_timestamp#3, event_comment#4, revision_id#52L, revision_parent_id#53L, revision_text_bytes#57L, revision_text_bytes_diff#58L, revision_text_sha1#59, revision_is_identity_reverted#64, revision_first_identity_reverting_revision_id#65L, revision_is_identity_revert#67, revision_tags#69, event_user_id#5L, event_user_text#7, event_user_is_anonymous#17, event_user_creation_timestamp#19, event_user_first_edit_timestamp#20, event_user_revision_count#21L, event_user_seconds_since_previous_revision#22L, page_id#23L, page_title_historical#24, page_title#25, page_namespace#28, page_is_redirect#30, ... 7 more fields]\\n                                          +- Filter (event_entity#1 = revision)\\n                                             +- Filter (snapshot#70 = 2019-04)\\n                                                +- SubqueryAlias mediawiki_history\\n                                                   +- Relation[wiki_db#0,event_entity#1,event_type#2,event_timestamp#3,event_comment#4,event_user_id#5L,event_user_text_historical#6,event_user_text#7,event_user_blocks_historical#8,event_user_blocks#9,event_user_groups_historical#10,event_user_groups#11,event_user_is_bot_by_historical#12,event_user_is_bot_by#13,event_user_is_created_by_self#14,event_user_is_created_by_system#15,event_user_is_created_by_peer#16,event_user_is_anonymous#17,event_user_registration_timestamp#18,event_user_creation_timestamp#19,event_user_first_edit_timestamp#20,event_user_revision_count#21L,event_user_seconds_since_previous_revision#22L,page_id#23L,... 47 more fields] parquet\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/lib/spark2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark2/python/lib/py4j-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o312.withColumn.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`revert_tools_match`' given input columns: [mediawiki_history.revision_parent_id, mediawiki_history.revision_deleted_parts_are_suppressed, mediawiki_history.revision_deleted_parts, mediawiki_history.page_seconds_since_previous_revision, mediawiki_history.page_revision_count, mediawiki_history.page_title_historical, mediawiki_history.page_namespace, mediawiki_history.event_user_revision_count, event_user_isbot2, mediawiki_history.wiki_db, event_user_isadmin, mediawiki_history.revision_text_bytes_diff, mediawiki_history.event_user_seconds_since_previous_revision, week, comment_match, role_type, mediawiki_history.page_is_deleted, mediawiki_history.event_user_text, anon_new_established, mediawiki_history.event_user_first_edit_timestamp, is_undo, is_rollback, mediawiki_history.event_timestamp, mediawiki_history.event_user_id, mediawiki_history.revision_id, mediawiki_history.event_user_creation_timestamp, event_user_ispatroller, mediawiki_history.revision_is_identity_revert, mediawiki_history.revision_first_identity_reverting_revision_id, mediawiki_history.page_id, event_user_isbot1, mediawiki_history.page_is_redirect, mediawiki_history.event_user_groups, mediawiki_history.event_comment, event_user_is_newcomer, mediawiki_history.revision_text_sha1, mediawiki_history.revision_is_identity_reverted, mediawiki_history.revision_text_bytes, mediawiki_history.revision_tags, mediawiki_history.event_user_is_bot_by, mediawiki_history.event_user_is_anonymous, mediawiki_history.page_title];;\n'Project [wiki_db#0, event_timestamp#3, event_comment#4, revision_id#52L, revision_parent_id#53L, revision_text_bytes#57L, revision_text_bytes_diff#58L, revision_text_sha1#59, revision_is_identity_reverted#64, revision_first_identity_reverting_revision_id#65L, revision_is_identity_revert#67, revision_tags#69, event_user_id#5L, event_user_text#7, event_user_is_anonymous#17, event_user_creation_timestamp#19, event_user_first_edit_timestamp#20, event_user_revision_count#21L, event_user_seconds_since_previous_revision#22L, page_id#23L, page_title_historical#24, page_title#25, page_namespace#28, page_is_redirect#30, ... 19 more fields]\n+- AnalysisBarrier\n      +- Project [wiki_db#0, event_timestamp#3, event_comment#4, revision_id#52L, revision_parent_id#53L, revision_text_bytes#57L, revision_text_bytes_diff#58L, revision_text_sha1#59, revision_is_identity_reverted#64, revision_first_identity_reverting_revision_id#65L, revision_is_identity_revert#67, revision_tags#69, event_user_id#5L, event_user_text#7, event_user_is_anonymous#17, event_user_creation_timestamp#19, event_user_first_edit_timestamp#20, event_user_revision_count#21L, event_user_seconds_since_previous_revision#22L, page_id#23L, page_title_historical#24, page_title#25, page_namespace#28, page_is_redirect#30, ... 18 more fields]\n         +- Project [wiki_db#0, event_timestamp#3, event_comment#4, revision_id#52L, revision_parent_id#53L, revision_text_bytes#57L, revision_text_bytes_diff#58L, revision_text_sha1#59, revision_is_identity_reverted#64, revision_first_identity_reverting_revision_id#65L, revision_is_identity_revert#67, revision_tags#69, event_user_id#5L, event_user_text#7, event_user_is_anonymous#17, event_user_creation_timestamp#19, event_user_first_edit_timestamp#20, event_user_revision_count#21L, event_user_seconds_since_previous_revision#22L, page_id#23L, page_title_historical#24, page_title#25, page_namespace#28, page_is_redirect#30, ... 17 more fields]\n            +- Project [wiki_db#0, event_timestamp#3, event_comment#4, revision_id#52L, revision_parent_id#53L, revision_text_bytes#57L, revision_text_bytes_diff#58L, revision_text_sha1#59, revision_is_identity_reverted#64, revision_first_identity_reverting_revision_id#65L, revision_is_identity_revert#67, revision_tags#69, event_user_id#5L, event_user_text#7, event_user_is_anonymous#17, event_user_creation_timestamp#19, event_user_first_edit_timestamp#20, event_user_revision_count#21L, event_user_seconds_since_previous_revision#22L, page_id#23L, page_title_historical#24, page_title#25, page_namespace#28, page_is_redirect#30, ... 16 more fields]\n               +- Project [wiki_db#0, event_timestamp#3, event_comment#4, revision_id#52L, revision_parent_id#53L, revision_text_bytes#57L, revision_text_bytes_diff#58L, revision_text_sha1#59, revision_is_identity_reverted#64, revision_first_identity_reverting_revision_id#65L, revision_is_identity_revert#67, revision_tags#69, event_user_id#5L, event_user_text#7, event_user_is_anonymous#17, event_user_creation_timestamp#19, event_user_first_edit_timestamp#20, event_user_revision_count#21L, event_user_seconds_since_previous_revision#22L, page_id#23L, page_title_historical#24, page_title#25, page_namespace#28, page_is_redirect#30, ... 15 more fields]\n                  +- Project [wiki_db#0, event_timestamp#3, event_comment#4, revision_id#52L, revision_parent_id#53L, revision_text_bytes#57L, revision_text_bytes_diff#58L, revision_text_sha1#59, revision_is_identity_reverted#64, revision_first_identity_reverting_revision_id#65L, revision_is_identity_revert#67, revision_tags#69, event_user_id#5L, event_user_text#7, event_user_is_anonymous#17, event_user_creation_timestamp#19, event_user_first_edit_timestamp#20, event_user_revision_count#21L, event_user_seconds_since_previous_revision#22L, page_id#23L, page_title_historical#24, page_title#25, page_namespace#28, page_is_redirect#30, ... 14 more fields]\n                     +- Project [wiki_db#0, event_timestamp#3, event_comment#4, revision_id#52L, revision_parent_id#53L, revision_text_bytes#57L, revision_text_bytes_diff#58L, revision_text_sha1#59, revision_is_identity_reverted#64, revision_first_identity_reverting_revision_id#65L, revision_is_identity_revert#67, revision_tags#69, event_user_id#5L, event_user_text#7, event_user_is_anonymous#17, event_user_creation_timestamp#19, event_user_first_edit_timestamp#20, event_user_revision_count#21L, event_user_seconds_since_previous_revision#22L, page_id#23L, page_title_historical#24, page_title#25, page_namespace#28, page_is_redirect#30, ... 13 more fields]\n                        +- Project [wiki_db#0, event_timestamp#3, event_comment#4, revision_id#52L, revision_parent_id#53L, revision_text_bytes#57L, revision_text_bytes_diff#58L, revision_text_sha1#59, revision_is_identity_reverted#64, revision_first_identity_reverting_revision_id#65L, revision_is_identity_revert#67, revision_tags#69, event_user_id#5L, event_user_text#7, event_user_is_anonymous#17, event_user_creation_timestamp#19, event_user_first_edit_timestamp#20, event_user_revision_count#21L, event_user_seconds_since_previous_revision#22L, page_id#23L, page_title_historical#24, page_title#25, page_namespace#28, page_is_redirect#30, ... 12 more fields]\n                           +- Project [wiki_db#0, event_timestamp#3, event_comment#4, revision_id#52L, revision_parent_id#53L, revision_text_bytes#57L, revision_text_bytes_diff#58L, revision_text_sha1#59, revision_is_identity_reverted#64, revision_first_identity_reverting_revision_id#65L, revision_is_identity_revert#67, revision_tags#69, event_user_id#5L, event_user_text#7, event_user_is_anonymous#17, event_user_creation_timestamp#19, event_user_first_edit_timestamp#20, event_user_revision_count#21L, event_user_seconds_since_previous_revision#22L, page_id#23L, page_title_historical#24, page_title#25, page_namespace#28, page_is_redirect#30, ... 11 more fields]\n                              +- Project [wiki_db#0, event_timestamp#3, event_comment#4, revision_id#52L, revision_parent_id#53L, revision_text_bytes#57L, revision_text_bytes_diff#58L, revision_text_sha1#59, revision_is_identity_reverted#64, revision_first_identity_reverting_revision_id#65L, revision_is_identity_revert#67, revision_tags#69, event_user_id#5L, event_user_text#7, event_user_is_anonymous#17, event_user_creation_timestamp#19, event_user_first_edit_timestamp#20, event_user_revision_count#21L, event_user_seconds_since_previous_revision#22L, page_id#23L, page_title_historical#24, page_title#25, page_namespace#28, page_is_redirect#30, ... 10 more fields]\n                                 +- Project [wiki_db#0, event_timestamp#3, event_comment#4, revision_id#52L, revision_parent_id#53L, revision_text_bytes#57L, revision_text_bytes_diff#58L, revision_text_sha1#59, revision_is_identity_reverted#64, revision_first_identity_reverting_revision_id#65L, revision_is_identity_revert#67, revision_tags#69, event_user_id#5L, event_user_text#7, event_user_is_anonymous#17, event_user_creation_timestamp#19, event_user_first_edit_timestamp#20, event_user_revision_count#21L, event_user_seconds_since_previous_revision#22L, page_id#23L, page_title_historical#24, page_title#25, page_namespace#28, page_is_redirect#30, ... 9 more fields]\n                                    +- Project [wiki_db#0, event_timestamp#3, event_comment#4, revision_id#52L, revision_parent_id#53L, revision_text_bytes#57L, revision_text_bytes_diff#58L, revision_text_sha1#59, revision_is_identity_reverted#64, revision_first_identity_reverting_revision_id#65L, revision_is_identity_revert#67, revision_tags#69, event_user_id#5L, event_user_text#7, event_user_is_anonymous#17, event_user_creation_timestamp#19, event_user_first_edit_timestamp#20, event_user_revision_count#21L, event_user_seconds_since_previous_revision#22L, page_id#23L, page_title_historical#24, page_title#25, page_namespace#28, page_is_redirect#30, ... 8 more fields]\n                                       +- Project [wiki_db#0, event_timestamp#3, event_comment#4, revision_id#52L, revision_parent_id#53L, revision_text_bytes#57L, revision_text_bytes_diff#58L, revision_text_sha1#59, revision_is_identity_reverted#64, revision_first_identity_reverting_revision_id#65L, revision_is_identity_revert#67, revision_tags#69, event_user_id#5L, event_user_text#7, event_user_is_anonymous#17, event_user_creation_timestamp#19, event_user_first_edit_timestamp#20, event_user_revision_count#21L, event_user_seconds_since_previous_revision#22L, page_id#23L, page_title_historical#24, page_title#25, page_namespace#28, page_is_redirect#30, ... 7 more fields]\n                                          +- Filter (event_entity#1 = revision)\n                                             +- Filter (snapshot#70 = 2019-04)\n                                                +- SubqueryAlias mediawiki_history\n                                                   +- Relation[wiki_db#0,event_entity#1,event_type#2,event_timestamp#3,event_comment#4,event_user_id#5L,event_user_text_historical#6,event_user_text#7,event_user_blocks_historical#8,event_user_blocks#9,event_user_groups_historical#10,event_user_groups#11,event_user_is_bot_by_historical#12,event_user_is_bot_by#13,event_user_is_created_by_self#14,event_user_is_created_by_system#15,event_user_is_created_by_peer#16,event_user_is_anonymous#17,event_user_registration_timestamp#18,event_user_creation_timestamp#19,event_user_first_edit_timestamp#20,event_user_revision_count#21L,event_user_seconds_since_previous_revision#22L,page_id#23L,... 47 more fields] parquet\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:88)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:85)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:288)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:286)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4$$anonfun$apply$11.apply(TreeNode.scala:344)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.map(List.scala:285)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:333)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:286)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:286)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:95)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:95)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:107)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:107)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:106)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:118)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$1.apply(QueryPlan.scala:122)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:122)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:127)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:127)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:95)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:85)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:80)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:80)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:92)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:74)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3296)\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1307)\n\tat org.apache.spark.sql.Dataset.withColumns(Dataset.scala:2192)\n\tat org.apache.spark.sql.Dataset.withColumn(Dataset.scala:2159)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-3a731c7e6727>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m             f.array_contains(f.col(\"revert_tools_match\"),\"rollback\"),'rollback').otherwise(                             f.when(f.array_contains(f.col(\"revert_tools_match\"),'huggle'),'huggle').otherwise(\n\u001b[1;32m      7\u001b[0m             f.when(f.array_contains(f.col(\"revert_tools_match\"),\"twinkle\"),\"twinkle\").otherwise(\n\u001b[0;32m----> 8\u001b[0;31m             \"otherTool\")))))\n\u001b[0m",
      "\u001b[0;32m/usr/lib/spark2/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mwithColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \"\"\"\n\u001b[1;32m   1848\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"col should be Column\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1849\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1851\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark2/python/lib/py4j-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"cannot resolve '`revert_tools_match`' given input columns: [mediawiki_history.revision_parent_id, mediawiki_history.revision_deleted_parts_are_suppressed, mediawiki_history.revision_deleted_parts, mediawiki_history.page_seconds_since_previous_revision, mediawiki_history.page_revision_count, mediawiki_history.page_title_historical, mediawiki_history.page_namespace, mediawiki_history.event_user_revision_count, event_user_isbot2, mediawiki_history.wiki_db, event_user_isadmin, mediawiki_history.revision_text_bytes_diff, mediawiki_history.event_user_seconds_since_previous_revision, week, comment_match, role_type, mediawiki_history.page_is_deleted, mediawiki_history.event_user_text, anon_new_established, mediawiki_history.event_user_first_edit_timestamp, is_undo, is_rollback, mediawiki_history.event_timestamp, mediawiki_history.event_user_id, mediawiki_history.revision_id, mediawiki_history.event_user_creation_timestamp, event_user_ispatroller, mediawiki_history.revision_is_identity_revert, mediawiki_history.revision_first_identity_reverting_revision_id, mediawiki_history.page_id, event_user_isbot1, mediawiki_history.page_is_redirect, mediawiki_history.event_user_groups, mediawiki_history.event_comment, event_user_is_newcomer, mediawiki_history.revision_text_sha1, mediawiki_history.revision_is_identity_reverted, mediawiki_history.revision_text_bytes, mediawiki_history.revision_tags, mediawiki_history.event_user_is_bot_by, mediawiki_history.event_user_is_anonymous, mediawiki_history.page_title];;\\n'Project [wiki_db#0, event_timestamp#3, event_comment#4, revision_id#52L, revision_parent_id#53L, revision_text_bytes#57L, revision_text_bytes_diff#58L, revision_text_sha1#59, revision_is_identity_reverted#64, revision_first_identity_reverting_revision_id#65L, revision_is_identity_revert#67, revision_tags#69, event_user_id#5L, event_user_text#7, event_user_is_anonymous#17, event_user_creation_timestamp#19, event_user_first_edit_timestamp#20, event_user_revision_count#21L, event_user_seconds_since_previous_revision#22L, page_id#23L, page_title_historical#24, page_title#25, page_namespace#28, page_is_redirect#30, ... 19 more fields]\\n+- AnalysisBarrier\\n      +- Project [wiki_db#0, event_timestamp#3, event_comment#4, revision_id#52L, revision_parent_id#53L, revision_text_bytes#57L, revision_text_bytes_diff#58L, revision_text_sha1#59, revision_is_identity_reverted#64, revision_first_identity_reverting_revision_id#65L, revision_is_identity_revert#67, revision_tags#69, event_user_id#5L, event_user_text#7, event_user_is_anonymous#17, event_user_creation_timestamp#19, event_user_first_edit_timestamp#20, event_user_revision_count#21L, event_user_seconds_since_previous_revision#22L, page_id#23L, page_title_historical#24, page_title#25, page_namespace#28, page_is_redirect#30, ... 18 more fields]\\n         +- Project [wiki_db#0, event_timestamp#3, event_comment#4, revision_id#52L, revision_parent_id#53L, revision_text_bytes#57L, revision_text_bytes_diff#58L, revision_text_sha1#59, revision_is_identity_reverted#64, revision_first_identity_reverting_revision_id#65L, revision_is_identity_revert#67, revision_tags#69, event_user_id#5L, event_user_text#7, event_user_is_anonymous#17, event_user_creation_timestamp#19, event_user_first_edit_timestamp#20, event_user_revision_count#21L, event_user_seconds_since_previous_revision#22L, page_id#23L, page_title_historical#24, page_title#25, page_namespace#28, page_is_redirect#30, ... 17 more fields]\\n            +- Project [wiki_db#0, event_timestamp#3, event_comment#4, revision_id#52L, revision_parent_id#53L, revision_text_bytes#57L, revision_text_bytes_diff#58L, revision_text_sha1#59, revision_is_identity_reverted#64, revision_first_identity_reverting_revision_id#65L, revision_is_identity_revert#67, revision_tags#69, event_user_id#5L, event_user_text#7, event_user_is_anonymous#17, event_user_creation_timestamp#19, event_user_first_edit_timestamp#20, event_user_revision_count#21L, event_user_seconds_since_previous_revision#22L, page_id#23L, page_title_historical#24, page_title#25, page_namespace#28, page_is_redirect#30, ... 16 more fields]\\n               +- Project [wiki_db#0, event_timestamp#3, event_comment#4, revision_id#52L, revision_parent_id#53L, revision_text_bytes#57L, revision_text_bytes_diff#58L, revision_text_sha1#59, revision_is_identity_reverted#64, revision_first_identity_reverting_revision_id#65L, revision_is_identity_revert#67, revision_tags#69, event_user_id#5L, event_user_text#7, event_user_is_anonymous#17, event_user_creation_timestamp#19, event_user_first_edit_timestamp#20, event_user_revision_count#21L, event_user_seconds_since_previous_revision#22L, page_id#23L, page_title_historical#24, page_title#25, page_namespace#28, page_is_redirect#30, ... 15 more fields]\\n                  +- Project [wiki_db#0, event_timestamp#3, event_comment#4, revision_id#52L, revision_parent_id#53L, revision_text_bytes#57L, revision_text_bytes_diff#58L, revision_text_sha1#59, revision_is_identity_reverted#64, revision_first_identity_reverting_revision_id#65L, revision_is_identity_revert#67, revision_tags#69, event_user_id#5L, event_user_text#7, event_user_is_anonymous#17, event_user_creation_timestamp#19, event_user_first_edit_timestamp#20, event_user_revision_count#21L, event_user_seconds_since_previous_revision#22L, page_id#23L, page_title_historical#24, page_title#25, page_namespace#28, page_is_redirect#30, ... 14 more fields]\\n                     +- Project [wiki_db#0, event_timestamp#3, event_comment#4, revision_id#52L, revision_parent_id#53L, revision_text_bytes#57L, revision_text_bytes_diff#58L, revision_text_sha1#59, revision_is_identity_reverted#64, revision_first_identity_reverting_revision_id#65L, revision_is_identity_revert#67, revision_tags#69, event_user_id#5L, event_user_text#7, event_user_is_anonymous#17, event_user_creation_timestamp#19, event_user_first_edit_timestamp#20, event_user_revision_count#21L, event_user_seconds_since_previous_revision#22L, page_id#23L, page_title_historical#24, page_title#25, page_namespace#28, page_is_redirect#30, ... 13 more fields]\\n                        +- Project [wiki_db#0, event_timestamp#3, event_comment#4, revision_id#52L, revision_parent_id#53L, revision_text_bytes#57L, revision_text_bytes_diff#58L, revision_text_sha1#59, revision_is_identity_reverted#64, revision_first_identity_reverting_revision_id#65L, revision_is_identity_revert#67, revision_tags#69, event_user_id#5L, event_user_text#7, event_user_is_anonymous#17, event_user_creation_timestamp#19, event_user_first_edit_timestamp#20, event_user_revision_count#21L, event_user_seconds_since_previous_revision#22L, page_id#23L, page_title_historical#24, page_title#25, page_namespace#28, page_is_redirect#30, ... 12 more fields]\\n                           +- Project [wiki_db#0, event_timestamp#3, event_comment#4, revision_id#52L, revision_parent_id#53L, revision_text_bytes#57L, revision_text_bytes_diff#58L, revision_text_sha1#59, revision_is_identity_reverted#64, revision_first_identity_reverting_revision_id#65L, revision_is_identity_revert#67, revision_tags#69, event_user_id#5L, event_user_text#7, event_user_is_anonymous#17, event_user_creation_timestamp#19, event_user_first_edit_timestamp#20, event_user_revision_count#21L, event_user_seconds_since_previous_revision#22L, page_id#23L, page_title_historical#24, page_title#25, page_namespace#28, page_is_redirect#30, ... 11 more fields]\\n                              +- Project [wiki_db#0, event_timestamp#3, event_comment#4, revision_id#52L, revision_parent_id#53L, revision_text_bytes#57L, revision_text_bytes_diff#58L, revision_text_sha1#59, revision_is_identity_reverted#64, revision_first_identity_reverting_revision_id#65L, revision_is_identity_revert#67, revision_tags#69, event_user_id#5L, event_user_text#7, event_user_is_anonymous#17, event_user_creation_timestamp#19, event_user_first_edit_timestamp#20, event_user_revision_count#21L, event_user_seconds_since_previous_revision#22L, page_id#23L, page_title_historical#24, page_title#25, page_namespace#28, page_is_redirect#30, ... 10 more fields]\\n                                 +- Project [wiki_db#0, event_timestamp#3, event_comment#4, revision_id#52L, revision_parent_id#53L, revision_text_bytes#57L, revision_text_bytes_diff#58L, revision_text_sha1#59, revision_is_identity_reverted#64, revision_first_identity_reverting_revision_id#65L, revision_is_identity_revert#67, revision_tags#69, event_user_id#5L, event_user_text#7, event_user_is_anonymous#17, event_user_creation_timestamp#19, event_user_first_edit_timestamp#20, event_user_revision_count#21L, event_user_seconds_since_previous_revision#22L, page_id#23L, page_title_historical#24, page_title#25, page_namespace#28, page_is_redirect#30, ... 9 more fields]\\n                                    +- Project [wiki_db#0, event_timestamp#3, event_comment#4, revision_id#52L, revision_parent_id#53L, revision_text_bytes#57L, revision_text_bytes_diff#58L, revision_text_sha1#59, revision_is_identity_reverted#64, revision_first_identity_reverting_revision_id#65L, revision_is_identity_revert#67, revision_tags#69, event_user_id#5L, event_user_text#7, event_user_is_anonymous#17, event_user_creation_timestamp#19, event_user_first_edit_timestamp#20, event_user_revision_count#21L, event_user_seconds_since_previous_revision#22L, page_id#23L, page_title_historical#24, page_title#25, page_namespace#28, page_is_redirect#30, ... 8 more fields]\\n                                       +- Project [wiki_db#0, event_timestamp#3, event_comment#4, revision_id#52L, revision_parent_id#53L, revision_text_bytes#57L, revision_text_bytes_diff#58L, revision_text_sha1#59, revision_is_identity_reverted#64, revision_first_identity_reverting_revision_id#65L, revision_is_identity_revert#67, revision_tags#69, event_user_id#5L, event_user_text#7, event_user_is_anonymous#17, event_user_creation_timestamp#19, event_user_first_edit_timestamp#20, event_user_revision_count#21L, event_user_seconds_since_previous_revision#22L, page_id#23L, page_title_historical#24, page_title#25, page_namespace#28, page_is_redirect#30, ... 7 more fields]\\n                                          +- Filter (event_entity#1 = revision)\\n                                             +- Filter (snapshot#70 = 2019-04)\\n                                                +- SubqueryAlias mediawiki_history\\n                                                   +- Relation[wiki_db#0,event_entity#1,event_type#2,event_timestamp#3,event_comment#4,event_user_id#5L,event_user_text_historical#6,event_user_text#7,event_user_blocks_historical#8,event_user_blocks#9,event_user_groups_historical#10,event_user_groups#11,event_user_is_bot_by_historical#12,event_user_is_bot_by#13,event_user_is_created_by_self#14,event_user_is_created_by_system#15,event_user_is_created_by_peer#16,event_user_is_anonymous#17,event_user_registration_timestamp#18,event_user_creation_timestamp#19,event_user_first_edit_timestamp#20,event_user_revision_count#21L,event_user_seconds_since_previous_revision#22L,page_id#23L,... 47 more fields] parquet\\n\""
     ]
    }
   ],
   "source": [
    "wmhist = wmhist.withColumn(\n",
    "    'revert_tool',\n",
    "    f.when(\n",
    "        f.array_contains(f.col(\"revert_tools_match\"),\"undo\"),'undo').otherwise(\n",
    "        f.when(\n",
    "            f.array_contains(f.col(\"revert_tools_match\"),\"rollback\"),'rollback').otherwise(                             f.when(f.array_contains(f.col(\"revert_tools_match\"),'huggle'),'huggle').otherwise(\n",
    "            f.when(f.array_contains(f.col(\"revert_tools_match\"),\"twinkle\"),\"twinkle\").otherwise(\n",
    "            \"otherTool\")))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we only want to look at the damaging reverts for measuring our vandalism fighting outcomes\n",
    "\n",
    "reverts = reverts.filter(f.col(\"is_damage\") == True)      \n",
    "# exclude reverts with ttr > 30 days = 60 seconds * 60 minutes / second * 24hours / day * 30 days           \n",
    "reverts = reverts.filter(f.col(\"time_to_revert\") <= 30*24*60*60)                                                                                                                                               \n",
    "reverts = reverts.withColumn(\"med_ttr\", f.expr('percentile_approx(time_to_revert, 0.5,1)').over(Window.partitionBy(['wiki_db','week'])))                                                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crap the tags only exist starting in 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wmhist.filter( (f.col(\"revert_tool\")==\"undo\") & f.col(\"wiki_db\") == \"enwiki\").select(f.min(f.col(\"week\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the spike in reverts is real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next lets look at time to revert\n",
    "reverteds = wmhist.filter((wmhist.page_namespace==0) & (wmhist.revision_is_identity_reverted == True)).select(['wiki_db','event_user_text','revision_id','event_timestamp','revision_first_identity_reverting_revision_id','anon_new_established'])\n",
    "\n",
    "reverteds = reverteds.withColumnRenamed(\"event_timestamp\",\"reverted_timestamp\")\n",
    "reverteds = reverteds.withColumnRenamed(\"revision_id\",\"reverted_revision_id\")\n",
    "reverteds = reverteds.withColumnRenamed(\"event_user_text\",\"reverted_user_text\")\n",
    "\n",
    "reverts = wmhist.filter((wmhist.page_namespace==0)&(wmhist.revision_is_identity_revert == True)).select(['wiki_db','week','event_user_text','event_timestamp','role_type','revision_id','revision_is_identity_reverted','revision_first_identity_reverting_revision_id','revert_tool','is_undo','is_rollback'])\n",
    "\n",
    "reverts = reverts.withColumnRenamed(\"event_user_text\",\"revert_user_text\")\n",
    "reverts = reverts.withColumnRenamed(\"event_timestamp\",\"revert_timestamp\")\n",
    "reverts = reverts.withColumnRenamed(\"revision_id\",\"revert_revision_id\")\n",
    "reverts = reverts.withColumnRenamed(\"wiki_db\",\"wiki_db_l\")\n",
    "reverts = reverts.withColumnRenamed(\"revision_is_identity_reverted\",\"revert_is_identity_reverted\")\n",
    "reverts = reverts.withColumnRenamed(\"revision_first_identity_reverting_revision_id\",\"revert_first_identity_reverting_revision_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude self-reverts\n",
    "reverts = reverts.join(reverteds, on=[reverts.wiki_db_l == reverteds.wiki_db, reverts.revert_revision_id == reverteds.revision_first_identity_reverting_revision_id, reverts.revert_user_text != reverteds.reverted_user_text],how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverted_reverts = reverts.filter(f.col('revert_is_identity_reverted')==True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverted_reverts = reverted_reverts.withColumnRenamed(\"revert_timestamp\",\"rr_timestamp\")\n",
    "reverted_reverts = reverted_reverts.withColumnRenamed(\"revert_revision_id\",\"rr_revision_id\")\n",
    "reverted_reverts = reverted_reverts.withColumnRenamed(\"revert_first_identity_reverting_revision_id\",\"rr_reverting_revision_id\")\n",
    "reverted_reverts = reverted_reverts.withColumnRenamed(\"wiki_db\",'rr_wiki_db')\n",
    "reverted_reverts = reverted_reverts.withColumnRenamed(\"week\",'rr_week')\n",
    "reverted_reverts = reverted_reverts.select(['rr_wiki_db','rr_timestamp','rr_revision_id','rr_reverting_revision_id','rr_week'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverts = reverts.join(reverted_reverts, on=[reverts.reverted_revision_id == reverted_reverts.rr_revision_id, reverts.wiki_db == reverted_reverts.rr_wiki_db],how='left_outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverts = reverts.withColumn(\"rr_ttr\", (f.unix_timestamp(reverts.revert_timestamp) - f.unix_timestamp(reverts.rr_timestamp)) / 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[rr_revision_id: bigint, reverted_revision_id: bigint, rr_reverting_revision_id: bigint, revert_revision_id: bigint, wiki_db: string, rr_timestamp: string, revert_timestamp: string, reverted_timestamp: string, rr_ttr: double]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spot check\n",
    "reverts.filter(f.isnull(f.col(\"rr_ttr\")) == False).filter(test_filter).select(['rr_revision_id','reverted_revision_id','rr_reverting_revision_id','revert_revision_id','wiki_db','rr_timestamp','revert_timestamp','reverted_timestamp','rr_ttr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverts = reverts.withColumn(\"is_damage\",f.when(f.isnull(f.col(\"rr_ttr\")), True).otherwise(f.col(\"rr_ttr\") >= 48*60*60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we only want to look at the damaging reverts for measuring our vandalism fighting outcomes\n",
    "reverts = reverts.filter(f.col(\"is_damage\") == True)\n",
    "# convert time to revert into seconds\n",
    "reverts = reverts.withColumn(\"time_to_revert\",(f.unix_timestamp(f.col(\"revert_timestamp\")) - f.unix_timestamp(f.col(\"reverted_timestamp\"))) / 1000)\n",
    "# let's use median ttr as the metric\n",
    "\n",
    "# exclude reverts with ttr > 30 days = 60 seconds * 60 minutes / second * 24hours / day * 30 days\n",
    "reverts = reverts.filter(f.col(\"time_to_revert\") <= 30*24*60*60)\n",
    "\n",
    "reverts = reverts.withColumn(\"med_ttr\", f.expr('percentile_approx(time_to_revert, 0.5,1)').over(Window.partitionBy(['wiki_db','week'])))                                                                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the distribution of reverting activity, we need the counts of reverts by user, wiki, and week\n",
    "reverts_by_user_week = reverts.groupby(['wiki_db',\"revert_user_text\",\"week\"]).agg(f.sum(f.when( (f.col(\"is_damage\") == True),1).otherwise(0)).alias(\"user_week_reverts\"))\n",
    "                                                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = reverts_by_user_week.groupby(['wiki_db','week']).agg(f.sum('user_week_reverts').alias(\"wiki_week_reverts\"),\n",
    "                                                         f.mean('user_week_reverts').alias(\"mean_user_reverts\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverts_by_user_week = reverts_by_user_week.join(gb,on=[\"wiki_db\",\"week\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverts_by_user_week = reverts_by_user_week.withColumn(\"p_reverts\",f.col(\"user_week_reverts\")/f.col(\"wiki_week_reverts\"))\n",
    "\n",
    "reverts_by_user_week = reverts_by_user_week.withColumn(\"p_reverts_sq\",f.col(\"p_reverts\") * f.col(\"p_reverts\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverts_by_user_week = reverts_by_user_week.withColumn(\"user_week_deviance\", reverts_by_user_week.user_week_reverts - reverts_by_user_week.mean_user_reverts)\n",
    "\n",
    "reverts_by_user_week = reverts_by_user_week.withColumn(\"user_week_var_part\", reverts_by_user_week.user_week_deviance * reverts_by_user_week.user_week_deviance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverts_by_week = reverts_by_user_week.groupby([\"wiki_db\",\"week\"]).agg(f.mean(\"user_week_reverts\"). \\\n",
    "                                                                       alias(\"mean_user_reverts\"),\n",
    "                                                                       f.sum(\"user_week_reverts\"). \\\n",
    "                                                                       alias(\"n_reverts\"),\n",
    "                                                                       f.mean(\"user_week_var_part\"). \\\n",
    "                                                                       alias(\"user_week_var\"),\n",
    "                                                                       f.sum(\"p_reverts_sq\"). \\\n",
    "                                                                       alias(\"revert_hhi\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverts_by_week = reverts_by_week.withColumn(\"user_week_sd\",f.sqrt(reverts_by_week.user_week_var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverts_by_week = reverts_by_week.withColumn(\"user_week_revert_cv\",reverts_by_week.user_week_sd / reverts_by_week.mean_user_reverts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_weeks = wiki_weeks.join(reverts_by_week,on=['wiki_db','week'],how='left_outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_weeks = wiki_weeks.join(edits_by_week,on=['wiki_db','week'], how='left_outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# spot check\n",
    "#reverts.filter(f.col(\"is_damage\") == False).filter(f.col(\"wiki_db\")==\"enwiki\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_to_revert_gb = reverts.groupby(['wiki_db','week'])\n",
    "time_to_revert = time_to_revert_gb.agg(\n",
    "    f.mean(f.col(\"time_to_revert\")).alias(\"mean_ttr\"),\n",
    "    f.stddev(f.col(\"time_to_revert\")).alias(\"sd_ttr\"),\n",
    "    f.exp(f.mean(f.log(f.col(\"time_to_revert\")))).alias(\"geom_mean_ttr\"),\n",
    "    f.first(f.col(\"med_ttr\")).alias(\"med_ttr\"),\n",
    "    f.count(f.col(\"week\")).alias(\"N_revert\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_weeks = wiki_weeks.join(time_to_revert,on=['wiki_db','week'],how='left_outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so we see that the count in the main table can be wrong. Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_weeks = wiki_weeks.fillna(0,[\"N_reverteds\",\"active_editors\",\"total_edits\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wiki_weeks.filter(f.isnull(f.col('geom_mean_ttr'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverts = reverts.withColumnRenamed(\"role_type\",\"revert_role_type\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_week_tools = reverts.groupby([\"wiki_db\",\"week\"]).pivot(\"revert_tool\",['undo','rollback','huggle','twinkle','otherTool']).agg(f.mean(f.col(\"time_to_revert\")).alias(\"mean_ttr\"),\n",
    "    f.stddev(f.col(\"time_to_revert\")).alias(\"sd_ttr\"),\n",
    "    f.exp(f.mean(f.log(f.col(\"time_to_revert\")))).alias(\"geom_mean_ttr\"),\n",
    "    f.first(f.col(\"med_ttr\")).alias(\"med_ttr\"),\n",
    "    f.count(\"reverted_revision_id\").alias(\"N_reverts\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wiki_week_roles = reverts.groupby([\"wiki_db\",\"week\"]).pivot(\"revert_role_type\",['admin','bot','other','patroller']).agg(f.mean(f.col(\"time_to_revert\")).alias(\"mean_ttr\"),\n",
    "    f.stddev(f.col(\"time_to_revert\")).alias(\"sd_ttr\"),\n",
    "    f.exp(f.mean(f.log(f.col(\"time_to_revert\")))).alias(\"geom_mean_ttr\"),\n",
    "    f.first(f.col(\"med_ttr\")).alias(\"med_ttr\"),\n",
    "    f.count(\"reverted_revision_id\").alias(\"N_reverts\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_week_editortypes = reverts.groupby([\"wiki_db\",\"week\"]).pivot(\"anon_new_established\",['anonymous','newcomer','established']).agg(f.mean(f.col(\"time_to_revert\")).alias(\"mean_ttr\"),\n",
    "    f.stddev(f.col(\"time_to_revert\")).alias(\"sd_ttr\"),\n",
    "    f.exp(f.mean(f.log(f.col(\"time_to_revert\")))).alias(\"geom_mean_ttr\"),\n",
    "    f.first(f.col(\"med_ttr\")).alias(\"med_ttr\"),\n",
    "    f.count(\"reverted_revision_id\").alias(\"N_reverts\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_weeks = wiki_weeks.join(wiki_week_tools, on=['wiki_db','week'],how='left_outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_weeks = wiki_weeks.join(wiki_week_roles, on=['wiki_db','week'],how='left_outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_weeks = wiki_weeks.join(wiki_week_editortypes, on=['wiki_db','week'],how='left_outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverts = reverts.withColumn('editortype_x_reverttool', f.concat_ws('_',f.col('anon_new_established'),f.col(\"revert_tool\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_values = [ed+'_'+tool for ed in ['anonymous','newcomer','established'] for tool in ['undo','rollback','huggle','twinkle','otherTool']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_week_editortypesxtools = reverts.groupby([\"wiki_db\",\"week\"]).pivot(\"editortype_x_reverttool\",pivot_values).agg(\n",
    "    f.exp(f.mean(f.log(f.col(\"time_to_revert\")))).alias(\"geom_mean_ttr\"),\n",
    "    f.first(f.col(\"med_ttr\")).alias(\"med_ttr\"),\n",
    "    f.count(\"reverted_revision_id\").alias(\"N_reverts\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_weeks = wiki_weeks.join(wiki_week_editortypesxtools, on=['wiki_db','week'],how='left_outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_namespaces = [0,1,2,3,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm_vars = wmhist.withColumn(\"user_new_anon\",f.when(wmhist.event_user_is_anonymous,\"anon\").otherwise(\n",
    "    f.when(wmhist.event_user_is_newcomer,\"newcomer\").otherwise(\"non_anon_newcomer\")))\n",
    "\n",
    "tm_vars = tm_vars.filter(tm_vars.page_namespace.isin(interesting_namespaces)).withColumn(\"user_new_anon_namespace\",f.concat_ws('_',tm_vars.page_namespace,tm_vars.user_new_anon))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm_vars = tm_vars.groupby(['wiki_db','week']) \\\n",
    "                .pivot(\"user_new_anon_namespace\") \\\n",
    "                .agg(f.count(wmhist.revision_id).alias(\"N_edits\"),\n",
    "                     f.countDistinct(wmhist.event_user_text).alias(\"N_editors\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tm_vars.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_weeks = wiki_weeks.join(tm_vars,on=['wiki_db','week'],how='left_outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_weeks = wiki_weeks.withColumn(\"year\",f.year(f.col('week')))\n",
    "wiki_weeks = wiki_weeks.withColumn(\"month\",f.month(f.col('week')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wiki_weeks.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wiki_weeks.write.parquet(\"/user/nathante/ores_bias/wiki_weeks.parquet\",partitionBy=[\"wiki_db\",'year','month'],mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from itertools import chain\n",
    "response = requests.api.get(\"https://meta.wikimedia.org/w/api.php\",{\"action\":\"sitematrix\",\"formatversion\":2,\"format\":\"json\",\"maxage\":3600,\"smaxage\":3600})\n",
    "sitematrix = response.json()[\"sitematrix\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "projname_re = re.compile(r\"https?://(.*).org\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {}\n",
    "rev_mapping = {}\n",
    "for _,v in sitematrix.items():\n",
    "    try: \n",
    "        site = v['site']\n",
    "        dbname = site[0]['dbname']\n",
    "        url = site[0]['url']\n",
    "        projname = projname_re.findall(url)[0]\n",
    "        mapping[projname]=dbname\n",
    "        rev_mapping[dbname]=projname\n",
    "    except Exception as e:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_mapping = f.create_map([f.lit(x) for x in chain(*mapping.items())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "pageviews = spark.read.table(\"wmf.projectview_hourly\")\n",
    "\n",
    "pageviews = pageviews.filter(pageviews.agent_type == 'user').select(['project','year','month','day','hour','view_count'])\n",
    "pageviews = pageviews.withColumn(\"date\", f.concat_ws(\"-\",pageviews.year,pageviews.month,pageviews.day))\n",
    "pageviews = pageviews.groupBy(['project',\"date\"]).agg(f.sum('view_count').alias(\"view_count\"))\n",
    "pageviews = pageviews.withColumn(\"week\", f.date_trunc('week',pageviews.date))\n",
    "pageviews = pageviews.withColumn(\"wiki_db\",spark_mapping.getItem(pageviews.project))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#missing_in_mapping = pageviews.filter(f.isnull(f.col(\"wiki_db\"))).select(['project','wiki_db']).distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_weeks = wiki_weeks.join(pageviews,on=['wiki_db','week'],how='left_outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wiki_weeks.filter(f.col(\"wiki_db\") == \"enwiki\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_views = wiki_weeks.groupBy('wiki_db').agg(f.mean(f.isnull(f.col(\"view_count\")).cast(IntegerType())).alias(\"p_no_views\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing_views = missing_views.filter(f.col(\"p_no_views\") == 1).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "week0 = wmhist.agg(f.min(\"week\")).collect()[0]['min(week)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_history = spark.read.table(\"wmf.mediawiki_page_history\")\n",
    "page_history = page_history.filter(f.col(\"snapshot\") == \"2019-04\")\n",
    "# we only need revisions during our time period\n",
    "# let's assume that pages don't get deleted for now\n",
    "page_history = page_history.withColumn(\"page_creation_week\",f.date_trunc(\"week\",\"page_creation_timestamp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pages_baseline = page_history.filter(\n",
    "    (page_history.page_creation_week < min_time)) \\\n",
    ".groupBy(['wiki_db']) \\\n",
    ".pivot(\"page_namespace\",interesting_namespaces) \\\n",
    ".count().alias(\"n_pages_baseline\")\n",
    "\n",
    "cols = [\"wiki_db\"]\n",
    "cols.extend(\"n_pages_baseline_ns_{0}\".format(i) for i in n_pages_baseline.columns[1:])\n",
    "n_pages_baseline = n_pages_baseline.toDF(* cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_new_pages_by_week = page_history \\\n",
    ".filter((page_history.page_creation_week >= min_time)) \\\n",
    ".groupBy([\"wiki_db\",\"page_creation_week\"]) \\\n",
    ".pivot(\"page_namespace\",interesting_namespaces)\\\n",
    ".agg(f.count(\"page_id\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"wiki_db\",'page_creation_week']\n",
    "cols.extend(\"n_pages_created_ns_{0}\".format(i) for i in n_new_pages_by_week.columns[2:])\n",
    "n_new_pages_by_week = n_new_pages_by_week.toDF(* cols)\n",
    "\n",
    "n_pages = n_new_pages_by_week.join(n_pages_baseline, on=['wiki_db'], how='full_outer')\n",
    "\n",
    "#n_pages = n_pages.withColumn(\"n_pages_ns\",n_pages.n_pages_baseline + n_pages.n_created_pages)\n",
    "n_pages = n_pages.withColumnRenamed('page_creation_week','week')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_weeks = wiki_weeks.join(n_pages,on=['wiki_db','week'],how='full_outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# newcomer survival\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_weeks = wiki_weeks.withColumn(\"year\",f.year(f.col(\"week\")))\n",
    "wiki_weeks = wiki_weeks.withColumn(\"month\",f.month(f.col(\"week\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_wiki_week = wmhist.groupBy('wiki_db','week').agg(f.sum(f.col(\"is_undo\").cast(\"Int\")).alias(\"n_undos\"),\n",
    "                                                          f.sum(f.col(\"is_rollback\").cast(\"Int\")).alias( \"n_rollbacks\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_weeks = wiki_weeks.join(by_wiki_week, on=['wiki_db','week'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[wiki_db: string, p_30_in_week: double]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_weeks.groupby(\"wiki_db\").agg(f.mean( (f.col(\"n_reverts\") > 10).cast(IntegerType()) ).alias(\"p_30_in_week\")).filter(f.col(\"p_30_in_week\") > 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.shuffle.file.buffer', '1mb'),\n",
       " ('spark.stage.maxConsecutiveAttempts', '10'),\n",
       " ('spark.shuffle.service.index.cache.size', '2048'),\n",
       " ('spark.shuffle.service.enabled', 'true'),\n",
       " ('spark.rpc.io.serverThreads', '64'),\n",
       " ('spark.io.compression.lz4.blockSize', '512kb'),\n",
       " ('spark.file.transferTo', 'false'),\n",
       " ('spark.executorEnv.LD_LIBRARY_PATH', '/usr/lib/hadoop/lib/native'),\n",
       " ('spark.unsafe.sorter.spill.reader.buffer.size', '1mb'),\n",
       " ('spark.yarn.archive', 'hdfs:///user/spark/share/lib/spark2-assembly.zip'),\n",
       " ('spark.sql.shuffle.partitions', '400'),\n",
       " ('spark.driver.blockManager.port', '13000'),\n",
       " ('spark.shuffle.unsafe.file.output.buffer', '5mb'),\n",
       " ('spark.app.name', 'pyspark-shell'),\n",
       " ('spark.ui.port', '4040'),\n",
       " ('spark.yarn.executor.memoryOverhead', '6500m'),\n",
       " ('spark.master', 'yarn'),\n",
       " ('spark.sql.catalogImplementation', 'hive'),\n",
       " ('spark.dynamicAllocation.cachedExecutorIdleTimeout', '3600s'),\n",
       " ('spark.yarn.isPython', 'true'),\n",
       " ('spark.dynamicAllocation.executorIdleTimeout', '60s'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.dynamicAllocation.enabled', 'true'),\n",
       " ('spark.driver.port', '12000'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.dynamicAllocation.maxExecutors', '128'),\n",
       " ('spark.port.maxRetries', '100')]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf.getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_weeks_out = wiki_weeks.repartition(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_weeks_out.write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_weeks.select(\"wiki_db\").distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_weeks.filter(f.col(\"wiki_db\")=='enwiki').select(['other_huggle_N_reverts','week']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_weeks.write.parquet(\"/user/nathante/ores_bias/wiki_weeks.parquet\",partitionBy=[\"wiki_db\",\"year\",\"month\"],mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.setCurrentDatabase(\"nathante\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_weeks = spark.read.parquet(\"/user/nathante/ores_bias/wiki_weeks.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os \n",
    "os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-8-openjdk-amd64'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs = pa.hdfs.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hdfs_file = pa.HdfsFile(\"/user/nathante/ores_bias/wiki_weeks.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_weeks_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pq.ParquetDataset(\"/user/nathante/ores_bias/wiki_weeks.parquet\",filesystem=hdfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?pd.read_parquet(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try pulling just the columns you really need. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ww_pdf = dataset.read_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file = hdfs.read_parquet(\"/user/nathante/ores_bias/wiki_weeks.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pq.ParquetDataset(\"/user/nathante/ores_bias/wiki_weeks.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ww_pddf = wiki_weeks_out.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ww_pdf.to_csv(\"ores_bias_data/wiki_weeks2.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark - YARN - 12g",
   "language": "python",
   "name": "spark_yarn_pyspak_nathante_12g"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
