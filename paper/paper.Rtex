\documentclass[format=acmsmall, natbib=true, review=true, screen=true]{acmart}
\citestyle{acmauthoryear}

%\usepackage{cdsc-memoir}        
% \usepackage{pdflscape}          %
% \usepackage{tikz}
% \usetikzlibrary{arrows}
% \usetikzlibrary{positioning}
% \usetikzlibrary{shapes}
%\usepackage{pgfgantt}

% there are two chapter styles: cdsc-article and cdsc-memo

% memo assumes that you remove the "\\" and the email address from the
% \author field below as well as that you will comment out the
% \published tag
%\chapterstyle{cdsc-article}
\usepackage[utf8]{inputenc}
\usepackage{wrapfig}
\usepackage{booktabs}
<<init, echo=FALSE>>=
library(knitr)
knit_hooks$set(document = function(x) {
  sub('\\usepackage[]{color}',
'\\usepackage[]{color}', x, fixed = TRUE)
})
opts_chunk$set(fig.path="figures/knitr-")
opts_chunk$set(dev='pdf')
opts_chunk$set(external=TRUE)
opts_chunk$set(cache=FALSE)
overwrite <- FALSE
source("resources/preamble.R")

@
\usepackage{subcaption}
\usepackage{tikz}
% \usepackage[T1]{fontenc}
% \usepackage{textcomp}
% \usepackage[garamond]{mathdesign}

% \usepackage[letterpaper,left=1.65in,right=1.65in,top=1.3in,bottom=1.2in]{geometry} 

% packages i use in essentially every document
\usepackage{graphicx}
\usepackage{enumerate}

% packages i use in many documents but leave off by default
\usepackage{amsmath, amsthm} %, amssymb}
%\usepackage{caption}
% \usepackage{dcolumn}
% \usepackage{endfloat}

% % import and customize urls
% \usepackage[breaklinks]{hyperref}
% \hypersetup{colorlinks=true, linkcolor=Black, citecolor=Black, filecolor=Blue,
%     urlcolor=Blue, unicode=true}

% list of footnote symbols for \thanks{}
% \makeatletter
% \renewcommand*{\@fnsymbol}[1]{\ensuremath{\ifcase#1\or *\or \dagger\or \ddagger\or
%  \mathsection\or \mathparagraph\or \|\or **\or \dagger\dagger
%   \or \ddagger\ddagger \else\@ctrerr\fi}}% \makeatother
% \newcommand*\samethanks[1][\value{footnote}]{\footnotemark[#1]}

% add bibliographic stuff 
\usepackage[american]{babel}
% \usepackage{csquotes}

%\usepackage[natbib=true, style=apa, backend=biber]{biblatex} 
%\DeclareLanguageMapping{american}{american-apa} 

% \defbibheading{secbib}[\bibname]{%
%   \section*{#1}%
%   \markboth{#1}{#1}%
%   \baselineskip 14.2pt%
%   \prebibhook}

% \def\citepos#1{\citeauthor{#1}'s (\citeyear{#1})}
% \def\citespos#1{\citeauthor{#1}' (\citeyear{#1})}

% memoir function to take out of the space out of the whitespace lists
% \firmlists

%p LATEX NOTE: these lines will import vc stuff after running `make vc` which
% will add version control information to the bottom of each page. This can be
% useful for keeping track of which version of a document somebody has:
% \input{vc}
% \pagestyle{cdsc-page-git}

% LATEX NOTE: this alternative line will just input a timestamp at the
% build process, useful for sharelatex
% \pagestyle{cdsc-page-sharelatex}

\hyphenation{social-psy-cho-lo-gi-cal}
%\newcommand{\oressource}{oresarchaeologist}
\newcommand{\TODO}[1]{{\color{red} TODO: #1}}
\newcommand{\oressource}{\oresdatabase}
\begin{document}

\setlength{\parskip}{4.5pt}
% LATEX NOTE: Ideal linespacing is usually said to be between 120-140% the
% typeface size. So, for 12pt (default in this document, we're looking for
% somewhere between a 14.4-17.4pt \baselineskip.  Single; 1.5 lines; and Double
% in MSWord are equivalent to ~117%, 175%, and 233%.

% \baselineskip 16pt

\title{Algorithmic flagging does not replace identity-based signals in online community moderation}
\author{Nate TeBlunthuis}

\date{\today}

% \published{\textsc{\textcolor{BrickRed}{This document is an
%   unpublished draft.\\ Please do not distribute or cite without
%   permission.}}}

\begin{abstract}
Algorithmic systems for enforcing rules are increasingly adopted by online communities, user generated content platforms, and peer production projects.  The large amount of activity in these spaces leads to a problem of scale due to the great human effort required to review and respond to violations of rules or norms.  Established approaches to community regulation include the use of identity-based signals such as reputation or experience, which moderators use to direct their attention.   Users lacking signals of quality, such as newcomers or unregistered participants, are both likely to cause problems and likely to be sanctioned, but may be ``over-profiled'' if moderators focus on them to the neglect of others. 


  Community moderators increasingly use machine learning algorithms in automated triage systems to help scale up monitoring and enforcement of rules and norms.  These systems may be more accurate than identity-based signals and they make norm violations by all kinds of individuals more visible to moderators.  However, it is not clear how algorithmic and identity-based signals will influence moderation when both are available.  In this study, we explore the effects of the deployment of an algorithmic quality signal into a massive online community.

  % that affords ``statistical discrimination,'' as users with visible signals of negative quality are more likely to be sanctioned. 
  % We have to problematize identity-based signals.  What's wrong with them:
  % Individuals who pass through the filters are hard to monitor (h1)
  % They are inaccurate "salient signals" that can influence evaluation.
  % A more accurate "salient signal" should improve evaluation (h1)

Specifically, we analyze the RCfilters system on Wikipedia which displays algorithmic flags along side identity-based signals in interfaces for reviewing changes to the encyclopedia.  We use the thresholds that trigger flagging to estimate the causal effect of being flagged on sanctioning. \TODO{Put results here} We hypothesize that previous ``over-profiled'' individuals are less sensitive to being flagged because moderators will still scrutinize individuals with visible identity-based signals even when their actions are not flagged.  Similarly, under a hypothesis that identity-based signals and algorithmic flags function as ``salient signals'' that prime moderators to issue sanctions, we predict that flagging actions by ``over-profiled'' individuals will cause an increase in the proportion of sanctions that are controversial, but that this increase will be smaller than the corresponding increase for ``under-profiled'' individuals. 


  % Recently communities and platforms are increasingly adopting automated triage systems that filter or flag  content to support governance work. These systems work by shifting moderator attention toward content and contributions predicted to be damaging. 
%  We consider two mechanisms by which the design and use  of such technologies influences how participants are treated by governance workers.  First, automated triage systems direct attention toward problematic contributions by making them more visible and immediate in user interfaces. Second, these systems may nudge governance workers to see contributions as more dappmaging when they are flagged by an algorithm. 


  % members of a salient class are less sensitive to being flagged and therefore still subject to scrutiny on the  has a disproportionate or 

  % only routes attention or if it also nudges reviewer decisions. 

% Before these models are introduced, change reviewers not using specialized tools had signals about editors available to help them identify problematic edits: for a given edit, they can see whether the editor was logged in or ``anonymous'' (in which case their IP address is shown). inherent such designs has consequences for governance by shaping what contributions are identity reverted.  Such reviewers could also see if an editor's user page and user-talk page exist, which may signal whether an editor is experienced or has been warned.
% Scoring above the thresholds increases the chances an edit will be reverted. 
\end{abstract}

\maketitle

%We believe that understanding the design of algorithmic governance systems requires accounting for how and to what extent the system serves both the surveillence and nudging functions. We analyze an algorithmic triage system in the wild

% Too much emphasis on criminal justice and claiming that as a major part of the contribution is creating extra work. Don't do that unless you really need it.

\section{Introduction}

% Paragraph motivating the question: What are we concerned about why does it matter?% What's the CSCW problem we're working on? It's a popular topic right now. 

% I need to develop / focus my concepts of enforcement or monitoring work. Use ostrom?

% Bring in blackwell and bowker and star and mary douglas more here. Blackwell's important for signaling that this is CSCW work, not FAT*. 
% Salient signals go in the introduction, but the point must be that they signal membership in suspicious categories.  
% broaden the focus beyond online communities to broader questions of algorithmic governance --- including the criminal justice system.
% Bring back surveillence/visibility and profiling in the intro.
% don't make it about online communities!


% Algorithms and the problem of scale
Moderators of online communities and social media platforms review an often large quantity of user generated content and actions to address violations of norms and rules. Upon finding a problematic action, they decide how to respond and whether to sanction the misbehavior.  Due to the ``problem of scale'' moderators may direct their attention according to identity-based signals of individual quality such as reputation, experience, or registration status instead of reviewing every action \cite{gillespie_custodians_2018, kraut_regulating_2012}. Increasingly, communities and platforms adopt algorithmic triage systems to direct moderators toward actions predicted to be problematic \cite{chandrasekharan_crossmod:_2019}. This study adopts a sociotechnical systems perspective by considering not only the design of predictive models and user experience, but also the ``work process'' of moderator users \citep{smith_keeping_nodate}.  


\begin{figure}[t]
  \centering
  \includegraphics[width=\textwidth]{resources/RCfilters_flags.png}
  \caption[Screenshot of edit metadata shown in RCfilters.]{Screenshot of Wikipedia edit metadata on Special:RecentChanges with RCfilters enabled.  Highlighted edits with a colored circle to the left other metadata are flagged by ORES.  Different circle colors (yellow and orange in the figure) correspond to different levels of confidence that the edit is damaging. Users can configure which colors are shown.   Visible identity-based characteristics of editors include registration status (i.e. whether a user name or an IP address is shown) and whether an editor's user page and user talk page exist.  RCfilters does not flag edits by new accounts, but does support filtering changes by newcomers.}
  \label{fig:rcfilters}
\end{figure}

% cite something not about wikipedia in this paragraph?
% Remember that algorithms can be profiling too 
Specifically, we consider how moderators use algorithmic flags alongside other visible signs of quality in the form of user characteristics. Drawing from legal philosopher Frederick Schauer's notion of ``profiling,'' ethicist Paul de Laat argues that characteristics like reputation and registration status become prone to ``overuse'' by moderators who may concentrate their attention on the activities on a narrow range of users \cite{de_laat_profiling_2016, de_laat_use_2015}. Similarly, economists describe ``statistical discrimination'' in which characteristics correlated with performance or deviance are used in making decisions \cite{bertrand_field_2016}.  To simplify language, we say that individuals with ``overused'' characteristics, who face statistical discrimination, are ``over-profiled'' and that other individuals are ``under-profiled.''  Importantly, the visibility of such characteristics to moderators is sufficient for ``over-profiling.'' Including them as predictors in an algorithm is not necessary.
 
De Laat specifically criticizes ``overuse'' of anonymity on Wikipedia because it increases the hazard that anonymous contributors will have their valid contributions rejected. Such barriers to contribution may limit community growth and diversity, as users with vulnerable identities may seek anonymity and blocking contributions from unregistered contributors can decrease positive contributions to peer production projects \cite{hill_hidden_2019,forte_privacy_2017}. % Keep in mind why these systems are valuable to moderators 
That said, overuse of identity-based characteristics such as experience levels, reputation, and registration status is instrumental for moderators to deal with the ``problem of scale'' and efficiently regulate online spaces \cite{gillespie_custodians_2018, de_laat_profiling_2016}.  

Another approach to the ``problem of scale''  is to use systems incorporating \emph{algorithmic triage} by using machine learning predictions to help support moderators sift through vast quantities of content \cite{gillespie_custodians_2018, chandrasekharan_crossmod:_2019}. Can such systems replace reliance on identity-based signals in community and platform governance?  Advocates of algorithmic risk prediction in criminal justice settings argue that algorithmic predictions can improve upon the discriminatory and inaccurate decisions of human judges \cite{kleinberg_discrimination_2019}.  Yet when moderators and judges can still see identity-based signals it is plausible that they will still use them in decision making.  We propose that algorithmic predictions will have less influence on outcomes for ``over-profiled'' individuals compared to ``under-profiled'' ones. In other words, our theory is that flagging an action by an algorithm will cause a greater increase in the likelihood of sanction for ``under-profiled'' individuals.  Similarly, we consider how algorithmic flags and identity-based signals may influence the consistency of sanctioning and theorize that flagging an action will cause an increase in the likelihood that a sanction is unfair, and thus controversial, more for ``under-profiled'' individuals. 

% There are two ways that predictions from an algorithmic governance tool can interact with group membership in monitoring work at scale. First, an algorithmic tool can amplify bias or increase unfairness if its predictions are positively correlated with group membership and both signals are given weight in decision making. Information from the predictive algorithm and group membership will ``add up'' to increase scrutiny on group members when the algorithm predicts their behavior is damaging.  This would occur, for example, in cases where algorithmic predictions can be used to rationalize profiling. 

% On the other hand, it is possible that algorithmic predictions \emph{substitute} for category memberships.  This will happen if the influence of an algorithmic prediction on decision making decreases the influence of a social category. In such cases, merely introducing algorithmic predictions can reduce systematic unfairness (assuming the algorithm is less biased than human decision makers).  

%"cues are created, propogated, and interpreted to become signals"
This paper reports our analysis of moderator behavior in the context of the ORES algorithm for edit quality prediction on Wikipedia and the RCfilters flagging and filtering user-interface that it powers \cite{halfaker_ores:_2019}.
As shown in figure \ref{fig:rcfilters}, this system displays algorithmic predictions alongside visible indicators of membership in salient social categories for reviewing actions on the encyclopedia.  The flags are triggered when ORES' prediction confidence crosses arbitrary operating thresholds. This allows a systematic analysis of edits near to the threshold to provide causal inferences of the effect of algorithmic predictions on moderation decisions. In addition, because algorithmic flags are presented to moderators alongside information about membership in categories associated with objectionable contributions, we can test predictions of our theories about how algorithmic flags will differently effect individuals with or without visible identity-based signals. 

\TODO{Improve high-level takeaways for designers and builders}
We find evidence in support of our hypotheses.  While the system we analyze successfully makes rule and norm violations by ``under-profiled'' editors visible to moderators, it does not altogether eliminate ``over-profiling.''  Flagging an action makes it more likely to be controversially sanctioned, especially for ``under-profiled'' individuals.  Our results suggest that designers of sociotechnical systems for online community and platform governance should consider how moderators may use all available signals as they review user actions and make sanctioning decisions.  Identity-based signals shape moderator actions and algorithmic predictions do not necessarily replace reliance on them.

% % we might have to do some more work to nail this. 
% We find that moderator actions in this context were substantially more sensitive to the algorithmic classification tool when editors were not group members.  This suggests that moderators still used group membership as a signal that contributions may be damaging.  In other words, we reject a hypothesis of pure substitution. However, we also observe a substantial algorithmic influence for members of these classes. This shows that algorithmic predictions can substitute for salient category memberships in surveillance and enforcement work. 

% We contribute to CSCW research on data surveillance and ethics \citep{chancellor_relationships_2019} by elaborating a theory of how algorithmic predictions interact with and salient social categories, providing a non-invasive methodology for identifying substitution between algorithmic predictions and salient social categories in naturalistic settings, which we use in a large-scale empirical test of our theory in a majoprr online platform for cooperative work.

%Are algorithmic tools that support governance work in online communities and social media platforms merely tools for efficient surveillance or do they also shape how content and communication are perceived by regulators?
% Ask the thought provoking questions

% In this paper, we analyze a case where the  %Will their adoption amplify scrutiny on users that resemble troublemakers or deviants or %of user that already %but shape the work that online  %, but will they harm or hurt newcomer retention, discrimination, but how will they  %experience of users who 

% Build the case that we should be suspicious of algorithms. No do that more  in the background section. Here we want to focus on our BIG IDEA. 


\section{Background}

\subsection{Governance in online communities} 

% Why regulate behavior? 
% Comment from overleaf: De Laat might help us make a normative argument, but we can also make it ourselves. 
Regulating behavior is a core task of online communities and social media platforms that requires moderation: ``governance mechanisms that structure participation in a community to facilitate cooperation and prevent abuse'' \cite{grimmelmann_virtues_2015, kraut_regulating_2012}.  Goals of regulation include preventing harassment, eliminating spam, combating misinformation, disinformation, and hateful ideologies,  compliance with requirements of the law or the platforms rules, keeping focus on the topic or purpose of the community, and maintaining the quality of content and outputs \cite{gillespie_custodians_2018}.  There are many possible devices by which communities pursue these goals including reputation systems, collaborative block lists \cite{blackwell_classification_2017}, documenting rules, or creating barriers to entry. 

Moderators are users who take responsibility for moderation work and can be organized in a variety of arrangements. Communities and platforms might have official paid or volunteers moderators and who potentially have special privileges. On the other hand, broadening participation in moderation work can help mobilize more people to contribute to moderation work.  Many platforms allow any user to report content to official moderators by flagging it \cite{crawford_what_2016}.   Sites like Slashdot and Reddit use forms of distributed moderation that aggregate judgements from many users \cite{lampe_crowdsourcing_2014}.  Wikipedia for instance, combines elements of distributed has many different formal roles such as ``administrators'' who can ban users and  ``patrollers'' who can edit more frequently and use some special tools (i.e. Huggle) for reviewing a large number of edits, any and user can contribute to moderation work by reviewing and undoing changes. 

% Governance mechanisms can be classified by whether they are proactive (i.e. systemsp that throttle activity, only publish approved content, or depend on privileges) or reactive (i.e. content is published and then moderated)  \cite{kraut_regulating_2012}.  We focus on reactive systems in this paper. Two common and interdependent reactive patterns in online community governance are sanctioning misbehavior and removing problematic content. 

%Comments from Charlie:
% Groomsman virtues of moderation for definition of moderation
% what kind of moderation are we talking about here? There's volunteer moderation and there is paid moderation. Our theory might apply to both, but we study a context of volunteer moderation where any user can contribute.

% % Define rules / normsg
% \subsubsection{Rules and norms}

\TODO{explain governance problems on Wikipedia in more depth.}
\subsubsection{Sanctions}
% Define norm enforcement

Punishing and deterring bad behavior is a key task for community moderators.  Writing down rules does not imply that they will be followed.  The rules must be enforced through sanctions or acts that discourage misbehavior.  Removing content is a common form of sanctioning that communicates that a contribution was not wanted or appropriate.   \cite{halfaker_dont_2011} study practices of reverting changes on Wikipedia and found that people tend to make higher quality contributions after being reverted. Similarly, \cite{srinivasan_content_2019} found on Reddit that people whose comments were removed became less likely to violate norms. This further suggests that removing content can be an effective form of sanctioning.  

However, sanctioning can also discourage participation, particularly by newcomers. 
New participants are likely to violate rules and norms and therefore be subject to sanctions, but sanctioned newcomers are less likely to continue participating.
This mechanism helps explain declines in Wikipedia participation and in many other online communities, and may be an obstacle to building a community that includes diverse participants  \cite{halfaker_rise_2013, teblunthuis_revisiting_2018, lam_wp:clubhouse?:_2011}.   Efforts to ameliorate this dynamic include better socialization of newcomers to help them learn community rules and norms, but these can be difficult to institute \cite{narayan_wikipedia_2017, morgan_evaluating_2018, halfaker_snuggle:_2014}. 

% Creating barriers that slow participation is a second approach to maintaining order by intentionally limiting growth \cite{kiene_surviving_2016, lin_better_2017}. But in peer production communities like Wikipedia, barriers to growth may also constitute barriers to expanding the quality of diverse knowledge and knowledge-producers \cite{lam_wp:clubhouse?:_2011}. 

%Charlie suggests citing Shagun in the intro.
Sanctions can be controversial when norms are contested or when enforcement is inconsistent or unaccountable \cite{blackwell_classification_2017, crawford_what_2016}.   Improving fairness in sanctioning might help ameliorate the negative effects of sanctioning on community growth. 
\cite{chang_trajectories_2019} study established Wikipedia editors who were  blocked and examined their communications for signs that they believed the block to be fair and found that they were more likely to return to regular participation and not receive another block when they seemed to believe the block was fair.
Approaches to improving the quality of moderation actions include
Slashdot's use of ``meta-moderators'' to review moderation decisions  \cite{lampe_slashdot_2004}. 

% community development as online communities face a dilemma between regulating behavior and attracting participants \citep{teblunthuis_revisiting_2018, halfaker_rise_2013, halfaker_dont_2011}.  

% \cite{halfaker_rise_2013} found that newcomers to Wikipedia were less likely to continue contributing to the encyclopedia after being sanctioned and \cite{teblunthuis_revisiting_2018} replicated this finding in a population of other Wikis.

% This paper contributes to understanding how algorithmic and identity-based signals are related to the fairness of sanctioning by analyze how effects of algorithmic flagging on fair sanctioning differ between groups of users with varying identity-based signals. 

\subsubsection{The problem of scale}
% Problem of scale in norm enforcement
Governance in online communities face a ``problem of scale'' in the challenge of sifting the great mass of comments, posts, or encyclopedia edits to identify objectionable content or behavior \citep{gillespie_custodians_2018}.   Distributed moderation can help communities scale and promote deliberation \citep{lampe_crowdsourcing_2014}, but the high levels of participation in such moderation work required to create orderly spaces can be difficult to sustain \citep{gilbert_widespread_2013}.  With growing attention to problems of disinformation and hate speech online, commercial platforms are expanding their pools of paid human moderators, but the work of paid moderators can be exploitative, difficult, traumatizing, and expensive \cite{roberts_commercial_2016}.  Moderation in contexts that face the problem of scale is likely to be stressful work involving a large number of judgment calls, often ambiguous, that must be made quickly. 

Visibility is an important part of the problem of scale.  For moderators to sanction behavior, they must first observe it. Flagging provides a tool for users to report activities to official moderators and helps platforms defend their enforcement actions. Flagging helps solve the problem of scale because flagged actions are made visible to moderators and thereby directs attention to actions more likely to be problematic.    One disadvantage of flagging is that users can organize strategic to overwhelm moderators or to target opposing viewpoints \cite{crawford_what_2016}. Like other forms of distributed moderation, it depends on the collective efforts of volunteers to review content and report violations.  So community-driven flagging may not be sufficient to detect and sanction a large proportion of offenses. 

Automatic triage systems that use predictions from a machine learning model to flag content may be less prone to strategic flagging, and may provide better coverage of problematic content.  Some systems use algorithms that automatically remove content like the PhotoDNA system which automatically removes child pornography \cite{gillespie_custodians_2018} and ClueBot on Wikipedia which uses a machine learning predictor to automatically remove obvious vandalism \cite{geiger_when_2013}.  However the accuracy of these systems on less clear-cut kinds of misbehavior remains insufficient to solve the problem of scale by automatically removing content \cite{gillespie_custodians_2018}. Furthermore, in user-organized communities, moderation decisions are an important part of building shared meeting, a task not easily left to a fully automated system \cite{seering_moderator_2019}.

% Expand to include other designs of algorithms for detecting norm violations or misbehavior
However, this study focuses on settings where an algorithm might flag content to make it visible to a human who can make an enforcement decision.  Such systems For example, Reddit allows moderators to define a system of rules based on regular expressions to automatically remove or flag content \cite{jhaver_human-machine_2019}. Applied machine learning research endeavors to predict deviant behavior in online communities such as \cite{wulczyn_ex_2017} who automatically classify harassing behavior on Wikipedia and \cite{liu_forecasting_2018} who predict when conversations on Instagram will turn hostile, but provides little guidance for deploying such systems in practice.  In constrast, \cite{chandrasekharan_crossmod:_2019} developed a practicable system for communities on Reddit to share information and collaborate on automatic flagging and account for differences between the rules of different communities.   Similarly, \cite{halfaker_ores:_2019} developed the Object Scoring Evaluation Service (ORES) system, which provides models to predict quality of contributions and content on Wikipedia.


\subsubsection{Sociotechnical evaluation of algorithmic systems}
\TODO{Split this paragraph in two so that we have one paragraph about the need for sociotechnical evaluation and merge the rest with the methods.} 
This project applies regression discontinuity analysis as a non-interventionist method for evaluating systemic effects of algorithmic systems that are deployed and used in consequential settings.  Considerable attention to the ethics and usability of algorithmic systems in the machine learning community aims to provide more transparent or ``fair'' predictors with a focuses on statistical and optimization problems, but from the perspectives of sociotechnical systems and value-sensitive algorithmic design it is important to expand the scope of design and evaluation to consider the user experience and how the introduction and use of new technologies interacts with social structures and shapes work processes \citep{selbst_fairness_2019, zhu_value-sensitive_2018}.  But the difficulty of simulating naturalistic structures and processes raises trade-offs between the ecological and construct validity and research ethics \citep{mcgrath_methods_1984}.  Natural experiments like A/B tests can provide ecological validity and support causal claims, but are ethically troubled as users can be subjected to intervention without consent \citep{lane_big_2015, jouhki_facebooks_2016}.  While even observational studies of social media can raise concerns and violate user's privacy expectations \citep{boyd_critical_2012,fiesler_participant_2018}, In settings such as Wikipedia, studies of editing behavior are generally considered public and open to scrutiny and our method regression discontinuity design can comply with these expectations while supporting causal claims subject to defensible assumptions.  


% Define flagging and filtering
% This section needs more special attention. 
\subsection{Discrimination and filtering}

%Mako says "I think you should start with the bigger theory and explain how it's relevant to Wikipedia (via de Laat, e.g.).  He means starting broad and zooming in.
Profiling based on identity based signals may lead to statistical discrimination on the basis of those signals. Economists of discrimination distinguish between   taste-based and statistical discrimination \citep{bertrand_field_2016}.  Discrimination is when authorities treat deferentially treat individuals based on membership in a group or identity. ``Taste-based'' discrimination is driven by preferences for members of one group or identity including ideological racism and implicit bias. But discrimination can also happen because identity-based signals are instrumental to improving the quality of decisions.  Such ``statistical discrimination'' may still lead to unequal outcomes, but might be justified in cases where differential treatment may be worth the price of expediency  \citep{bertrand_field_2016}. 

%Taste-based and statistical discrimination can be difficult to tell apart in real-world empirical settings, but field experiments can help.  \citep{bertrand_field_2016}. \cite{bertrand_are_2004} conducted an audit study in a labor market. They applied for jobs using resumes of simulated job applicants with either high or low levels of  experience level and either white or black sounding names.  They observed racial discrimination as white applicants were much more likely to receive an interview invitation compared to black applicants.  Now, under a hypothesis of statistical discrimination, additional information about experience levels should reduce reliance on race as a signal of performance, and the gap between white and black applicants should decrease within the group of high quality resumes. However, \cite{bertrand_are_2004} found the opposite --- the gap between white and black sounding applicants was greater in the group of high-quality resumes.  Taste-based discrimination is a plausible mechanism for this finding as more information about applicants amplified rather than decreased the gap as predicted by statistical discrimination, but it is difficult to rule out alternative explanations. 

Proponents of algorithmic governance systems in the legal system argue that such systems can reduce discrimination by replacing reliance on identity-based signals like race with algorithmic predictions that are more accurate than judicial decisions \cite{kleinberg_discrimination_2019}.  However, introducing algorithmic predictions to governance systems does not on its own obscure identity-based signals.  Thus it is important to consider how judges or moderators will use an algorithmic predictor along side identity-based signals in practice.  At one extreme, an algorithm might obviate the usefulness of statistical discrimination if relying on the algorithm is always more effective than looking to an identity-based signal.  On the other hand, introducing algorithmic predictions should be of little consequence to taste-based discrimination. 

% Probably the degree to which algorithms substititute for identity is a function of the quality of the algorithm, how much users trust it, and how much discrimination is taste-based vs statistical. 

% A group is discriminated against when a relevant For example, a judge discriminates against black defendants if they are less likely to be released on bail than apparently identical defendants of a different race. That said, there are multiple mechanisms that may lead to patterns of discrimination. ``Statistical discrimination'' would occur if the reason the judge discriminates is that the judge knows that, all else being equal, black defendants are less likely to appear in court.  In this case the judge is discriminating because doing so advances the judge's goal of carrying out an efficient and orderly judicial process.  However, the judge's discrimination might instead be attributable to ideological racism, or a ``taste'' disfavoring releasing black defendants \citep{bertrand_field_2016}. The distinction between taste-based and statistical discrimination is salient because statistical dissemination might be considered an acceptable form of differential treatment between groups, particularly if historical oppression is not a factor, as in discrimination against newcomers in regulating an online community.  Indeed we think that statistical, but not taste-based discrimination against new and anonymous contributors is likely in online communities.

% consider deleting this paragraph entirely
In the context of Wikipedia, \cite{de_laat_profiling_2016} adopts the concept of ``profiling'' from legal scholar Frederick Schauer to argue that displaying identity-based signals like registration status or experience levels in interfaces for reviewing changes or in algorithmic governance tools on Wikipedia may be unethical or at least inconsistent with Wikipedia's founding principles.  Similar to statistical discrimination, de Laat contends that such signals are prone to ``over-use''  as moderators are much more likely to scrutinize types of contributors who mayp have legitimate reasons for editing anonymously or editing through a new account. We modify de Laat's vocabulary to call such editors ``over-profiled.'' On the other hand other kinds of editors will be ``under-profiled'' as their contributions may be less likely to come under scrutiny.

Assuming that community moderators are using an algorithmic flagging system to find actions that merit sanctioning, when the system flags an action, that will increase the likelihood that a moderator responds with a sanction.  However, the  magnitude of the increase will depend on the answer to the counterfactual question: ``what would have happened if the action had not been flagged?''  We think the answer will be different between over-profiled and under-profiled individuals. Mainly, actions 
of over-profiled individuals are likely to face scrutiny even when not flagged by an algorithm, but actions of under-profiled individuals are only likely to face scrutiny when they are flagged.

We think that statistical discrimination is very likely to occur against new or unregistered participants in online communities with cheap pseudonyms. Cheap pseudonyms make it easy for rule breakers to evade sanctions by creating new accounts \cite{friedman_social_2001}.  Therefore, new accounts are suspect and likely face more scrutiny.  Similarly,  when identity-based signals such as experience level, registration status, or reputation are visible and salient to moderators, such information will lead to over-profiling of editors whose values for such characteristics are associated with misbehavior.

We propose that identity-based signals will still lead to over-profiling even alongside algorithmic flags.  Since actions of over-profiled individuals are likely to be scrutinized even when they are not flagged by an algorithm, we expect algorithmic flagging to play a relatively smaller role in moderation of their actions.  Therefore, when a piece of content is flagged by an algorithm, the increase in the likelihood that a moderator responds with a sanction will be smaller for actions made by over-profiled users compared to under-profiled users. Thus our first hypothesis is:
% ISN'T the absence of a signal a signal of positive quality?

\textbf{H1:} Flagging an action causes a greater increase in the likelihood the action is sanctioned when the action is by an under-profiled individual than when is by an over-profiled individual.
 

Next we consider the relationship between identity-based signals and the consistency of sanctioning.  This means considering not just how moderators direct their attention, but also how they make decisions.  When moderators use aspects of user identity such as account age, registration, experience or reputation are to choose what contributions to review or whether to sanction behavior, these attributes are acting as ``salient signals'': visible signs used in fast decision making.  When people are faced with many choices where the correct decision is uncertain or where finding and analyzing the information necessary to arrive at a correct decision is difficult, they tend to rely on salient signals  \citep{bordalo_salience_2012, kleinberg_human_2018, tversky_judgment_1974}.  

% important term related to salient signal is "cue"
If algorithmic flagging functions as a salient signal that influences moderators making uncertain decisions then moderators may be more likely to issue controversial sanctions against flagged actions. When an action is flagged, a moderator will be suspicious of it and may act conservatively to sanction even if the decision is uncertain. The flag suggests to the moderator that the action is problematic. We hypothesize that the increase in sanctioning caused by flagging an action will also lead to an increase in the proportion of sanctions that are controversial.

\textbf{H2:} Within the set of sanctioned actions, flagging an action causes an increase in the likelihood that it receives a controversial sanction.

Finally, we propose that, as with algorithmic flags, identity-based signals function as salient signals that can lead to controversial sanctioning.  Similar to \textbf{H1}, we hypothesize that using algorithmic flagging alongside identity-based signals will partly, but not entirely, reduce reliance on identity-based signals. Actions by under-profiled individuals will be moderated more conservatively when they are flagged, but more liberally when not flagged, but actions by over-profiled individuals will still be moderated conservatively when not flagged. This implies that the increase in controversial sanctions among flagged actions will be smaller for over-profiled individuals compared to under-profiled individuals.

\textbf{H3:} Within the set of sanctioned actions, flagging an action causes a greater increase in the likelihood that the sanction is controversial when the action is by an under-profiled individuals than when it is by an over-profiled individual.

\section{Data and measures}

%when was it introduced?
The RCfilters system on Wikipedia is a relatively new tool for monitoring changes to Wikipedia (edits). It provides flagging and filtering according to algorithmic triage flags, a limited set of editor characteristics, and other metadata fields. RCfilters stands for ``Recent Changes filters,'' signaling the special page on Wikipedia for observing the latest edits, \footnote{\url{https://en.wikipedia.org/wiki/Special:RecentChanges}} but RCfiters flags and filters are also on users' watchlists, which show edits to pages the user has followed. 
Figure \ref{fig:rcfilters} shows highlighting and flagging in the RCfilters interface.

% 
Algorithmic flagging in the RCfilters system is powered by the ORES edit quality models trained to predict whether edits are labeled ``damaging'' or ``not damaging.'' 
The models are gradient boosted decision trees trained on a mixture of human labeled Wikipedia edits and other edits made by established editors that are assumed to be ``not damaging.''   It is important to note that ORES models do not merely reproduce profiling patterns typical of moderation on Wikipedia.  The interface for labeling training data obscures identity-based signals from the  volunteer Wikipedians doing labeling work and the models are predictive of damage from users that are not anonymous or newcomers. 
For more information on the design and implementation of ORES see \cite{halfaker_ores:_2019}. 

An edit is flagged by RCfilters flags if and only if the continuously valued risk score output by the ORES model exceeds a threshold, formally called an operating point.  RCfilters uses multiple operating points corresponding to green, yellow, orange, and red flags.  By default only orange and red flags are shown, but users can configure which colors to display in edit review tools. Green flags and filters are to help Wikipedia editors find good edits  As we are interested in flagging for the purposes of finding damaging edits we consider them no further.  Red, orange, and yellow correspond to thresholds making different trade-offs between precision (the proportion of flagged edits that are truly damaging) and recall (the proportion of truly damaging edits that are flagged).  Red corresponds to a high precision threshold and edits flagged in are labeled ``very likely damaging.'' Orange flags corresponds to a ``likely damaging'' label with greater recall, but less precision compared to red, and edits with yellow flags are ``maybe damaging'' with a high recall and lower precision.  A special page displays the thresholds and their corresponding levels of precision and recall.  Figure \ref{fig:ores_thresholds} shows this page for English Wikipedia \footnote{\url{https://en.wikipedia.org/wiki/Special:OresModels}}.
  
\begin{figure}[t]
  \centering
\includegraphics[width=0.7\textwidth]{resources/Ores_Thresholds.png}  
  \caption[Screenshot showing RCfilters thresholds for English Wikipedia.]{Screenshot of Special:OresModels from English Wikipedia showing levels of precision and recall corresponding to different flags in RCfilters.}
  \label{fig:ores_thresholds}
\end{figure}

It is not obvious that algorithmic filtering in RCfilters has any substantial influence on Wikipedia governance as algorithmic filtering features are not enabled by default and must be enabled in user preferences.  Therefore, we will present a preliminary analysis that shows that these tools were adopted by demonstrating an overall causal effect of flagging on sanctioning after presenting our methods.  First we will describe our other measures.

\subsection{Sanctioning}

% cite some more stuff that uses reverts and sanctioning.
% Should we mention Twinkle?
\emph{Identity reverts} are our measure of sanctioning.  Identity reverts are a common measure of contribution rejection on Wikipedia, entail undoing an edit to by restoring a page to an earlier state, and are straightforward to measure by comparing hashes of page revisions  \cite{halfaker_dont_2011}.  That said, identity reverts are an imperfect measure of sanctioning.  A type of vandalism called ``blanking'' removes all content on a page and therefore identity reverts all prior edits to the page. It is also possible for an individual to ``self-revert'' by undoing their own edit.  To help mitigate such issues, we only label revisions as \emph{reverted} if they were undone within 30 days and were not undone by self-reverts and we label revisions as \emph{not reverted}  otherwise.
\subsection{Controversial Sanctioning}

We follow \cite{piskorski_testing_2017} by considering identity reverts that are subsequently reverted by a third party as controversial sanctions.  Specifically, we label a sanction as \emph{controversial} if the sanction is undone by a third editor who was not the original editor or the reverting editor.  Such interactions likely correspond to cases in which a third part observes the initial revert, disagrees with the initial sanction, and then acts to reverse the sanction.

\subsection{Identity-based signals}
As shown in figure \ref{fig:rcfilters}, the RCfilters interface includes metadata with two key identity-based signals: whether the editor who made the change was logged into a registered account and whether or not the editor is new enough to have not yet created a "user page".  

\emph{IP editors} are individuals editing Wikipedia without logging in. 
IP editors are individuals who may not have a registered account, or may choose not to log in when making an edit for any reason.  Also called ``anonymous,'' such editors are associated with misbehavior have long had a controversial status on Wikipedia.  \cite{geiger_work_2010} describes how tools for moderators highlighed IP editors and how such edits are often scrutinized, and \cite{de_laat_profiling_2016} described such editor characteristics as prone to ``overuse.''  Online collaboration platforms understand that anonymous users are likely to violate norms and make low quality contributions \cite{mcdonald_privacy_2019}.  Recently, concerns about privacy and vandalism related to the use of IP addresses for edit attribution sparked discussions about alternatives, including proposals to ban anonymous editors from creating pages or even to eliminate anonymous editing entirely.\footnote{see \url{https://meta.wikimedia.org/wiki/Talk:IP_Editing:_Privacy\_Enhancement\_and\_Abuse\_Mitigation}}
% https://en.wikipedia.org/wiki/Wikipedia:Editors_should_be_logged-in_users_(failed_proposal)
% https://en.wikipedia.org/wiki/Wikipedia:Disabling_edits_by_unregistered_users_and_stricter_registration_requirement
% https://en.wikipedia.org/wiki/Wikipedia:IPs_are_human_too

That said, communities such as Wikipedia may wish to allow anonymous contributions due to the benefits anonymity may provide.  Anonymity may help diversify participation as those who face targeted harassment based on their identities are likely to seek anonymity \cite{forte_privacy_2017}. Anonymity may also increase productive contribution by removing the frictions of creating an account or logging in  \cite{mcdonald_privacy_2019}. Wikis on other platforms have disallowed unregistered editing, resulting in a decrease in norm and rule violation, but also a decrease in beneficial contributions \cite{hill_hidden_2019}.

\emph{Newcomers without user pages} are a second class of editor with identity-based signals visible in the metadata in RCfilters.   De Laat uses the existence of a user page as an example of an indicator of vandalism that may be prone to overuse \cite{de_laat_profiling_2016}. User pages are places on Wikipedia for editors to create profiles and it is normal for experienced editors to create their user page, so lacking a user page is a good sign that an editor is inexperienced.   The metadata on edit reviewing interfaces links to the user page of the edit in the text of the editor's name.  For example in figure \ref{fig:rcfilters} edits by users ``Llavoro'' and ``MilovanPa'' are shown and their users names are colored red.  Red links are widely understood to link to wiki pages that do not exist, so seeing a red link account in Recent Changes metadata is a clue that the editor is new.

We identify whether a user's user page exists by matching the titles of user pages against the editor's user name and checking if the creation of the user page was prior to the edit in question.  


\subsection{Data: Wikimedia History}

% 
We build our dataset from two publicly available tables of Wikimedia history maintained by a team of data engineers at the Wikimedia foundation by running spark scripts on the Wikimedia analytics cluster.\footnote{Documented at \url{https://wikitech.wikimedia.org/wiki/Analytics/Data\_Lake/Edits/Mediawiki\_history}} \footnote{see \url{https://dumps.wikimedia.org/other/mediawiki\_history/readme.html}} We also use an internal Wikimedia foundation database that logs the scores from the ores models.\TODO{Can we make this public?}  Although Wikipedia is published and collaborated on in many languages, the vast majority of knowledge about collaboration on Wikipedia is derived from studies of English Wikipedia alone.  Therefore, we aim to be inclusive by analyzing data from all 21 language editions of Wikipedia where edit quality flags are displayed in the RCfilters interface. 

For all of our analyses, our unit of analysis is the \emph{revision}, representing an edit to a page by a participant on Wikipedia.  Since we care about how algorithmic flagging and identity-based signals are used by human moderators, we limit our analysis to actions taken by humans by excluding revisions by bots.  We exclude wikis with less than \Sexpr{min.obs.per.wiki.threshold.cutoff} edits above and below each threshold.  This means that different wikis may be included in different models. For each model we report the quantity of edits from each wiki and how many fall on either side of the thresholds. 

We analyze a stratified sample to allows us to keep the total size of our dataset manageable while providing adequate statistical power from the diversity of Wikis and editor types we wish to analyze. We stratify by \emph{wiki}, by whether the editor is an IP editor or not, by whether the editor has a user page or not, by whether an edit was reverted in 2 hours, 48 hours, or 30 days, and by whether the revert was controversial. Most Wikipedia edits comply with norms, and accordingly the ORES scores are left-skewed, therefore we also stratify our sample by the decile of the ORES scores. We sample up to \Sexpr{strata_sample_size} edits from within each strata.  Stratified sampling introduces a known bias in our sample and we correct for this bias using sample weights throughout our analysis.  RCfilters powered by ORES damaging models were introduced to different wikis at different times, but we wish to estimate the average effect for edits to any of the wikis in our sample.  Therefore, while we must sample only edits following the introduction of ORES, we weight our sample according to the number of edits to each wiki over the entire study period.   The number of observations sampled and the total weight assigned to each Wiki for each model and threshold are available in the supplementary material. 

% Paragraph summarizing how ores was trained and routing people to halfak's preprint.

% briefly describe the release of the feature and what it takes to turn it on. 
%Prior to the development and release of RCfilters,  tools with features such as algorithmic flagging or filtering by user characteristics were available in special interfaces such as huggle.  None of the above  

\subsection{ORES edit classifier damaging scores}

To know whether an edit was flagged in RCfilters, we need to obtain the ORES score that was assigned to the edit and the thresholds that were active at the time the edit was reverted.  We obtain historical ORES scores from a log maintained by the Wikimedia foundation.
%for each wiki from the public mirror of the ORES scores database hosted by the Wikimedia foundation's quarry service.  


\subsection{RCfilters thresholds}

The thresholds that trigger RCfilters flagging are not constant, but depend on the precision and recall of deployed ORES models, and have also been changed in response to community feedback.  Since new models were deployed during our study period, scraping the page where the active thresholds are displayed would not provide the correct thresholds that were in use when an edit was made or that moderators reviewing changes would observe.

Fortunately, the configuration determining the thresholds, the trained ORES models, the code to run them are open source, and the exact time that changes are deployed is published at the Wikimedia foundation's server admin log.  So we wrote a script to combine this information to determine the precise thresholds that were active for each edit.

\section{Analytic plan \label{sec:analytic.plan}}

We test our hypotheses using a regression discontinuity design (RDD) for causal estimation of the effect of flagging an action on sanctioning (for \textbf{H1}) and controversial sanctioning (for \textbf{H2} and \textbf{H3}).

RDDs are an increasingly popular approach for causal inference in natural settings in economics because they resemble a randomized control trial for data points in the neighborhood of a discontinuity \cite{lee_regression_2010}.  RDDs model an outcome $Y$, as a function of a continuous ``forcing variable'' $Z$, other covariates $X$, and a cutoff $c$ such that $Z>c$ determines treatment assignment.  In principle treatment assignment conditional on $Y$ is ``as good as random'' under two assumptions: (1) that agents have at most limited control over $Z>c$ and (2) that the relationship between $Y$ and $Z$ is smooth.   If the assumptions hold then causal inference is simplified to the problem of statistically conditioning on the forcing variable $Z$ using a linear regression in the neighborhood of the cutoff $c$ (defined by $[c-\rho,c+\rho]$, for a ``bandwidth'', $\rho$) \cite{lee_regression_2010}.  

In the social computing,  \cite{narayanan_all_2019} and \cite{hill_hidden_nodate} use within-subjects designs similar to RDDs to analyze sociotechnical consequences of policy and design interventions for online communities.  Both studies use time as a forcing variable which threatens validity as the timing of intervention may be influenced by unobserved factors, which would violate assumption (1).  Our treatment, being flagged in RCfilters, is a good candidate for an RDD from the perspective of assumption (1).  Editors are unlikely to have much control over the scores that their edits receive.  While attempts to evade sanction by specially crafting edits to evade algorithmic detection may be possible,  we do not think they will be wide-spread.

Assumption (2) would be violated if unobserved treatments effecting our outcomes occur at discrete levels of ORES scores.  For example, if another moderation tool is triggered by the ORES damaging scores, the effects of usage of that tool on our outcomes would confound our analysis of RCfilters.  This is a realistic scenario that is part of the design of the ORES system which makes scores available via API so that community members can use them to power their own tools.  We are aware of bots that automatically revert edits and are triggered by the ``very damaging'' threshold on some of the Wikis in our sample. Since we exclude reverts by bots from our analysis these bots are not a threat.  We are also aware that Huggle, a tool for reviewing encyclopedia edits incorporates ORES scores as a feature in it's own models for detecting damage.  However, since the ORES scores are not the only feature in the Huggle models, it is unlikely that thresholds in Huggle will constitute discontinuities in the relationship between ORES scores and our outcomes.  As a robustness check against threats to assumption (2) we conduct ``placebo tests'' by running our analysis at artificial cutoffs not equal to the real thresholds.  We present results of this robustness check in the supplementary material.

%(see \cite{chancellor_thyghgapp:_2016} for an example of evading content moderation through lexical variation in social media) we do not think this will be wide-spread or successful on Wikipedia. 

% 3 * 2 * 2 = 12 
\newcounter{equationcnt}
\newcounter{figuretmp}
\setcounter{figuretmp}{\thefigure}
\setcounter{figure}{0}

We present results from a total of 9 logistic regression models.  For \textbf{H1} and
\textbf{H3} we fit separate models for IP editors, non-IP editors, editors with user pages and editors without user pagers and for \textbf{H2} we model all editors. 
We incorporate the three RCfilters thresholds that we analyze in each model following the example of \cite{litschig_impact_2013}.  Our goal is to estimate ($\tau_j$) the causal effect of being flagged at level $j$ where $j \in \{1,2,3\}$ corresponding to labels of ``maybe damaging'', ``likely damaging'' and ``very likely damaging''.  
For each cutoff $(c_{jw})$, we select all revisions $r$ to wiki $w$ that fall within radius $p$ such that $c_{wj}- p < score_{r} < c_{jw} + p$ where $score_r$ is the output from the ORES classifier.  Following established approaches to RDD, we fit ``kink'' models that have a change in slope at the discontinuity \cite{lee_regression_2010,litschig_impact_2013}. Equation \eqref{eq:rdd_reverted} shows our specification for our models (the only differences between our models are the dependent variables, $Y$ and the type of editor whose edits are modeled.

\begin{figure}
\renewcommand\figurename{Eq.}
%\begin{small}
\begin{equation*}
    \begin{split}
            P(Y_{rw}) & = \left[ \tau_1 \mathbf{1} [score_{r} > c_{1w}] + \alpha_{10}(score_{r} - c_1) + \alpha_{11}\left(score_{r}-c_{1w}\right) \mathbf{1} [score_{r} > c_{1w}]\right]\mathbf{1_{1p}}  \\
        & + \left[ \tau_2\mathbf{1}[score_{r} > c_{2w}] + \alpha_{20}(score_{r} - c_2) + \alpha_{21}\left(score_{r}-c_{2w}\right)\mathbf{1}[score_{r} > c_{2w}]\right]\mathbf{1_{2p}}  \\
        & + \left[ \tau_3\mathbf{1}[score_{r} > c_{3w}] + \alpha_{30}(score_{r} - c_3) + \alpha_{31}\left(score_{r}-c_{3w}\right)\mathbf{1}[score_{r} > c_{3w}]\right]\mathbf{1_{3p}} \\
        & + \sum_{j=1}^3B_j\mathbf{1}[seg_{j-1} < score
        \le  seg_{j}]\mathbf{1}_{jp} + \alpha_w + \mu_{rw}
    \end{split} 
\end{equation*}
\begin{equation*}
    \begin{split}
     \mathbf{1_{jp}} & =  \mathbf{1}[c_{wj}(1-p) < score_{rw} < c_{jw}(1+p)]  \\ 
     j=1,2,3; &  ~~p=0.05
    \end{split}
\end{equation*}
%\end{small}
\caption{Specification of locally linear logistic regression model for a regression discontinuity design with three cutoffs.  $\mathbf{1}$ is the indicator function. \label{eq:rdd_reverted}}
\end{figure}    

\setcounter{equationcnt}{\thefigure}
\setcounter{figure}{\thefiguretmp}
% % We conduct placebo tests to 


We use Bayesian inference to estimate our models for two reasons.  First, within some of the wikis we analyze virtually all edits above the ``very damaging'' level are reverted.  This ``separation'' is a problem for classical estimation approaches as coefficients head to infinity \cite{allison_convergence_2004}. Preferred solutions in the classical framework include penalized likelihood methods that introduce bias.  Our Bayesian approach uses weakly-informative priors that pull our estimates slightly toward zero, but does not have the problem of separation.  This leads to a conservative analysis less likely to result in false discovery.  We fit our models using the rstanarm package (version 2.19.2) and the default priors which are chosen to be weakly informative and which we provide for reference in the supplementary material. 

The second reason we choose to use Bayesian inference is that it considerably simplifies our analysis.  Our hypotheses compare effects of flagging between different types of editors.  Testing them in a classical framework can be done by fitting a joint model including all editor types and conducting a Wald test.  In a Bayesian framework, we can sample parameter estimates from the posterior distribution and test our hypotheses using statistical tests for differences between these samples \cite{morey_fallacy_2016}. Prior work in \cite{gan_gender_2018} makes a similar rationale for a Bayesian approach. 

In Bayesian analysis, fitted models take the form of \emph{posterior distributions} constituting a probability distribution of model coefficients conditional on our model, data, and priors.  We consider a hypothesis supported if it is consistent with at least 95\% of posterior draws. In other words, we accept our hypotheses if our parameter estimate has the predicted sign and the 95\% credible interval ($95\%~\mathrm{CI}$) does not contain zero.

\subsection{Presentation of marginal effects plots}

To assist readers in interpreting our results, we provide marginal effects plots for each models (e.g. \ref{fig:adoption.me}). These plots visualize the modeled relationship between ORES scores and the probability that an average edit in our sample is reverted (in the case of H1) or that a revert is controversial (for H2; H3) in the neighborhood of thresholds that trigger flags.  In each such plot, the x-axis shows the distance from the threshold such that discontinuities at 0 represent the effect of being flagged on the relevant outcome.  These plots show inferences for the English language edition of Wikipedia, but since intercepts are the only part of our model that depend on \emph{wiki}, the slopes and the discontinuities caused by algorithmic flagging represent inferences over all our data.    

%To check that our models fit the data and that observed discontinuities are not spurious, these plots also present probabilities of reversion estimated directly from our data within bins around different values of x. 


\subsection{Adoption Check \label{sec:adoption}}

Before discussing our statistical hypothesis tests, we first test that the system is adopted by community moderators on Wikipedia before turning to results for our hypotheses. This is necessary because, prior to our study, little was known of the extent of RCfilters usage beyond anecdotal conversations on Wikis, irc channels, meet-ups, and mailing lists.  So it not obvious RCfilters will have the sort of causal effects on sanctioning that would allow us to test our hypotheses.  

To demonstrate that RCfilters flags are being used by Wikipedia moderators, we look for evidence that flagging has a causal effect on sanctioning over all types of editors using model following equation \ref{eq:rdd_reverted}.  Observing discontinuous increases in the probability of reversion at a given thresholds constitutes evidence that flags in RCFilters have a causal effect on moderation actions on Wikipedia.  Specifically, we test the hypothesis that flagging increases the probability that an edit is reverted.  If Wikipedians are indeed using flags in RCfilters to review potentially damaging edits, then we should find positive values for $\tau_j$, the coefficients for the variable representing the effect of crossing the threshold powering flags and filters in our regression models predicting if edits are reverted.


<<set.adoption.check.vars, echo=FALSE, message=F,results='hide'>>=
tau.1 <- mod.adoption.draws[[tau.1.name]]
tau.2 <- mod.adoption.draws[[tau.2.name]]
tau.3 <- mod.adoption.draws[[tau.3.name]]

tau.overall <- tau.1+tau.2+tau.3
tau.overall.025 <- quantile(tau.overall,0.025)
tau.overall.975 <- quantile(tau.overall,0.975)
vld.proto.below <- proto.reverted(mod.adoption.me.data.df, where='below', threshold.name='verylikelybad')

vld.proto.above <- proto.reverted(mod.adoption.me.data.df, where='above', threshold.name='verylikelybad')

ld.proto.below <- proto.reverted(mod.adoption.me.data.df, where='below', threshold.name='likelybad')

ld.proto.above <- proto.reverted(mod.adoption.me.data.df, where='above', threshold.name='likelybad')

md.proto.below <- proto.reverted(mod.adoption.me.data.df, where='below', threshold.name='maybebad')

md.proto.above <- proto.reverted(mod.adoption.me.data.df, where='above', threshold.name='maybebad')
threshold.delta <- signif(abs(md.proto.below[['d.nearest.threshold']]),2)

suppressMessages({
assertthat::are_equal(threshold.delta, signif(abs(ld.proto.below[['d.nearest.threshold']]),2))
assertthat::are_equal(threshold.delta, signif(abs(vld.proto.below[['d.nearest.threshold']]),2))
assertthat::are_equal(threshold.delta, signif(abs(md.proto.below[['d.nearest.threshold']]),2))
})
@ 

 
As shown in table \ref{tab:adoption.check}, we observe discontinuous increases in the likelihood of reversion at the ``maybe damaging'', and ``likely damaging'' thresholds, but at not at the ``very likely damaging'' threshold.  We find the greatest effect at the ``maybe damaging'' threshold  ($\overline{\tau_1} = \Sexpr{signif(mean(tau.1),2)}$,  $95\%~\mathrm{CI}=\Sexpr{get.CI.str(tau.1,digits=3)}$).
Per our model and figure \ref{fig:adoption.me}, the likelihood of reversion just \Sexpr{threshold.delta} below the ``maybe damaging'' threshold is only \Sexpr{proto.reverted.CI.str(md.proto.below, digits.1=2,digits.2=3)} so reverts of unflagged edits are relatively rare.  However, being flagged causes a dramatic increase in the reversion probability to \Sexpr{proto.reverted.CI.str(md.proto.above, digits=2)} for edits just \Sexpr{threshold.delta} above the threshold.  Flagging an edit at the ``maybe damaging'' level increases the odds it will be reverted by a factor of $\Sexpr{signif(exp(mean(tau.1)),2)}$ ($95\%~\mathrm{CI}=\Sexpr{get.CI.str(tau.1,transform.f=exp)}$). 

Similarly, we find evidence that algorithmic flags at the ``likely damaging'' level causes a very large increase in the chances that an edit will be undone: an increases in odds by a factor of $\Sexpr{signif(exp(mean(tau.2)),2)}$ ($95\%~\mathrm{CI}=\Sexpr{get.CI.str(tau.2,transform.f=exp)}$).  Per figure \ref{fig:adoption.me}, from \Sexpr{threshold.delta} below to \Sexpr{threshold.delta} above the ``likely damaging'' threshold the likelihood an edit is reverted jumps from \Sexpr{proto.reverted.CI.str(ld.proto.below, digits.1=2,digits.2=3)} to \Sexpr{proto.reverted.CI.str(ld.proto.above, digits.1=2,digits.2=3)}.
  
However, we do not observe an increase in the likelihood of reversion at the ``very likely damaging'' level ($\overline{\tau_3} = \Sexpr{signif(mean(tau.3),2)}$,  $95\%~\mathrm{CI}=\Sexpr{get.CI.str(tau.3)}$).  While this is surprising given the large increases observed at the other two thresholds, it makes sense given that only the ``likely damaging'' and ``very likely damaging'' levels but not at the ``maybe damaging'' level are enabled by default.  So if the ``likely damaging'' filters or flags are visible then probably so will  ``very likely damaging'' flags and filters. In our experience filtering at the ``very likely damaging'' threshold alone when patrolling recent changes on English Wikipedia was not very useful as few edits cross this threshold and those that do are frequently reverted by bots before a human editor can perform the revert.  Racing the bots to revert obvious damage seems like less useful and rewarding work compared to enabling the ``maybe damaging'' threshold to surface more ambiguous edits requiring human judgment to review.

From this analysis we conclude that the RCfilters system powered by ORES was put in to use by Wikipedians and has a detectable influence on which edits are reverted within 48 hours.   The large discontinuous increases in reversion we observe have important implications for design of sociotechnical systems that use algorithmic predictions to guide human attention which we discuss in \ref{sec:design.implications}.  We did not observe significant effects at the ``very likely damaging'' threshold and therefore we exclude this threshold from our subsequent hypothesis tests.

%   with  below this threshold Reversion below this threshold is rare  are rarely reverted, with a likelihood of

%   Similarly, being flagged at the ``likely damaging'' threshold additionally increases the odds an edit is reverted by a factor of $\Sexpr{signif(exp(mean(tau.2)),2)}$ ($95\%~\mathrm{CI}=\Sexpr{get.CI.str(tau.2,transform.f=exp)}$

% Indeed, by averaging over the sum of the three thresholds, we observe that being flagged generally increases the chances of reversion ($\overline{\sum_j \tau_j} = \Sexpr{signif(mean(tau.overall),2)}$, $95\%~\mathrm{CI} = \Sexpr{get.CI.str(tau.overall)}$).

% We observe the greatest effect for the third threshold, which corresponds to the red flag indicating that an edit is ``very likely damaging''  
% Our estimate indicates that flagging an edit at the ``very likely damaging'' level increases it's odds of being reverted by a factor of ).  An average edit with an ORES score only $\Sexpr{signif(abs(vld.proto.below[['d.nearest.threshold']]),2)}$ below the threshold triggering ``very likely damaging'' flags will be reverted with probability \Sexpr{proto.reverted.CI.str(vld.proto.below, digits=2)}, but right above the threshold the probability increases to \Sexpr{proto.reverted.CI.str(vld.proto.above)}.

% Being flagged at the orange, or ``likely damaging'' level ($\overline{\tau_2} = \Sexpr{mean(tau.2)}$, $95\%~\mathrm{CI}=\Sexpr{get.CI.str(tau.2)}$) causes an increase in the odds of being reverted by a factor of $\Sexpr{signif(exp(mean(tau.2)),2)}$ ($95\%~\mathrm{CI}=\Sexpr{get.CI.str(tau.2,transform.f=exp)}$), corresponding to a jump in reversion probability from \Sexpr{proto.reverted.CI.str(ld.proto.below)} to \Sexpr{proto.reverted.CI.str(ld.proto.above)}.  \TODO{Is it a problem if these CI overlap? They might overlap just because of uncertainty about the kinks, not just the discontinuity.}
% For the yellow, or ``maybe damaging'' level  ($\overline{\tau_3} = \Sexpr{signif(mean(tau.1),2)}$, $95\%~\mathrm{CI}=\Sexpr{get.CI.str(tau.1)}$) we do not estimate a statistically significant difference between the probability of being reverted above and below the cutoff.

\begin{figure}[t]
  \centering
\begin{subfigure}[t]{\textwidth}
  \centering
<<regplot.adoption, fig.height=2, fig.width=0.6*7, out.width="0.6\\textwidth",echo=FALSE, warning=F,message=F, error=F, result='asis'>>=
p <- my_mcmc_intervals(mod.adoption.draws,symbols=FALSE, tex=TRUE) + ggtitle("Effect of algorithmic flagging on sanctioning")

print(p)
@   
\caption{Plots of marginal posteriors for effects of flagging on reversion in the adoption check. \label{fig:adoption.posterior}}
\end{subfigure}
~
\begin{subfigure}[b]{\textwidth}
  \centering
  <<adoption.me.plot, echo=FALSE, fig.height=2.5, out.width='\\textwidth'>>=
  make.rdd.plot(mod.adoption.me.data.df, mod.adoption.bins.df, title="Prob reverted, all editors",digits=2)
  @
  \caption{Marginal effects plot showing model predicted relationship between ores score and reversion around the cutoffs \label{fig:adoption.me}}
\end{subfigure}
\caption{\label{tab:adoption.check} Results from adoption check show strong evidence of RCfilters use on Wikipedia with flagging at the maybe bad or likely bad thresholds roughly doubling the likelihood that an edit is reverted. \ref{fig:adoption.posterior} shows our inferred parameters with 95\% credible intervals and \ref{fig:adoption.me} plots the inferred relationship between ORES score and the probability of reversion around the threshold. }
\end{figure}

\subsection{Hypothesis tests}


We consider that our analysis supports \textbf{H1} in terms of the two kinds of identity-based signals visible in the moderation tools we consider:

\textbf{H1a:} Flagging an action causes a greater increase in the likelihood the action is sanctioned when the action is by a registered editor than when is by an unregistered editor.

We consider the effect of flagging over both the ``maybe damaging'' and ``very likely damaging'' thresholds.  In terms of our regression models we consider \textbf{H1a} supported if:

$$\sum_j{\tau^{\mathbf{H1}, not\_IP}_j} = \tau^{\mathbf{H1},not\_IP}_{1} + \tau^{\mathbf{H1},not\_IP}_{2}  > \tau^{\textbf{H1}, IP}_{1} + \tau^{\textbf{H1}, IP}_{2} = \sum_j{\tau^{\mathbf{H1}, IP}_j}$$ 

\textbf{H1b:} Flagging an action causes a greater increase in the likelihood the action is sanctioned when the action is by an editor with a user page by an editor that does have a user page.

In terms of our regression models we consider \textbf{H1b} supported if:

$$\sum_j{\tau^{\textbf{H1}, u.p}_j} = \tau^{\textbf{H1}, u.p.}_{1} + \tau^{\textbf{H1},u.p.}_{2}  > \tau^{\textbf{H1},no\_u.\_p.}_{1} + \tau^{\textbf{H1},no\_u.p.}_{2} = \sum_j{\tau^{\textbf{H1}, no\_u.p.}_j}$$ 

Where we use the abbreviation ``no u.p.'' to indicate coefficients for edits by editors without user pages and ``u.p'' to stand for edits by editors having user pages.


Our analysis supports \textbf{H2} if the total effect of being flagged on controversial reversion is greater than 0.

$$\sum_j{\tau^{\mathbf{H2}}_j} = \tau^{\mathbf{H2}}_{1} + \tau^{\mathbf{H2}}_{2} > 0 $$

Similarly to \textbf{H1}, we break \textbf{H3} down into \textbf{H3a}, which is supported if the total effect of being flagged on controversial reversion is less for actions by Non-IP editors is greater than for IP editors and \textbf{H3b} which is supported if this effect is greater for edits by users with a user page than those without, formally:

$$ \sum_j{\tau^{\mathrm{H3},not\_IP}_j} = \tau^{\mathbf{H3},not\_IP}_{1} + \tau^{\mathbf{H3},not\_IP}_{2}  > \tau^{\textbf{H3}, IP}_{1} + \tau^{\textbf{H3}, IP}_{2} = \sum_j{\tau^{\mathrm{H3},IP}_j}$$
and
$$\sum_j{\tau^{\mathrm{H3}, u.p}_j} = \tau^{\textbf{H3}, u.p.}_{1} + \tau^{\textbf{H3},u.p.}_{2}  > \tau^{\textbf{H3},no\_u.p.}_{1} + \tau^{\textbf{H3},no\_u.p.}_{2} = \sum_j{\tau^{\mathrm{H3}, no\_u.p.}_j}$$.

\section{Results}
<<set.h1.vars,echo=FALSE>>=
h1.tau.anon <- apply(matrix(c(h1.tau.1.anon,h1.tau.2.anon),ncol=2,byrow=FALSE),1,sum)
h1.tau.non.anon <- apply(matrix(c(h1.tau.1.non.anon,h1.tau.2.non.anon),ncol=2,byrow=FALSE),1,sum)
h1.tau.1.non.anon.sub.anon <- h1.tau.1.non.anon - h1.tau.1.anon
h1.tau.2.non.anon.sub.anon <- h1.tau.2.non.anon - h1.tau.2.anon
h1.tau.non.anon.sub.anon <- apply(matrix(c(h1.tau.non.anon, -1*h1.tau.anon),ncol=2,byrow=FALSE),1,sum)

h1.tau.user.page <- apply(matrix(c(h1.tau.1.user.page,h1.tau.2.user.page),ncol=2,byrow=FALSE),1,sum)
h1.tau.no.user.page <- apply(matrix(c(h1.tau.1.no.user.page,h1.tau.2.no.user.page),ncol=2,byrow=FALSE),1,sum)
h1.tau.1.user.page.sub.no.user.page <- h1.tau.1.user.page - h1.tau.1.no.user.page
h1.tau.2.user.page.sub.no.user.page <- h1.tau.2.user.page - h1.tau.2.no.user.page
h1.tau.user.page.sub.no.user.page <- apply(matrix(c(h1.tau.user.page, -1*h1.tau.no.user.page),ncol=2,byrow=FALSE),1,sum)

anon.ld.proto.below <- proto.reverted(mod.anon.reverted.me.data.df, where='below', threshold.name='likelybad')
anon.ld.proto.above <- proto.reverted(mod.anon.reverted.me.data.df, where='above', threshold.name='likelybad')
anon.md.proto.below <- proto.reverted(mod.anon.reverted.me.data.df, where='below', threshold.name='maybebad')
anon.md.proto.above <- proto.reverted(mod.anon.reverted.me.data.df, where='above', threshold.name='maybebad')
non.anon.ld.proto.below <- proto.reverted(mod.non.anon.reverted.me.data.df, where='below', threshold.name='likelybad')
non.anon.ld.proto.above <- proto.reverted(mod.non.anon.reverted.me.data.df, where='above', threshold.name='likelybad')
non.anon.md.proto.below <- proto.reverted(mod.non.anon.reverted.me.data.df, where='below', threshold.name='maybebad')
non.anon.md.proto.above <- proto.reverted(mod.non.anon.reverted.me.data.df, where='above', threshold.name='maybebad')

up.ld.proto.below <- proto.reverted(mod.user.page.reverted.me.data.df, where='below', threshold.name='likelybad')
up.ld.proto.above <- proto.reverted(mod.user.page.reverted.me.data.df, where='above', threshold.name='likelybad')
up.md.proto.below <- proto.reverted(mod.user.page.reverted.me.data.df, where='below', threshold.name='maybebad')
up.md.proto.above <- proto.reverted(mod.user.page.reverted.me.data.df, where='above', threshold.name='maybebad')
no.up.ld.proto.below <- proto.reverted(mod.no.user.page.reverted.me.data.df, where='below', threshold.name='likelybad')
no.up.ld.proto.above <- proto.reverted(mod.no.user.page.reverted.me.data.df, where='above', threshold.name='likelybad')
no.up.md.proto.below <- proto.reverted(mod.no.user.page.reverted.me.data.df, where='below', threshold.name='maybebad')
no.up.md.proto.above <- proto.reverted(mod.no.user.page.reverted.me.data.df, where='above', threshold.name='maybebad')

@ 
\begin{figure}[b]
  \centering
<<reverted.me.plot, echo=FALSE, fig.height=5,out.width='\\textwidth',cache=F>>=
make.comparison.me.plot(mod.anon.reverted.me.data.df,
                        mod.anon.reverted.bins.df,
                        'IP',
                        mod.non.anon.reverted.me.data.df,
                        mod.non.anon.reverted.bins.df,
                        'Not IP',
                        mod.no.user.page.reverted.me.data.df,
                        mod.no.user.page.reverted.bins.df,                        
                        "No user page",
                        mod.user.page.reverted.me.data.df, 
                        mod.user.page.reverted.bins.df, 
                        "User page",
                        plot.data=TRUE,
                        digits=2,
                        used.thresholds=c('maybe bad','likely bad')
                        )

@ 
  \caption{Marginal effects plot showing model predicted relationship between ores score and reversion around the cutoffs for registered and IP editors.\label{fig:h1.me}}
\end{figure}

\subsection{\textbf{H1}: Effect of flagging on sanctioning}

We turn now to our main results for our hypotheses, beginning with \textbf {H1:} that algorithmic flags will be relatively less influential in regulating over-profiled editors.  Figure \ref{fig:h1.regplot} summarizes our findings and \ref{fig:h1.me} shows marginal effects plots useful for interpreting our findings in terms of the probability that an edit to English Wikipedia will be reverted in the neighborhood of the thresholds. 



\begin{figure}[t]
\centering
% I have so much data and these marginal posteriors are so normal that there isn't much point in showing the intervals
% \begin{subfigure}[t]{0.49\textwidth}
% \centering
<<regplot.H1.anon,fig.height=3.3,out.width="100%",echo=FALSE,warning=F, message=F,results='asis',cache=F>>=

h1.mcmc.data <- data.table(h1.tau.1.non.anon,
                           h1.tau.1.anon,
                           h1.tau.1.non.anon.sub.anon,
                           h1.tau.2.non.anon,
                           h1.tau.2.anon,
                           h1.tau.2.non.anon.sub.anon,
                           h1.tau.non.anon,
                           h1.tau.anon,
                           h1.tau.non.anon.sub.anon,
                           h1.tau.1.user.page,
                           h1.tau.1.no.user.page,
                           h1.tau.1.user.page.sub.no.user.page,                           
                           h1.tau.2.user.page,
                           h1.tau.2.no.user.page,
                           h1.tau.2.user.page.sub.no.user.page,
                           h1.tau.user.page,
                           h1.tau.no.user.page,
                           h1.tau.user.page.sub.no.user.page)

p <- big_reg_plot2(h1.mcmc.data) + ggtitle("Effects of algorithmic flagging on sanctioning")

print(p)
@       
\caption{Results for \textbf{H1} that flagging has a greater causal effect on reversion for editors that are not over-profiled than for editors that are.  Concerning \textbf{H1a},  effects of flagging for Non IP editors are greater than for IP editors at both the ``maybe bad'' and the ``likely bad'' thresholds and the difference over both thresholds is also positive.  On the other hand, flagging increases sanctioning, but not more for editors with user pages than for editors without. 
  We place points over the X-axis corresponding to posterior means, which are the most likely parameter value. Lines show 95\% credible intervals for each parameter, indicating uncertainty in our inferences.  For each threshold, we also model parameters for each type of editor and then the difference between groups of editors below.  We also show overall effects obtained by summing posteriors over thresholds. We draw conclusions when the credible intervals do not contain 0, which is the Bayesian analog to testing a hypothesis with $p<0.05$. \label{fig:h1.regplot}}
\end{figure}                                                                                      


We proposed that Wikipedia moderators reviewing edits will be likely to scrutinize edits by over-profiled users whether they are flagged or not, but that they be likely to scrutinize edits by under-profiled editors if they are flagged.
We find strong support for \textbf{H1a} which uses whether an edit is attributed to a registered editor or to an IP address. Per figure \ref{fig:h1.me}, which shows the marginal effects plots for \textbf{H1}, edits to English with an ORES score just \Sexpr{threshold.delta} below the ``maybe damaging'' threshold jump in reversion probability from  \Sexpr{proto.reverted.CI.str(anon.md.proto.below, digits.1=2,digits.2=3)} to \Sexpr{proto.reverted.CI.str(anon.md.proto.above, digits.1=2,digits.2=3)} for IP editors. This corresponds to a $\Sexpr{signif(exp(mean(h1.tau.1.anon)),2)}$-factor $(95\%~\mathrm{CI}=\Sexpr{get.CI.str(h1.tau.1.anon,transform.f=exp,format.percent=F)}$ increase in the odds of reversion.
But for non-IP editors we see an even bigger jump: a $\Sexpr{signif(exp(mean(h1.tau.1.non.anon)),2)}$-factor $(95\%~\mathrm{CI}=\Sexpr{get.CI.str(h1.tau.1.non.anon,transform.f=exp,format.percent=F)}$ increase in reversion odds. This means receiving an ORES score just \Sexpr{threshold.delta} above the threshold increases the probability of reversion from \Sexpr{proto.reverted.CI.str(non.anon.md.proto.below, digits.1=2,digits.2=3)} to \Sexpr{proto.reverted.CI.str(non.anon.md.proto.above, digits.1=2,digits.2=3)} compared to an edit receiving a score \Sexpr{threshold.delta} below the threshold.

Similarly, at the ``likely damaging'' we find a $\Sexpr{signif(exp(mean(h1.tau.2.non.anon)),2)}$-factor $(95\%~\mathrm{CI}=\Sexpr{get.CI.str(h1.tau.2.non.anon,transform.f=exp,format.percent=F)}$ increase in the odds of reversion for Non-IP edits which is greater than the $\Sexpr{signif(exp(mean(h1.tau.2.anon)),2)}$-factor $(95\%~\mathrm{CI}=\Sexpr{get.CI.str(h1.tau.2.anon,transform.f=exp,format.percent=F)}$ increase in odds for IP edits. This corresponds to an increase in the probability that an edit to English Wikipedia will be reverted from \Sexpr{proto.reverted.CI.str(non.anon.md.proto.below, digits.1=2,digits.2=3)} to \Sexpr{proto.reverted.CI.str(non.anon.md.proto.above, digits.1=2,digits.2=3)} for Non-IP editors and an an increase from \Sexpr{proto.reverted.CI.str(anon.md.proto.below, digits.1=2,digits.2=3)} to \Sexpr{proto.reverted.CI.str(anon.md.proto.above, digits.1=2,digits.2=3)} for IP editors. 

Since we detect that edits by Non-IP editors are more sensitive to flagging at both thresholds, we also observe a greater overall effect for Non-IP editors $(\tau^{\mathrm{Non IP}}=\Sexpr{signif(mean(h1.tau.non.anon),2)}$ $(95\%~\mathrm{CI}=\Sexpr{get.CI.str(h1.tau.non.anon,format.percent=F)}$) than for IP editors $(\tau^{\mathrm{IP}}=\Sexpr{signif(mean(h1.tau.anon),2)}$ $(95\%~\mathrm{CI}=\Sexpr{get.CI.str(h1.tau.anon,format.percent=F)}$. Therefore we conclude that our analysis supports \textbf{H1a}.

On the other hand our analysis of \textbf{H1b} does not suggest that flagging has a greater effect for editors with user pages than those without.  While we do observe increases in the probability that an edit will be reverted when edit's ORES scores cross the ``maybe bad'' and ``likely damaging'' thresholds these changes are pretty similar between editors with and without user pages. For the ``maybe damaging'' threshold we find a jump in reversion probability from  \Sexpr{proto.reverted.CI.str(no.up.md.proto.below, digits.1=2,digits.2=3)} to \Sexpr{proto.reverted.CI.str(no.up.md.proto.above, digits.1=2,digits.2=3)} for editors without user pages and a jump from \Sexpr{proto.reverted.CI.str(up.md.proto.below, digits.1=2,digits.2=3)} to \Sexpr{proto.reverted.CI.str(up.md.proto.above, digits.1=2,digits.2=3)} for editors that do have user pages. Contrary to our proposed hypothesis, at the ``maybe damaging'' level the odds ratio of $\Sexpr{signif(exp(mean(h1.tau.1.no.user.page)),2)}$ $(95\%~\mathrm{CI}=\Sexpr{get.CI.str(h1.tau.1.no.user.page,transform.f=exp,format.percent=F)})$ for editors that have created user pages is greater the odds ratio of $\Sexpr{signif(exp(mean(h1.tau.1.user.page)),2)}$ $(95\%~\mathrm{CI}=\Sexpr{get.CI.str(h1.tau.1.user.page,transform.f=exp,format.percent=F)})$ for those that do not. 

We do not detect a statistical difference between editors with and without user pages at the ``likely damaging'' threshold.  The chances that an edit by an editor without a user page increase in probability from  \Sexpr{proto.reverted.CI.str(no.up.ld.proto.below, digits.1=2,digits.2=3)} to \Sexpr{proto.reverted.CI.str(no.up.ld.proto.above, digits.1=2,digits.2=3)} and for editors that do have user pages we find an increase from \Sexpr{proto.reverted.CI.str(up.ld.proto.below, digits.1=2,digits.2=3)} to \Sexpr{proto.reverted.CI.str(up.ld.proto.above, digits.1=2,digits.2=3)}.  These changes correspond to odds ratios of $\Sexpr{signif(exp(mean(h1.tau.2.no.user.page)),2)}$ $(95\%~\mathrm{CI}=\Sexpr{get.CI.str(h1.tau.2.no.user.page,transform.f=exp,format.percent=F)}$) and $\Sexpr{signif(exp(mean(h1.tau.2.user.page)),2)}$ ($95\%~\mathrm{CI}=\Sexpr{get.CI.str(h1.tau.2.user.page,transform.f=exp,format.percent=F)}$) respectively, which we cannot statistically distinguish.  Therefore we do not know from our models if there is a difference in flagging effect between editors with and without user pages at the ``likely damaging'' level.  This uncertainty is driven by the scarcity of edits around the ``likely damaging'' threshold for registered editors.

Similarly, we observe no overall statistical difference between edits by editors that have user pages $(\tau^{\mathrm{No~u.p.}}=\Sexpr{signif(mean(h1.tau.user.page),2)}$ $(95\%~\mathrm{CI}=\Sexpr{get.CI.str(h1.tau.user.page,format.percent=F)})$ and edits by those that do not $(\tau^{\mathrm{IP}}=\Sexpr{signif(exp(mean(h1.tau.no.user.page)),2)}$ $(95\%~\mathrm{CI}=\Sexpr{get.CI.str(h1.tau.no.user.page,transform.f=exp,format.percent=F)})$.  Therefore we conclude that our analysis supports \textbf{H1a}, does not support \textbf{H1b}, and therefore provides mixed support for \textbf{H1} overall.


\subsection{\textbf{H2}: Effect of flagging on controversial sanctioning}

<<set.h2.vars,echo=FALSE>>=
h2.tau.1 <- mod.all.controversial.draws[[tau.1.name]]
h2.tau.2 <- mod.all.controversial.draws[[tau.2.name]]
h2.tau.3 <- mod.all.controversial.draws[[tau.3.name]]

h2.tau <- h2.tau.1 + h2.tau.2

h2.ld.proto.below <- proto.reverted(mod.all.controversial.me.data.df, where='below', threshold.name='likelybad')
h2.ld.proto.above <- proto.reverted(mod.all.controversial.me.data.df, where='above', threshold.name='likelybad')
h2.md.proto.below <- proto.reverted(mod.all.controversial.me.data.df, where='below', threshold.name='maybebad')
h2.md.proto.above <- proto.reverted(mod.all.controversial.me.data.df, where='above', threshold.name='maybebad')

@ 

We now turn to our results for \textbf{H2:} that edits being flagged will increase the chances of a controversial revert (meaning that a revert to an edit was undone by a third party).  Contrary to this hypothesis, we detect that having an ORES score crossing the ``maybe damaging'' or ``likely damaging''  decreases the chances that a reverted edit is controversial.  Figure \ref{fig:h2.regplot} summarizes our parameter estimates and figure \ref{fig:h2.me} is a marginal effects plots showing our modeled relationship between ORES score and the probability of a controversial sanction in the neighborhood of the thresholds.


\begin{figure}[t]
  \centering
\begin{subfigure}[t]{0.6\textwidth}
  \centering
<<regplot.controversial, fig.height=1.5, fig.width=0.6*7, echo=FALSE, warning=F,message=F, error=F, result='asis'>>=
  p <- my_mcmc_intervals(mod.all.controversial.draws,pars=c(tau.1.name,tau.2.name), symbols=FALSE, tex=TRUE,overall=TRUE) + ggtitle("Effect of flagging on controversial sanctioning")
  print(p)
@   
\caption{Plots of marginal posteriors for effects of flagging on whether sanctions are controversial.\label{fig:h2.regplot}}
\end{subfigure}
~
\begin{subfigure}[b]{\textwidth}
\centering  
<<me.plot.H2, echo=FALSE, fig.height=2, out.width='\\textwidth', results='asis'>>=
make.rdd.plot(mod.all.controversial.me.data.df, mod.all.controversial.bins.df, used.thresholds=c("maybebad","likelybad"), title="Prob. controversial, all reverts",digits=3)
@ 
\caption[H2. me plot]{Marginal effects plots for models predicting whether a revert is controversial, all editors. \label{fig:h2.me}}

\end{subfigure}
\end{figure}

For reverts to edits receiving an ORES scores just \Sexpr{threshold.delta} below the ``maybe damaging'' level, the probability that the revert is controversial is \Sexpr{proto.reverted.CI.str(h2.md.proto.below)}.  But for reverts scoring just across the threshold this drops slightly to \Sexpr{proto.reverted.CI.str(h2.md.proto.above)}, a decrease in odds of factor of $\Sexpr{signif(exp(mean(h2.tau.1)),2)}$ ($95\%~\mathrm{CI}=\Sexpr{get.CI.str(h2.tau.1,transform.f=exp)}$).  Similarly, being flagged at the ``likely damaging'' level decreases the odds that a revert is controversial by a factor of $\Sexpr{signif(exp(mean(h2.tau.2)),2)}$ ($95\%~\mathrm{CI}=\Sexpr{get.CI.str(h2.tau.2,transform.f=exp)}$).  Just before the ``likely damaging'' threshold a revert has a \Sexpr{proto.reverted.CI.str(h2.ld.proto.below)} probability of being undone by a third party, but flagging decreases this to \Sexpr{proto.reverted.CI.str(h2.ld.proto.below)}.  Algorithmic flagging has a negative effect over both thresholds $(\tau^{\mathrm{H2}}=\Sexpr{signif(mean(h2.tau),2)}$ $(95\%~\mathrm{CI}=\Sexpr{get.CI.str(h2.tau,format.percent=F)}$).



\subsection{\textbf{H3}: Identity-based signals and effects of flagging on controversial sanctioning }

\begin{figure}[h]
\centering
% I have so much data and these marginal posteriors are so normal that there isn't much point in showing the intervals
% \begin{subfigure}[t]{0.49\textwidth}
% \centering
<<regplot.H3.anon,fig.height=3.3,out.width="100%",echo=FALSE,warning=F, message=F,results='asis'>>=

h3.tau.anon <- apply(matrix(c(h3.tau.1.anon,h3.tau.2.anon),ncol=3,byrow=FALSE),1,sum)
h3.tau.non.anon <- apply(matrix(c(h3.tau.1.non.anon,h3.tau.2.non.anon),ncol=3,byrow=FALSE),1,sum)
h3.tau.non.anon.sub.anon <- apply(matrix(c(h3.tau.non.anon, -1*h3.tau.anon),ncol=2,byrow=FALSE),1,sum)

h3.tau.anon.2 <- apply(matrix(c(h3.tau.1.anon,h3.tau.2.anon),ncol=3,byrow=FALSE),1,sum)
h3.tau.non.anon.2 <- apply(matrix(c(h3.tau.1.non.anon,h3.tau.2.non.anon),ncol=3,byrow=FALSE),1,sum)
h3.tau.non.anon.sub.anon.2 <- apply(matrix(c(h3.tau.non.anon, -1*h3.tau.anon),ncol=2,byrow=FALSE),1,sum)

h3.tau.user.page <- apply(matrix(c(h3.tau.1.user.page,h3.tau.2.user.page),ncol=3,byrow=FALSE),1,sum)
h3.tau.no.user.page <- apply(matrix(c(h3.tau.1.no.user.page,h3.tau.2.no.user.page),ncol=3,byrow=FALSE),1,sum)
h3.tau.user.page.sub.no.user.page <- apply(matrix(c(h3.tau.user.page, -1*h3.tau.no.user.page),ncol=2,byrow=FALSE),1,sum)
h3.mcmc.data <- data.table(h3.tau.1.non.anon,
                           h3.tau.1.anon,
                           h3.tau.1.non.anon.sub.anon,
                           h3.tau.2.non.anon,
                           h3.tau.2.anon,
                           h3.tau.2.non.anon.sub.anon,
                           h3.tau.non.anon,
                           h3.tau.anon,
                           h3.tau.non.anon.sub.anon,
                           h3.tau.1.user.page,
                           h3.tau.1.no.user.page,
                           h3.tau.1.user.page.sub.no.user.page,                           
                           h3.tau.2.user.page,
                           h3.tau.2.no.user.page,
                           h3.tau.2.user.page.sub.no.user.page,
                           h3.tau.user.page,
                           h3.tau.no.user.page,
                           h3.tau.user.page.sub.no.user.page)

p <- big_reg_plot2(h3.mcmc.data) + ggtitle("Effects of algorithmic flagging on sanctioning")

print(p)
@       
\caption{Results for \textbf{H3} that flagging has a greater causal effect on controversial sanctioning for non-anonymous editors than for anonymous editors. Points show posterior medians and lines show 95\% credible intervals for each parameter. Dashed lines represent inferences based on aggregations of model parameters.}
\end{figure}                                                                                      



\begin{figure}[b]
  \centering
<<controversial.me.comparison.plot, echo=FALSE, fig.height=5,  out.width='\\textwidth'>>=
make.comparison.me.plot(mod.anon.controversial.me.data.df,
                        mod.anon.controversial.bins.df,
                        'IP',
                        mod.non.anon.controversial.me.data.df,
                        mod.non.anon.controversial.bins.df,
                        'Not IP',
                        mod.no.user.page.controversial.me.data.df,
                        mod.no.user.page.controversial.bins.df,  
                        "No user page",
                        mod.user.page.controversial.me.data.df, 
                        mod.user.page.controversial.bins.df, 
                        "User page",
                        digits=3,
                        used.thresholds=c("maybe bad", "likely bad")
                        )


@ 
  
  \caption{Marginal effects plot for models predicting whether a revert is controversial}
  \label{fig:me.controversial.comp}
\end{figure}

\section{Discussion}

\subsection{Limitations}

Readers should consider the following limitations of our analysis:

\subsubsection{Causality:} 
We believe that our regression discontinuity design provides relatively strong evidence of causal relationships between algorithmic flagging and sanctioning.  That said, causal interpretation of our estimates depends on untestable assumptions that we can model the secular relationship between ores scores and our outcomes, that editors do not manipulate their edits around thresholds and that other discontinuities triggered by ORES scores do not confound our analysis.  We argue that these assumptions are believable, but they are strong compared to those of a randomized controlled experiment.  Our analysis provides strengths that an experiment would not including ecological validity and non-intervention.  Furthermore, the limitations stemming from regression discontinuity assumptions are relatively minor compared to those facing our comparison of edits by editors having different visible identity-characteristics.

While our study design affords causal inference for effects of flagging, we it does not provide causal evidence for the interactions between identity-based signals and flagging.   Our theory proposes that the presence of identity-based signals cause moderators to make some sanctioning actions instead of others, but our evidence only allows us to describe differences between IP editors, editors with no user page, and other editors.  It does not show that the presence or absence of identity-based signals in moderation tools explains the observed differences.  A promising direction for future work is to conduct a randomized controlled trial that varies identity-based signals and algorithmic flags in online community moderation.   

\subsubsection{Generalizability:}

While our theories of interactions between identity-based signals and algorithmic flags is general, we study a single system, deployed on multiple Wikipedia projects.  These projects are not representative of online communities in general or even of Wikipedia language editions.  We analyze the broadest possible sample in an effort to improve generalizability beyond English Wikipedia alone.  Wikipedia language communities adopted ORES according to their perceived needs and their ability to label training data.  We do not claim that our findings generalize beyond the specific pool of communities that we study. 

\subsubsection{Missing data:}

% if we switch to the "quarry" dataset will rewrite this 
Constructing our measure of flagging required retroactive reconstruction of software environments for running the models that were active at the time that edits were made and re-running the models on historical edits.  This process inevitably lead to missing data in the cases of deleted edits or accounts.  We excluded wikis from our analysis with a high proportion of missing data, but it remains conceivable that data is missing in unknown ways that may introduce bias in our estimates.  

\subsubsection{Alternative Explanations:}

Finally, our analysis cannot rule out plausible alternative explanations for our findings related to systematic differences between edits with or without identity-based signals.  For example, if damaging edits by IP editors, or editors without user pages are more difficult for the ORES model to detect, that could drive our findings for \textbf{H1} as sanctioning would be less driven by algorithmic flagging for such editors. Similarly, if over-profiled editors make edits that are prone to controversial sanctioning that might explain our findings for \textbf{H3}.  

% maybe newcomers with user pages are more suspect?


Both of these scenarios seem to suggest that over-profiled editors are sophisticated relative to other editors, which is doubtful. Yet other unknown systematic differences between editors cannot be ruled out.   

\subsection{Conclusion}

As algorithmic flagging is increasingly adopted in online community governance, it is important to study the consequences of these systems for collaboration and fairness.  
Critics of machine learning trace how algorithms can encode discriminatory patterns in human behavior. In response, some machine learning researchers propose adaptations to traditional modeling building practice to limit and control algorithmic bias.  But when tools for predictive governance are introduced into a sociotechnical system, they may become incorporated in ways that may not be anticipated from an analysis of the tool alone.  Theories of ``over-profiling'' and ``statistical discrimnation'' make opposite claims about how 

We investigate how the RCfilters/ORES was put into use by Wikipedians by analyzing the effects of being flagged by the algorithm using a regression discontinuity design. We found that the tool was put into use, but that it did not amp 

critiques of algorithmic fairness or discrimination
machine learning practitioners pursue methods for building algorithms   Field studies of such systems that are deployed and used in the wild, as we do in this study of the RCfilters/ORES system on Wikipedia.  Our research design based on regression discontinuity causal  

Based on the logic of ``over-profiling,'' statistical discrimination, and salient signals, we proposed that Wikipedia moderators would rely more on algorithmic flags to guide them to discover damaging edits by users that are not already over-profiled.  Instead we found little difference in the effect of flagging on the likelihood of reversion between registered and IP editors.

What explains this surprising result?  One possibility is that Wikipedians do not actually ``over-profile'' IP editors at all. If Wikipedians already scrutinize edits by IP editors and by registered users equally, then we would not expect to find a difference in how flagging effects reversion in ways associated with editor type.  This explanation is dubious since Wikipedians are thought to be highly suspicious of anonymous editors.

However, if the availability of an algorithmic flag obviates the need for statistical discrimination against ``over-profiled'' editors, then Wikipedians may use only signals from the algorithmic flagging system instead of using algorithmic flags alongside identity-based characteristics.  This explanation is encouraging for it suggests that introducing algorithmic predictions into governance systems can reduce statistical discrimination.

\bibliographystyle{ACM-Reference-Format}
\bibliography{OresAudit.bib}

% \setcounter{biburlnumpenalty}{9001}
% \printbibliography[title = {References}, heading=secbib]

\end{document}

% LOCAL_WORDS: decile
