\documentclass[format=acmsmall, natbib=true, review=true, anonymous=true, screen=true]{acmart}

\citestyle{acmauthoryear}

%\usepackage{cdsc-memoir}        
% \usepackage{pdflscape}          %
% \usepackage{tikz}
% \usetikzlibrary{arrows}
% \usetikzlibrary{positioning}
% \usetikzlibrary{shapes}
%\usepackage{pgfgantt}

% there are two chapter styles: cdsc-article and cdsc-memo

% memo assumes that you remove the "\\" and the email address from the
% \author field below as well as that you will comment out the
% \published tag
%\chapterstyle{cdsc-article}
\usepackage[utf8]{inputenc}
\usepackage{wrapfig}
\usepackage{booktabs}
<<init, echo=FALSE, warning=FALSE>>=
library(knitr)
knit_hooks$set(document = function(x) {
  sub('\\usepackage[]{color}',
'\\usepackage[]{color}', x, fixed = TRUE)
})
opts_chunk$set(fig.path="figures/knitr-")
opts_chunk$set(dev='tikz')
opts_chunk$set(external=TRUE)
opts_chunk$set(cache=TRUE)
overwrite <- FALSE
source("resources/preamble.R")

@
\usepackage{subcaption}
\usepackage{tikz}
% \usepackage[T1]{fontenc}
% \usepackage{textcomp}
% \usepackage[garamond]{mathdesign}

% \usepackage[letterpaper,left=1.65in,right=1.65in,top=1.3in,bottom=1.2in]{geometry} 

% packages i use in essentially every document
\usepackage{graphicx}
\usepackage{enumerate}

% packages i use in many documents but leave off by default
\usepackage{amsmath, amsthm} %, amssymb}
%\usepackage{caption}
% \usepackage{dcolumn}
% \usepackage{endfloat}

% % import and customize urls
% \usepackage[breaklinks]{hyperref}
% \hypersetup{colorlinks=true, linkcolor=Black, citecolor=Black, filecolor=Blue,
%     urlcolor=Blue, unicode=true}

% list of footnote symbols for \thanks{}
% \makeatletter
% \renewcommand*{\@fnsymbol}[1]{\ensuremath{\ifcase#1\or *\or \dagger\or \ddagger\or
%  \mathsection\or \mathparagraph\or \|\or **\or \dagger\dagger
%   \or \ddagger\ddagger \else\@ctrerr\fi}}% \makeatother
% \newcommand*\samethanks[1][\value{footnote}]{\footnotemark[#1]}

% add bibliographic stuff 
\usepackage[american]{babel}
% \usepackage{csquotes}

%\usepackage[natbib=true, style=apa, backend=biber]{biblatex} 
%\DeclareLanguageMapping{american}{american-apa} 

% \defbibheading{secbib}[\bibname]{%
%   \section*{#1}%
%   \markboth{#1}{#1}%
%   \baselineskip 14.2pt%
%   \prebibhook}

% \def\citepos#1{\citeauthor{#1}'s (\citeyear{#1})}
% \def\citespos#1{\citeauthor{#1}' (\citeyear{#1})}

% memoir function to take out of the space out of the whitespace lists
% \firmlists

%p LATEX NOTE: these lines will import vc stuff after running `make vc` which
% will add version control information to the bottom of each page. This can be
% useful for keeping track of which version of a document somebody has:
% \input{vc}
% \pagestyle{cdsc-page-git}

% LATEX NOTE: this alternative line will just input a timestamp at the
% build process, useful for sharelatex
% \pagestyle{cdsc-page-sharelatex}

\hyphenation{social-psy-cho-lo-gi-cal}
%\newcommand{\oressource}{oresarchaeologist}
\newcommand{\TODO}[1]{{\color{red} TODO: #1}}
\newcommand{\todo}[1]{\TODO{#1}}
\newcommand{\oressource}{\oresdatabase}
\begin{document}

\setlength{\parskip}{4.5pt}
% LATEX NOTE: Ideal linespacing is usually said to be between 120-140% the
% typeface size. So, for 12pt (default in this document, we're looking for
% somewhere between a 14.4-17.4pt \baselineskip.  Single; 1.5 lines; and Double
% in MSWord are equivalent to ~117%, 175%, and 233%.

% \baselineskip 16pt

\title[Algorithmic flagging]{Algorithmic flagging and identity-based signals in online community moderation: a regression 
discontinuity analysis}
% \shorttitle{Algorithmic flagging and identity-based signals in online community moderation}
\author{Nate TeBlunthuis}

\date{\today}

% \published{\textsc{\textcolor{BrickRed}{This document is an
%   unpublished draft.\\ Please do not distribute or cite without
%   permission.}}}


% Discontinuous operating points create discontinuous behaviors. 
% New H1: The system will be used and thereby shape what gets quickly reverted.
% New H2: The system will be more fair those those who choose to display signals associated with low-quality (i.e. anonymous editors)
% New H3: Algorithmic flags act as salient signals that nudge moderators to make poor decisions.
% Alternatively: Flags provide useful information that can improve adherence to second-order norms.
% New H4: They especially improve such adherence for over profiled editors. 

\begin{abstract}
Online communities face the problem of scale stemming from the great human effort needed to monitor behavior and enforce rules and norms.  In efforts to scale up regulation work, platforms increasingly deploy automated triage systems using machine predictions to help moderators routing their attention using flags and filters.  We propose that when put into use, such systems can also make online community governance more fair to those who display signals associated with low quality.  Established approaches to community regulation involve the use of identity-based signals such as registration status and user profiles by moderators to direct their attention to actions by users seen as likely to cause problems. Such users are likely to be sanctioned, but may be ``over-profiled'' if moderators focus on them to the neglect of others.  Algorithmic systems make norm violations by all kinds of individuals, not just those displaying characteristics associated with low-quality more visible to moderators. We test our propositions in a system deployed on Wikipedia which displays algorithmic flags side-by-side identity-based signals associated with vandalism in interfaces for reviewing encyclopedia revisions. We use classification thresholds to estimate the causal effect of being flagged on sanctioning and find that flagged edits became more likely to be quickly reverted, especially edits by registered editors, and that the flagging increases fairness as reverts to flagged edits were less likely to be reverted themselves.


  % that affords ``statistical discrimination,'' as users with visible signals of negative quality are more likely to be sanctioned. 
  % We have to problematize identity-based signals.  What's wrong with them:
  % Individuals who pass through the filters are hard to monitor (h1)
  % They are inaccurate "salient signals" that can influence evaluation.
  % A more accurate "salient signal" should improve evaluation (

%  Similarly, under a hypothesis that identity-based signals and algorithmic flags function as ``salient signals'' that prime moderators to issue sanctions, we predict that flagging actions by ``over-profiled'' individuals will cause an increase in the proportion of sanctions that are controversial, but that this increase will be smaller than the corresponding increase for ``under-profiled'' individuals. 


  % Recently communities and platforms are increasingly adopting automated triage systems that filter or flag  content to support governance work. These systems work by shifting moderator attention toward content and contributions predicted to be damaging. 
%  We consider two mechanisms by which the design and use  of such technologies influences how participants are treated by governance workers.  First, automated triage systems direct attention toward problematic contributions by making them more visible and immediate in user interfaces. Second, these systems may nudge governance workers to see contributions as more dappmaging when they are flagged by an algorithm. 


  % members of a salient class are less sensitive to being flagged and therefore still subject to scrutiny on the  has a disproportionate or 

  % only routes attention or if it also nudges reviewer decisions. 

% Before these models are introduced, change reviewers not using specialized tools had signals about editors available to help them identify problematic edits: for a given edit, they can see whether the editor was logged in or ``anonymous'' (in which case their IP address is shown). inherent such designs has consequences for governance by shaping what contributions are identity reverted.  Such reviewers could also see if an editor's user page and user-talk page exist, which may signal whether an editor is experienced or has been warned.
% Scoring above the thresholds increases the chances an edit will be reverted. 
\end{abstract}

\maketitle

%We believe that understanding the design of algorithmic governance systems requires accounting for how and to what extent the system serves both the surveillence and nudging functions. We analyze an algorithmic triage system in the wild

% Too much emphasis on criminal justice and claiming that as a major part of the contribution is creating extra work. Don't do that unless you really need it.

\section{Introduction}

% Paragraph motivating the question: What are we concerned about why does it matter?% What's the CSCW problem we're working on? It's a popular topic right now. 

% I need to develop / focus my concepts of enforcement or monitoring work. Use ostrom?

% Bring in blackwell and bowker and star and mary douglas more here. Blackwell's important for signaling that this is CSCW work, not FAT*. 
% Salient signals go in the introduction, but the point must be that they signal membership in suspicious categories.  
% broaden the focus beyond online communities to broader questions of algorithmic governance --- including the criminal justice system.
% Bring back surveillence/visibility and profiling in the intro.
% don't make it about online communities!


% Algorithms and the problem of scale
Moderators of online communities and social media platforms review an often large quantity of user generated content and actions to address violations of norms and rules.  This ``problem of scale'' makes it difficult to review every action to find behavior that should be sanctioned \cite{gillespie_custodians_2018}. Increasingly, communities and platforms adopt algorithmic triage systems to direct moderators toward actions predicted to be problematic \cite{chandrasekharan_crossmod:_2019}, but community moderators also direct their attention according to identity-based signals of individual quality such as reputation, experience, or registration status \cite{broughton_wikipedia_2008,kraut_building_2012}.  We explore how algorithmic flags and identity-based signals interact in online community moderation through a evaluation of the RCfilters system on Wikipedia powered by ORES algorithm for edit quality prediction on Wikipedia \cite{halfaker_ores:_2019}.  


\begin{figure}[t]
  \centering
  \includegraphics[width=\textwidth]{resources/RCfilters_flags.png}
  \caption[Screenshot of edit metadata shown in RCfilters.]{Screenshot of Wikipedia edit metadata on Special:RecentChanges with RCfilters enabled.  Highlighted edits with a colored circle to the left other metadata are flagged by ORES.  Different circle colors (yellow and orange in the figure) correspond to different levels of confidence that the edit is damaging. Users can configure which colors are shown.   Visible identity-based characteristics of editors include registration status (i.e. whether a user name or an IP address is shown) and whether an editor's user page and user talk page exist.  RCfilters does not flag edits by new accounts, but does support filtering changes by newcomers.}
  \label{fig:rcfilters}
\end{figure}

% cite something not about wikipedia in this paragraph?
% Remember that algorithms can be profiling too 
Moderators often use algorithmic flags alongside other visible signs of quality in the form of user characteristics.  Drawing from legal philosopher Frederick Schauer's notion of ``profiling,'' ethicist Paul de Laat argues that characteristics like reputation and registration status become prone to ``overuse'' by moderators who may concentrate their attention on the activities on a narrow range of users \cite{de_laat_profiling_2016, de_laat_use_2015}. Similarly, economists describe ``statistical discrimination'' in which characteristics correlated with performance or deviance are used in making decisions \cite{bertrand_field_2016}.  To simplify language, we say that individuals with ``overused'' characteristics, who face statistical discrimination, are ``over-profiled'' and that other individuals are ``under-profiled.''  Importantly, the visibility of such characteristics to moderators is sufficient for ``over-profiling.'' Including them as predictors in an algorithm is not necessary.
 
De Laat specifically criticizes ``overuse'' of anonymity on Wikipedia because it increases the hazard that anonymous contributors will have their valid contributions rejected. Such barriers to contribution may limit community growth and diversity, as users with vulnerable identities may seek anonymity and blocking contributions from unregistered contributors can decrease positive contributions to peer production projects \cite{hill_hidden_2020, forte_privacy_2017}. % Keep in mind why these systems are valuable to moderators 
That said, overuse of identity-based characteristics such as experience levels, reputation, and registration status is instrumental for moderators to deal with the ``problem of scale'' and efficiently regulate online spaces \cite{gillespie_custodians_2018, de_laat_profiling_2016}.  

Another approach to the ``problem of scale''  is to use systems incorporating \emph{algorithmic triage} by using machine learning predictions to help support moderators sift through vast quantities of content \cite{gillespie_custodians_2018, chandrasekharan_crossmod:_2019}. 
Such systems are designed to make governance more efficient by routing moderator attention to behaviors predicted to violate rules or norms in ways that require moderator intervention.  

%This study adopts a sociotechnical systems perspective by considering not only the design of predictive models and user experience, but also the ``work process'' of moderator users \citep{smith_keeping_nodate}.  

Can such systems replace reliance on identity-based signals in community and platform governance?  Advocates of algorithmic risk prediction in criminal justice settings argue that algorithmic predictions can improve upon the discriminatory and inaccurate decisions of human judges \cite{kleinberg_discrimination_2019}.  Yet when moderators and judges can still see identity-based signals it is plausible that they will still use them in decision making.  We propose that algorithmic predictions will have less influence on outcomes for ``over-profiled'' individuals compared to ``under-profiled'' ones. In other words, our theory is that flagging an action by an algorithm will cause a greater increase in the likelihood of sanction for ``under-profiled'' individuals.  We also consider how competing theories algorithmic flags and identity-based signals shape the fairness of sanctioning.  Dual-process models of decision making suggest that algorithmic flags may nudge moderators in to issuing more unfair sanctions, especially against users that receive less moderator attention when not flagged. On the other hand, information from algorithmic flags can improve fairness by helping moderators avoid violating second-order norms. 

% There are two ways that predictions from an algorithmic governance tool can interact with group membership in monitoring work at scale. First, an algorithmic tool can amplify bias or increase unfairness if its predictions are positively correlated with group membership and both signals are given weight in decision making. Information from the predictive algorithm and group membership will ``add up'' to increase scrutiny on group members when the algorithm predicts their behavior is damaging.  This would occur, for example, in cases where algorithmic predictions can be used to rationalize profiling. 

% On the other hand, it is possible that algorithmic predictions \emph{substitute} for category memberships.  This will happen if the influence of an algorithmic prediction on decision making decreases the influence of a social category. In such cases, merely introducing algorithmic predictions can reduce systematic unfairness (assuming the algorithm is less biased than human decision makers).  

%"cues are created, propogated, and interpreted to become signals"
This paper reports our analysis of moderator behavior in the context of the ORES algorithm for edit quality prediction on Wikipedia and the RCfilters flagging and filtering user-interface that it powers \cite{halfaker_ores:_2019}.  As shown in Figure \ref{fig:rcfilters},  this system displays algorithmic predictions alongside visible indicators of membership in salient social categories for reviewing actions on the encyclopedia.  Similar to other designs for algorithmic triage systems with humans-in-the loop \cite[e.g.][]{chandrasekharan_crossmod:_2019}, flags are triggered when ORES' prediction confidence crosses arbitrary operating points or thresholds. Similarly, RCfilters enables users to view only that subset of edits above a threshold. These features allow a systematic statistical analysis of edits near to the threshold to provide causal inferences of the effect of algorithmic triage on moderation decisions. In addition, because algorithmic flags are presented to moderators alongside information about membership in categories associated with objectionable contributions, we can test predictions of our theories about how algorithmic flags will differently effect individuals with or without visible identity-based signals. 

We find that algorithmic flags reduce overprofiling of anonymous (IP) Wikipedia editors as flagging causes a greater increase in the likelihood of reversion for registered editors compared to IP editors.  We also find that flagging increases fairness of sanctioning for IP editors as reverts of their flagged edits are less likely to be sanctioned for violating second-order norms.  However, analysis of editors without profiles, a visible proxy of newcomer status,  does not support these conclusions. Our methods based on discontinuous operating points suggest that designers consider alternative approaches to surfacing model predictions in review interfaces. Our results suggest that designers of sociotechnical systems for online community and platform governance should consider how moderators may use available signals as they review user actions and that algorithmic triage can, at least in some cases, improve fair treatment of contributors with visible traits that may be discriminated against.   

% % we might have to do some more work to nail this. 
% We find that moderator actions in this context were substantially more sensitive to the algorithmic classification tool when editors were not group members.  This suggests that moderators still used group membership as a signal that contributions may be damaging.  In other words, we reject a hypothesis of pure substitution. However, we also observe a substantial algorithmic influence for members of these classes. This shows that algorithmic predictions can substitute for salient category memberships in surveillance and enforcement work. 

% We contribute to CSCW research on data surveillance and ethics \citep{chancellor_relationships_2019} by elaborating a theory of how algorithmic predictions interact with and salient social categories, providing a non-invasive methodology for identifying substitution between algorithmic predictions and salient social categories in naturalistic settings, which we use in a large-scale empirical test of our theory in a majoprr online platform for cooperative work.

%Are algorithmic tools that support governance work in online communities and social media platforms merely tools for efficient surveillance or do they also shape how content and communication are perceived by regulators?
% Ask the thought provoking questions

% In this paper, we analyze a case where the  %Will their adoption amplify scrutiny on users that resemble troublemakers or deviants or %of user that already %but shape the work that online  %, but will they harm or hurt newcomer retention, discrimination, but how will they  %experience of users who 

% Build the case that we should be suspicious of algorithms. No do that more  in the background section. Here we want to focus on our BIG IDEA. 


\section{Background}

\subsection{Governance in online communities} 

% Why regulate behavior? 
% Comment from overleaf: De Laat might help us make a normative argument, but we can also make it ourselves. 
Regulating behavior is a core task of online communities and social media platforms that requires moderation: ``governance mechanisms that structure participation in a community to facilitate cooperation and prevent abuse'' \cite{grimmelmann_virtues_2015, kraut_regulating_2012}.  Goals of regulation include preventing harassment, eliminating spam, combating misinformation, disinformation, and hateful ideologies,  compliance with requirements of the law or the platforms rules, keeping focus on the topic or purpose of the community, and maintaining the quality of content and outputs \cite{gillespie_custodians_2018}.  There are many possible devices by which communities pursue these goals including reputation systems, collaborative block lists \cite{blackwell_classification_2017}, documenting rules, or creating barriers to entry. 

Moderators are users who take responsibility for moderation work and can be organized in a variety of arrangements. Communities and platforms might have official paid or volunteers moderators and who potentially have special privileges. On the other hand, broadening participation in moderation work can help mobilize more people to contribute to moderation work.  Many platforms allow any user to report content to official moderators by flagging it \cite{crawford_what_2016}.   Sites like Slashdot and Reddit use forms of distributed moderation that aggregate judgements from many users \cite{lampe_crowdsourcing_2014}.  Wikipedia for instance, combines elements of distributed has many different formal roles such as ``administrators'' who can ban users and  ``patrollers'' who can edit more frequently and use some special tools (i.e. Huggle) for reviewing a large number of edits, any and user can contribute to moderation work by reviewing and undoing changes. 

% Governance mechanisms can be classified by whether they are proactive (i.e. systemsp that throttle activity, only publish approved content, or depend on privileges) or reactive (i.e. content is published and then moderated)  \cite{kraut_regulating_2012}.  We focus on reactive systems in this paper. Two common and interdependent reactive patterns in online community governance are sanctioning misbehavior and removing problematic content. 

%Comments from Charlie:
% Groomsman virtues of moderation for definition of moderation
% what kind of moderation are we talking about here? There's volunteer moderation and there is paid moderation. Our theory might apply to both, but we study a context of volunteer moderation where any user can contribute.

% % Define rules / normsg
% \subsubsection{Rules and norms}

%\TODO{explain governance probalems on Wikipedia in more depth.} We maybe don't need that. This isn't opensym.
\subsubsection{Sanctions}
% Define norm enforcement

Punishing and deterring bad behavior is a key task for community moderators.  Writing down rules does not imply that they will be followed.  The rules must be enforced through sanctions or acts that discourage misbehavior.  Removing content is a common form of sanctioning that communicates that a contribution was not wanted or appropriate.   \cite{halfaker_dont_2011} study practices of reverting changes on Wikipedia and found that people tend to make higher quality contributions after being reverted. Similarly, \cite{srinivasan_content_2019} found on Reddit that people whose comments were removed became less likely to violate norms. This further suggests that removing content can be an effective form of sanctioning.  

However, sanctioning can also discourage participation, particularly by newcomers. 
New participants are likely to violate rules and norms and therefore be subject to sanctions, but sanctioned newcomers are less likely to continue participating.
This mechanism helps explain declines in Wikipedia participation and in many other online communities, and may be an obstacle to building a community that includes diverse participants  \cite{halfaker_rise_2013, teblunthuis_revisiting_2018, lam_wp:clubhouse?:_2011}.   Efforts to ameliorate this dynamic include better socialization of newcomers to help them learn community rules and norms, but these can be difficult to institute \cite{narayan_wikipedia_2017, morgan_evaluating_2018, halfaker_snuggle:_2014}. 

% Creating barriers that slow participation is a second approach to maintaining order by intentionally limiting growth \cite{kiene_surviving_2016, lin_better_2017}. But in peer production communities like Wikipedia, barriers to growth may also constitute barriers to expanding the quality of diverse knowledge and knowledge-producers \cite{lam_wp:clubhouse?:_2011}. 

%Charlie suggests citing Shagun in the intro.
Sanctions can be controversial when norms are contested or when enforcement is inconsistent or unaccountable \cite{blackwell_classification_2017, crawford_what_2016}.   Improving fairness in sanctioning might help ameliorate the negative effects of sanctioning on community growth. 
\cite{chang_trajectories_2019} study established Wikipedia editors who were  blocked and examined their communications for signs that they believed the block to be fair and found that they were more likely to return to regular participation and not receive another block when they seemed to believe the block was fair.
Approaches to improving the quality of moderation actions include
Slashdot's use of ``meta-moderators'' to review moderation decisions  \cite{lampe_slashdot_2004}. 

% community development as online communities face a dilemma between regulating behavior and attracting participants \citep{teblunthuis_revisiting_2018, halfaker_rise_2013, halfaker_dont_2011}.  

% \cite{halfaker_rise_2013} found that newcomers to Wikipedia were less likely to continue contributing to the encyclopedia after being sanctioned and \cite{teblunthuis_revisiting_2018} replicated this finding in a population of other Wikis.

% This paper contributes to understanding how algorithmic and identity-based signals are related to the fairness of sanctioning by analyze how effects of algorithmic flagging on fair sanctioning differ between groups of users with varying identity-based signals. 

\subsubsection{The problem of scale}
% Problem of scale in norm enforcement
Governance in online communities face a ``problem of scale'' in the challenge of sifting the great mass of comments, posts, or encyclopedia edits to identify objectionable content or behavior \citep{gillespie_custodians_2018}.   Distributed moderation can help communities scale and promote deliberation \citep{lampe_crowdsourcing_2014}, but the high levels of participation in such moderation work required to create orderly spaces can be difficult to sustain \citep{gilbert_widespread_2013}.  With growing attention to problems of disinformation and hate speech online, commercial platforms are expanding their pools of paid human moderators, but the work of paid moderators can be exploitative, difficult, traumatizing, and expensive \cite{roberts_commercial_2016}.  Moderation in contexts that face the problem of scale is likely to be stressful work involving a large number of judgment calls, often ambiguous, that must be made quickly. 

Visibility is an important part of the problem of scale.  For moderators to sanction behavior, they must first observe it. Flagging provides a tool for users to report activities to official moderators and helps platforms defend their enforcement actions. Flagging helps solve the problem of scale because flagged actions are made visible to moderators and thereby directs attention to actions more likely to be problematic.    One disadvantage of flagging is that users can organize strategic to overwhelm moderators or to target opposing viewpoints \cite{crawford_what_2016}. Like other forms of distributed moderation, it depends on the collective efforts of volunteers to review content and report violations.  So community-driven flagging may not be sufficient to detect and sanction a large proportion of offenses. 

Automatic triage systems that use predictions from a machine learning model to flag and filter content may be less prone to strategic flagging, and may provide better coverage of problematic content.  Some systems use algorithms that automatically remove content like the PhotoDNA system which automatically removes child pornography \cite{gillespie_custodians_2018} and ClueBot on Wikipedia which uses a machine learning predictor to automatically remove obvious vandalism \cite{geiger_when_2013}.  However the accuracy of these systems on less clear-cut kinds of misbehavior remains insufficient to solve the problem of scale by automatically removing content \cite{gillespie_custodians_2018}. Furthermore, in user-organized communities, moderation decisions are an important part of building shared meaning, a task not easily left to a fully automated system \cite{seering_moderator_2019}.

% Expand to include other designs of algorithms for detecting norm violations or misbehavior
However, this study focuses on settings where an algorithm might flag content to make it visible to a human who can make an enforcement decision.  For example, Reddit allows moderators to define a system of rules based on regular expressions to automatically remove or flag content \cite{jhaver_human-machine_2019}. Applied machine learning research endeavors to predict deviant behavior in online communities such as \cite{wulczyn_ex_2017} who automatically classify harassing behavior on Wikipedia and \cite{liu_forecasting_2018} who predict when conversations on Instagram will turn hostile, but provides little guidance for deploying such systems in practice.  In constrast, \cite{chandrasekharan_crossmod:_2019} developed a practicable system for communities on Reddit to share information and collaborate on automatic flagging and account for differences between the rules of different communities.   Similarly, \cite{halfaker_ores:_2019} developed the Object Scoring Evaluation Service (ORES) system, which provides models to predict quality of contributions and content on Wikipedia.


\subsubsection{Sociotechnical evaluation of algorithmic systems}
%\TODO{Split this paragraph in two so that we have one paragraph about the need for sociotechnical evaluation and merge the rest with the methods.} 
This project applies regression discontinuity analysis as a non-interventionist method for evaluating systemic effects of algorithmic systems that are deployed and used in consequential settings.  Considerable attention to the ethics and usability of algorithmic systems in the machine learning community aims to provide more transparent or ``fair'' predictors with a focuses on statistical and optimization problems, but from the perspectives of sociotechnical systems and value-sensitive algorithmic design it is important to expand the scope of design and evaluation to consider the user experience and how the introduction and use of new technologies interacts with social structures and shapes work processes \citep{selbst_fairness_2019, zhu_value-sensitive_2018}.  But the difficulty of simulating naturalistic structures and processes raises trade-offs between the ecological and construct validity and research ethics \citep{mcgrath_methods_1984}.  Natural experiments like A/B tests can provide ecological validity and support causal claims, but are ethically troubled as users can be subjected to intervention without consent \citep{lane_big_2015, jouhki_facebooks_2016}.  While even observational studies of social media can raise concerns and violate user's privacy expectations \citep{boyd_critical_2012,fiesler_participant_2018}, in settings such as Wikipedia, editing behavior is generally considered public and open to scrutiny. Our method can comply with these expectations while supporting causal claims subject to defensible assumptions.  


% Define flagging and filtering
% This section needs more special attention. 
\subsection{Discrimination and filtering}

%Mako says "I think you should start with the bigger theory and explain how it's relevant to Wikipedia (via de Laat, e.g.).  He means starting broad and zooming in.
Profiling based on identity based signals may lead to statistical discrimination on the basis of those signals. Economists of discrimination distinguish between   taste-based and statistical discrimination \citep{bertrand_field_2016}.  Discrimination is when authorities treat deferentially treat individuals based on membership in a group or identity. ``Taste-based'' discrimination is driven by preferences for members of one group or identity including ideological racism and implicit bias. But discrimination can also happen because identity-based signals are instrumental to improving the quality of decisions.  Such ``statistical discrimination'' may still lead to unequal outcomes, but might be justified in cases where differential treatment may be worth the price of expediency  \citep{bertrand_field_2016}. 

%Taste-based and statistical discrimination can be difficult to tell apart in real-world empirical settings, but field experiments can help.  \citep{bertrand_field_2016}. \cite{bertrand_are_2004} conducted an audit study in a labor market. They applied for jobs using resumes of simulated job applicants with either high or low levels of  experience level and either white or black sounding names.  They observed racial discrimination as white applicants were much more likely to receive an interview invitation compared to black applicants.  Now, under a hypothesis of statistical discrimination, additional information about experience levels should reduce reliance on race as a signal of performance, and the gap between white and black applicants should decrease within the group of high quality resumes. However, \cite{bertrand_are_2004} found the opposite --- the gap between white and black sounding applicants was greater in the group of high-quality resumes.  Taste-based discrimination is a plausible mechanism for this finding as more information about applicants amplified rather than decreased the gap as predicted by statistical discrimination, but it is difficult to rule out alternative explanations. 

Proponents of algorithmic governance systems in the legal system argue that such systems can reduce discrimination by replacing reliance on identity-based signals like race with algorithmic predictions that are more accurate than judicial decisions \cite{kleinberg_discrimination_2019}.  However, introducing algorithmic predictions to governance systems does not on its own obscure identity-based signals.  Thus it is important to consider how judges or moderators will use an algorithmic predictor along side identity-based signals in practice.  At one extreme, an algorithm might obviate the usefulness of statistical discrimination if relying on the algorithm is always more effective than looking to an identity-based signal.  On the other hand, introducing algorithmic predictions should be of little consequence to taste-based discrimination. 

% Probably the degree to which algorithms substititute for identity is a function of the quality of the algorithm, how much users trust it, and how much discrimination is taste-based vs statistical. 

% A group is discriminated against when a relevant For example, a judge discriminates against black defendants if they are less likely to be released on bail than apparently identical defendants of a different race. That said, there are multiple mechanisms that may lead to patterns of discrimination. ``Statistical discrimination'' would occur if the reason the judge discriminates is that the judge knows that, all else being equal, black defendants are less likely to appear in court.  In this case the judge is discriminating because doing so advances the judge's goal of carrying out an efficient and orderly judicial process.  However, the judge's discrimination might instead be attributable to ideological racism, or a ``taste'' disfavoring releasing black defendants \citep{bertrand_field_2016}. The distinction between taste-based and statistical discrimination is salient because statistical dissemination might be considered an acceptable form of differential treatment between groups, particularly if historical oppression is not a factor, as in discrimination against newcomers in regulating an online community.  Indeed we think that statistical, but not taste-based discrimination against new and anonymous contributors is likely in online communities.

% consider deleting this paragraph entirely
In the context of Wikipedia, \cite{de_laat_profiling_2016} adopts the concept of ``profiling'' from legal scholar Frederick Schauer to argue that displaying identity-based signals like registration status or experience levels in interfaces for reviewing changes or in algorithmic governance tools on Wikipedia may be unethical or at least inconsistent with Wikipedia's founding principles.  Similar to statistical discrimination, de Laat contends that such signals are prone to ``over-use''  as moderators are much more likely to scrutinize types of contributors who may have legitimate reasons for editing anonymously or editing through a new account. We modify de Laat's vocabulary to call such editors ``over-profiled.'' On the other hand other kinds of editors will be ``under-profiled'' as their contributions may be less likely to come under scrutiny.

Assuming that community moderators are using an algorithmic flagging system to find actions that merit sanctioning, when the system flags an action, that will increase the likelihood that a moderator responds with a sanction.  However, the  magnitude of the increase will depend on the answer to the counterfactual question: ``what would have happened if the action had not been flagged?''  We think the answer will be different between over-profiled and under-profiled individuals. Mainly, actions 
of over-profiled individuals are likely to face scrutiny even when not flagged by an algorithm, but actions of under-profiled individuals are only likely to face scrutiny when they are flagged.

We think that statistical discrimination is very likely to occur against new or unregistered participants in online communities with cheap pseudonyms. Cheap pseudonyms make it easy for rule breakers to evade sanctions by creating new accounts \cite{friedman_social_2001}.  Therefore, new accounts are suspect and likely face more scrutiny.  John Broughton advises would-be vandal fighters on Wikipedia to ``consider the source'' of an edit when ``estimating the likelihood that an edit is vandalism'' and highlights Anonymous edits and red links to user talk pages, reasoning that user accounts without talk pages are probably new \cite{broughton_wikipedia_2008}.  Similarly, moderators on other communities might use characteristics such as the number of posts to a forum, or reputational signals such as karma on Reddit or achievements on StackOverflow.  When visible and salient to moderators, such information will can over-profiling of editors who lack such positive signals or display signals associated with misbehavior.

We propose that algorithmic flags can reduce reliance on such signals in online community governance. Since actions of over-profiled individuals are likely to be scrutinized even when they are not flagged by an algorithm, we expect algorithmic flagging to play a relatively smaller role in moderation of their actions. On the other hand, actions by under-profiled users are unlikely to attract moderator attention unless flagged by the algorithm.  Therefore, when a piece of content is flagged by an algorithm, the increase in the likelihood that a moderator responds with a sanction will be smaller for actions made by over-profiled users compared to under-profiled users. Thus our first hypothesis is:
% ISN'T the absence of a signal a signal of positive quality?

\textbf{H1:} Flagging an action causes a greater increase in the likelihood the action is sanctioned when the action is by an under-profiled individual than when is by an over-profiled individual.
 
Next we consider how algorithmic flags shape the consistency of sanctioning for over-profiled editors.  That is, will flagging help or hurt fair treatment of these editors by changing the fairness of the application of sanctioning?  Answering this question requires first a definition of what it means for a sanctioning action to be fair.  Piskorski and Gorbattai use the concept of ``second-order norms'' to describe norms that govern enforcement behaviors related to first order norms \cite{piskorski_testing_2017}.  In the case of Wikipedia, a first-order norm governs right and wrong ways of editing Wiki pages.  Sanctions of perceived first-order norm violations are themselves governed by second-order norms about what sorts of edits merit reverts.  This notion of ``second-order norms'' provides a concept and measure of the fairness of sanctions applied to first-order norm violations derived from observable behaviors reflecting what community members consider to be fair with emic validity in the case of Wikipedia \cite{piskorski_testing_2017}.    

We consider two alternative competing theories of how flags will shape the consistency of first-order norm enforcement. First, we consider theories from dual process models of behavioral economics that predict that for rapid decision making in conditions of uncertainty, or where finding and analyzing the information necessary to arrive at correct decisions is difficult, people tend to rely on ``salient signals''  \citep{bordalo_salience_2012, kleinberg_human_2018, tversky_judgment_1974}.  When moderators use aspects of user identity such as account age, registration, experience or reputation are to choose what contributions to review or whether to sanction behavior, these attributes serve as salient signals.

% important term related to salient signal is "cue"
We think that it is plausible that moderators will use algorithmic flags as salient signals and therefore sanctions against flagged actions will be less accurate and more likely to violate second-order norms. When an action is flagged, a moderator will be suspicious of it and may act conservatively to sanction even if the decision is uncertain. The flag suggests to the moderator that the action is problematic. We hypothesize that the increase in sanctioning caused by flagging an action will also lead to an increase in the proportion of sanctions that receive sanctions themselves (controversial sanctions). 

\textbf{H2a:} Within the set of sanctioned actions by over-profiled editors, flagging an action causes an \emph{increase} in the likelihood that it receives a controversial sanction.

Alternatively, we think that it is plausible that flags will instead \emph{decrease} second-order norm violations.  As noted above, the Wikipedia community is governed not only by norms about right ways of editing articles, but also by higher-order norms about right ways of constructing or enforcing first-order norms.  Wikipedia moderators have an interest in complying with these norms so information sources that help them regulate behavior while avoiding sanction for violating second order norms should decrease controversial sanctioning.  A tool that increases the proportion of damage that can reverted in line with second-order norms will decrease violation of such norms. 

\textbf{H2b:} Within the set of sanctioned actions by over-profiled editors, flagging an action causes a \emph{decrease} in the likelihood that it receives a controversial sanction.

Finally, similar to \textbf{H1}, we hypothesize that using algorithmic flagging alongside identity-based signals will partly, but not entirely, reduce reliance on identity-based signals.  Under the logic of dual-process models, both types of signals can serve as salient signals nudging moderators to issue sanctions.  But a substitution effect between identity-based signals and algorithmic flags can imply that algorithmic flags will be less salient for users that are already over-profiled.  If we also accept that such nudges increase the risk of second-order norm violations then we expect that:

\textbf{H3:} Within the set of sanctioned actions, the effect of flagging an action on controversial sanction will be more positive for under-profiled individuals than for over-profiled individuals.

\section{Data and measures}

%when was it introduced?
The RCfilters system on Wikipedia is a relatively new tool for monitoring changes to Wikipedia (edits). It provides flagging and filtering according to algorithmic triage flags, a limited set of editor characteristics, and other metadata fields. RCfilters stands for ``Recent Changes filters,'' signaling the special page on Wikipedia for observing the latest edits, \footnote{\url{https://en.wikipedia.org/wiki/Special:RecentChanges}} but RCfiters flags and filters are also on users' watchlists, which show edits to pages the user has followed. 
Figure \ref{fig:rcfilters} shows highlighting and flagging in the RCfilters interface.

% 
Algorithmic flagging in the RCfilters system is powered by the ORES edit quality models trained to predict whether edits are labeled ``damaging'' or ``not damaging.'' 
The models are gradient boosted decision trees trained on a mixture of human labeled Wikipedia edits and other edits made by established editors that are assumed to be ``not damaging.''   It is important to note that ORES models do not merely reproduce profiling patterns typical of moderation on Wikipedia.  The interface for labeling training data obscures identity-based signals from the  volunteer Wikipedians doing labeling work and the models are predictive of damage from users that are not anonymous or newcomers. 
For more information on the design and implementation of ORES see \cite{halfaker_ores:_2019}. 

An edit is flagged by RCfilters flags if and only if the continuously valued risk score output by the ORES model exceeds a threshold, formally called an operating point.  RCfilters uses multiple operating points corresponding to green, yellow, orange, and red flags.  By default only orange and red flags are shown, but users can configure which colors to display in edit review tools. Green flags and filters are to help Wikipedia editors find good edits  As we are interested in flagging for the purposes of finding damaging edits we consider them no further.  Red, orange, and yellow correspond to thresholds making different trade-offs between precision (the proportion of flagged edits that are truly damaging) and recall (the proportion of truly damaging edits that are flagged).  Red corresponds to a high precision threshold and edits flagged in are labeled ``very likely damaging.'' Orange flags corresponds to a ``likely damaging'' label with greater recall, but less precision compared to red, and edits with yellow flags are ``maybe damaging'' with a high recall and lower precision.  A special page displays the thresholds and their corresponding levels of precision and recall.  Figure \ref{fig:ores_thresholds} shows this page for English Wikipedia \footnote{\url{https://en.wikipedia.org/wiki/Special:OresModels}}.
  
\begin{figure}[t]
  \centering
\includegraphics[width=0.7\textwidth]{resources/Ores_Thresholds.png}  
  \caption[Screenshot showing RCfilters thresholds for English Wikipedia.]{Screenshot of Special:OresModels from English Wikipedia showing levels of precision and recall corresponding to different flags in RCfilters.}
  \label{fig:ores_thresholds}
\end{figure}

It is not obvious that algorithmic filtering in RCfilters has any substantial influence on Wikipedia governance as algorithmic filtering features are not enabled by default and must be enabled in user preferences.  Therefore, we will present a preliminary analysis that shows that these tools were adopted by demonstrating an overall causal effect of flagging on sanctioning after presenting our methods.  First we will describe our other measures.

\subsection{Sanctioning}

% cite some more stuff that uses reverts and sanctioning.
% Should we mention Twinkle?
\emph{Identity reverts} are our measure of sanctioning.  Identity reverts are a common measure of contribution rejection on Wikipedia, entail undoing an edit to by restoring a page to an earlier state, and are straightforward to measure by comparing hashes of page revisions  \cite{halfaker_dont_2011}.  That said, identity reverts are an imperfect measure of sanctioning.  A type of vandalism called ``blanking'' removes all content on a page and therefore identity reverts all prior edits to the page. It is also possible for an individual to ``self-revert'' by undoing their own edit.  To help mitigate such issues, we only label revisions as \emph{reverted} if they were undone within 30 days and were not undone by self-reverts and we label revisions as \emph{not reverted}  otherwise.
\subsection{Controversial Sanctioning}

We follow \cite{piskorski_testing_2017} by considering identity reverts that are subsequently reverted by a third party as sanctions of second-order norm violations.  We label a sanction as \emph{controversial} if the sanction is undone by a third editor who was not the original editor or the reverting editor.  Such interactions likely correspond to cases in which a third part observes the initial revert, disagrees with the initial sanction, and then acts to reverse the sanction.

\subsection{Identity-based signals}

As shown in Figure \ref{fig:rcfilters}, the RCfilters interface includes metadata with two key identity-based signals: whether the editor who made the change was logged into a registered account and whether or not the editor is new enough to have not yet created a "user page."   A red link to a user talk page may turn blue when a vandal is warned, and so may be a less reliable signal of newness compared to a red user page link. So we consider user talk pages here. These editor characteristics are well known in the Wikipedia community as suggestive that an edit is questionable.


\emph{IP editors} are individuals editing Wikipedia without logging in. 
IP editors are individuals who may not have a registered account, or may choose not to log in when making an edit for any reason.  Also called ``anonymous,'' such editors are associated with misbehavior have long had a controversial status on Wikipedia.  \cite{geiger_work_2010} describes how tools for moderators highlighed IP editors and how such edits are often scrutinized, and \cite{de_laat_profiling_2016} described such editor characteristics as prone to ``overuse.''  Online collaboration platforms understand that anonymous users are likely to violate norms and make low quality contributions \cite{mcdonald_privacy_2019}.  Recently, concerns about privacy and vandalism related to the use of IP addresses for edit attribution sparked discussions about alternatives, including proposals to ban anonymous editors from creating pages or even to eliminate anonymous editing entirely.\footnote{see \url{https://meta.wikimedia.org/wiki/Talk:IP_Editing:_Privacy\_Enhancement\_and\_Abuse\_Mitigation}}
% https://en.wikipedia.org/wiki/Wikipedia:Editors_should_be_logged-in_users_(failed_proposal)
% https://en.wikipedia.org/wiki/Wikipedia:Disabling_edits_by_unregistered_users_and_stricter_registration_requirement
% https://en.wikipedia.org/wiki/Wikipedia:IPs_are_human_too

That said, communities such as Wikipedia may wish to allow anonymous contributions due to the benefits anonymity may provide.  Anonymity may help diversify participation as those who face targeted harassment based on their identities are likely to seek anonymity \cite{forte_privacy_2017}. Anonymity may also increase productive contribution by removing the frictions of creating an account or logging in  \cite{mcdonald_privacy_2019}. Wikis on other platforms have disallowed unregistered editing, resulting in a decrease in norm and rule violation, but also a decrease in beneficial contributions \cite{hill_hidden_2020}.

\emph{Newcomers without user pages} are a second class of editor with identity-based signals visible in the metadata in RCfilters.   De Laat uses the existence of a user page as an example of an indicator of vandalism that may be prone to overuse \cite{de_laat_profiling_2016}. User pages are places on Wikipedia for editors to create profiles and it is normal for experienced editors to create their user page, so lacking a user page is a good sign that an editor is inexperienced.   The metadata on edit reviewing interfaces links to the user page of the edit in the text of the editor's name.  For example in Figure \ref{fig:rcfilters} edits by users ``Llavoro'' and ``MilovanPa'' are shown and their users names are colored red.  Red links are widely understood to link to wiki pages that do not exist, so seeing a red link account in Recent Changes metadata is a clue that the editor is new.

We identify whether a user's user page exists by matching the titles of user pages against the editor's user name and checking if the creation of the user page was prior to the edit in question.  


\subsection{Data: Wikimedia History}
% 
We build our dataset from two publicly available tables of Wikimedia history maintained by a team of data engineers at the Wikimedia foundation by running spark scripts on the Wikimedia analytics cluster.\footnote{Documented at \url{https://wikitech.wikimedia.org/wiki/Analytics/Data\_Lake/Edits/Mediawiki\_history}} \footnote{see \url{https://dumps.wikimedia.org/other/mediawiki\_history/readme.html}} We also use an internal Wikimedia foundation database that logs the scores from the ores models. Although Wikipedia is published and collaborated on in many languages, the vast majority of knowledge about collaboration on Wikipedia is derived from studies of English Wikipedia alone \cite{hecht_tower_2010,hara_cross-cultural_2010}.  Therefore, we aim to be inclusive by analyzing data from all 21 language editions of Wikipedia where edit quality flags are displayed in the RCfilters interface. 

For all of our analyses, our unit of analysis is the \emph{revision}, representing an edit to a page by a participant on Wikipedia.  Since we care about how algorithmic flagging and identity-based signals are used by human moderators, we limit our analysis to actions taken by humans by excluding revisions by bots.  We exclude wikis with less than \Sexpr{min.obs.per.wiki.threshold.cutoff} edits above and below each threshold.  

%This means that different wikis may be included in different models. For each model we report the quantity of edits from each wiki and how many fall on either side of the thresholds. 

We analyze a stratified sample to allows us to keep the total size of our dataset manageable while providing adequate statistical power from the diversity of Wikis and editor types we wish to analyze. We stratify by \emph{wiki}, by whether the editor is an IP editor or not, by whether the editor has a user page or not, by whether an edit was reverted in 2 hours, 48 hours, or 30 days, and by whether the revert was controversial. Most Wikipedia edits comply with norms, and accordingly the ORES scores are left-skewed, therefore we also stratify our sample by the decile of the ORES scores. We sample up to \Sexpr{strata_sample_size} edits from within each strata.  Stratified sampling introduces a known bias in our sample and we correct for this bias using sample weights throughout our analysis.  RCfilters powered by ORES damaging models were introduced to different wikis at different times, but we wish to estimate the average effect for edits to any of the wikis in our sample.  Therefore, while we must sample only edits following the introduction of ORES, we weight our sample according to the number of edits to each wiki over the entire study period.   The number of observations sampled and the total weight assigned to each Wiki for each model and threshold are available in the supplementary material. 

% Paragraph summarizing how ores was trained and routing people to halfak's preprint.

% briefly describe the release of the feature and what it takes to turn it on. 
%Prior to the development and release of RCfilters,  tools with features such as algorithmic flagging or filtering by user characteristics were available in special interfaces such as huggle.  None of the above  

\subsection{ORES edit classifier damaging scores}

To know whether an edit was flagged in RCfilters, we need to obtain the ORES score that was assigned to the edit and the thresholds that were active at the time the edit was reverted.  We obtain historical ORES scores from a log maintained by the Wikimedia foundation.
%for each wiki from the public mirror of the ORES scores database hosted by the Wikimedia foundation's quarry service.  


\subsection{RCfilters thresholds}

The thresholds that trigger RCfilters flagging are not constant, but depend on the precision and recall of deployed ORES models, and have also been changed in response to community feedback.  Since new models were deployed during our study period, scraping the page where the active thresholds are displayed would not provide the correct thresholds that were in use when an edit was made or that moderators reviewing changes would observe.

Fortunately, the configuration determining the thresholds, the trained ORES models, the code to run them are open source, and the exact time that changes are deployed is published at the Wikimedia foundation's server admin log.  So we wrote a script to combine this information to determine the precise thresholds that were active for each edit.

\section{Analytic plan \label{sec:analytic.plan}}

We test our hypotheses using a regression discontinuity design (RDD) for causal estimation of the effect of flagging an action on sanctioning (for \textbf{H1}) and controversial sanctioning (for \textbf{H2} and \textbf{H3}).

RDDs are an increasingly popular approach for causal inference in natural settings in economics because they resemble a randomized control trial for data points in the neighborhood of a discontinuity \cite{lee_regression_2010}.  RDDs model an outcome $Y$, as a function of a continuous ``forcing variable'' $Z$, other covariates $X$, and a cutoff $c$ such that $Z>c$ determines treatment assignment.  In principle treatment assignment conditional on $Y$ is ``as good as random'' under two assumptions: (1) that agents have at most limited control over $Z>c$ and (2) that the relationship between $Y$ and $Z$ is smooth.   If the assumptions hold then causal inference is simplified to the problem of statistically conditioning on the forcing variable $Z$ using a linear regression in the neighborhood of the cutoff $c$ (defined by $[c-\rho,c+\rho]$, for a ``bandwidth'', $\rho$) \cite{lee_regression_2010}.  

In the social computing,  \cite{narayanan_all_2019} and \cite{hill_hidden_2020} use within-subjects designs similar to RDDs to analyze sociotechnical consequences of policy and design interventions for online communities.  Both studies use time as a forcing variable which threatens validity as the timing of intervention may be influenced by unobserved factors, which would violate assumption (1).  Our treatment, being flagged in RCfilters, is a good candidate for an RDD from the perspective of assumption (1).  Editors are unlikely to have much control over the scores that their edits receive.  While attempts to evade sanction by specially crafting edits to evade algorithmic detection may be possible,  we do not think they will be wide-spread.

Assumption (2) would be violated if unobserved treatments effecting our outcomes occur at discrete levels of ORES scores.  For example, if another moderation tool is triggered by the ORES damaging scores, the effects of usage of that tool on our outcomes would confound our analysis of RCfilters.  This is a realistic scenario that is part of the design of the ORES system which makes scores available via API so that community members can use them to power their own tools.  We are aware of bots that automatically revert edits and are triggered by the ``very damaging'' threshold on some of the Wikis in our sample. Since we exclude reverts by bots from our analysis these bots are not a threat.  We are also aware that Huggle, a tool for reviewing encyclopedia edits incorporates ORES scores as a feature in it's own models for detecting damage.  However, since the ORES scores are not the only feature in the Huggle models, it is unlikely that thresholds in Huggle will constitute discontinuities in the relationship between ORES scores and our outcomes.  As a robustness check against threats to assumption (2) we conduct ``placebo tests'' by running our analysis at artificial cutoffs not equal to the real thresholds.  We present results of this robustness check in the supplementary material.

%(see \cite{chancellor_thyghgapp:_2016} for an example of evading content moderation through lexical variation in social media) we do not think this will be wide-spread or successful on Wikipedia. 

% 3 * 2 * 2 = 12 
\newcounter{equationcnt}
\newcounter{figuretmp}
\setcounter{figuretmp}{\thefigure}
\setcounter{figure}{0}

We present results from a total of 9 logistic regression models.  For \textbf{H1} and
\textbf{H3} we fit separate models for IP editors, non-IP editors, editors with user pages and editors without user pagers and for \textbf{H2} we model all editors. 
We incorporate the three RCfilters thresholds that we analyze in each model following the example of \cite{litschig_impact_2013}.  Our goal is to estimate ($\tau_j$) the causal effect of being flagged at level $j$ where $j \in \{1,2,3\}$ corresponding to labels of ``maybe damaging'', ``likely damaging'' and ``very likely damaging''.  
For each cutoff $(c_{jw})$, we select all revisions $r$ to wiki $w$ that fall within radius $p$ such that $c_{wj}- p < score_{r} < c_{jw} + p$ where $score_r$ is the output from the ORES classifier.  Following established approaches to RDD, we fit ``kink'' models that have a change in slope at the discontinuity \cite{lee_regression_2010,litschig_impact_2013}. Equation \eqref{eq:rdd_reverted} shows our specification for our models (the only differences between our models are the dependent variables, $Y$ and the type of editor whose edits are modeled.)

\begin{figure}
\renewcommand\figurename{Eq.}
%\begin{small}
\begin{equation*}
    \begin{split}
            P(Y_{rw}) & = \left[ \tau_1 \mathbf{1} [score_{r} > c_{1w}] + \alpha_{10}(score_{r} - c_1) + \alpha_{11}\left(score_{r}-c_{1w}\right) \mathbf{1} [score_{r} > c_{1w}]\right]\mathbf{1_{1p}}  \\
        & + \left[ \tau_2\mathbf{1}[score_{r} > c_{2w}] + \alpha_{20}(score_{r} - c_2) + \alpha_{21}\left(score_{r}-c_{2w}\right)\mathbf{1}[score_{r} > c_{2w}]\right]\mathbf{1_{2p}}  \\
        & + \left[ \tau_3\mathbf{1}[score_{r} > c_{3w}] + \alpha_{30}(score_{r} - c_3) + \alpha_{31}\left(score_{r}-c_{3w}\right)\mathbf{1}[score_{r} > c_{3w}]\right]\mathbf{1_{3p}} \\
        & + \sum_{j=1}^3B_j\mathbf{1}[seg_{j-1} < score
        \le  seg_{j}]\mathbf{1}_{jp} + \alpha_w + \mu_{rw}
    \end{split} 
\end{equation*}
\begin{equation*}
    \begin{split}
     \mathbf{1_{jp}} & =  \mathbf{1}[c_{wj}(1-p) < score_{rw} < c_{jw}(1+p)]  \\ 
     j=1,2,3; &  ~~p=0.05
    \end{split}
\end{equation*}
%\end{small}
\caption{Specification of locally linear logistic regression model for a regression discontinuity design with three cutoffs.  $\mathbf{1}$ is the indicator function. \label{eq:rdd_reverted}}
\end{figure}    

\setcounter{equationcnt}{\thefigure}
\setcounter{figure}{\thefiguretmp}
% % We conduct placebo tests to 


We use Bayesian inference to estimate our models for two reasons.  First, within some of the wikis we analyze virtually all edits above the ``very damaging'' level are reverted.  This ``separation'' is a problem for classical estimation approaches as coefficients head to infinity \cite{allison_convergence_2004}. Preferred solutions in the classical framework include penalized likelihood methods that introduce bias.  Our Bayesian approach uses weakly-informative priors that pull our estimates slightly toward zero, but does not have the problem of separation.  This leads to a conservative analysis less likely to result in false discovery.  We fit our models using the rstanarm package (version 2.19.2) and the default priors which are chosen to be weakly informative and which we provide for reference in the supplementary material. 

The second reason we choose to use Bayesian inference is that it considerably simplifies our analysis.  Our hypotheses compare effects of flagging between different types of editors.  Testing them in a classical framework can be done by fitting a joint model including all editor types and conducting a Wald test.  In a Bayesian framework, we can sample parameter estimates from the posterior distribution and test our hypotheses using statistical tests for differences between these samples \cite{morey_fallacy_2016}. Prior work in \cite{gan_gender_2018} makes a similar rationale for a Bayesian approach. 

In Bayesian analysis, fitted models take the form of \emph{posterior distributions} constituting a probability distribution of model coefficients conditional on our model, data, and priors.  We consider a hypothesis supported if it is consistent with at least 95\% of posterior draws. In other words, we accept our hypotheses if our parameter estimate has the predicted sign and the 95\% credible interval ($95\%~\mathrm{CI}$) does not contain zero.

\subsection{Presentation of marginal effects plots}

To assist readers in interpreting our results, we provide marginal effects plots for each models (e.g. \ref{fig:adoption.me}). These plots visualize the modeled relationship between ORES scores and the probability that an average edit in our sample is reverted (in the case of H1) or that a revert is controversial (for H2; H3) in the neighborhood of thresholds that trigger flags.  In each such plot, the x-axis shows the distance from the threshold such that discontinuities at 0 represent the effect of being flagged on the relevant outcome.  These plots show inferences for the English language edition of Wikipedia, but since intercepts are the only part of our model that depend on \emph{wiki}, the slopes and the discontinuities caused by algorithmic flagging represent inferences over all our data.    

%To check that our models fit the data and that observed discontinuities are not spurious, these plots also present probabilities of reversion estimated directly from our data within bins around different values of x. 


\subsection{Adoption Check \label{sec:adoption}}

Before discussing our statistical hypothesis tests, we first test that the system is adopted by community moderators on Wikipedia before turning to results for our hypotheses. This is necessary because, prior to our study, little was known of the extent of RCfilters usage beyond anecdotal reports from Wikis, chatrooms, in-person conversations, and mailing lists.  So it is not obvious if RCfilters will have the sort of causal effects on sanctioning that would allow us to test our hypotheses.  

To demonstrate that RCfilters flags are being used by Wikipedia moderators, we look for evidence that flagging has a causal effect on sanctioning over all types of editors using model following equation \ref{eq:rdd_reverted}.  Observing discontinuous increases in the probability of reversion at a given thresholds constitutes evidence that flags in RCFilters have a causal effect on moderation actions on Wikipedia.  Specifically, we test the hypothesis that flagging increases the probability that an edit is reverted.  If Wikipedians are indeed using flags in RCfilters to review potentially damaging edits, then we should find positive values for $\tau_j$, the coefficients for the variable representing the effect of crossing the threshold powering flags and filters in our regression models predicting if edits are reverted.


<<set.adoption.check.vars, echo=FALSE, message=F,results='hide'>>=
tau.1 <- mod.adoption.draws[[tau.1.name]]
tau.2 <- mod.adoption.draws[[tau.2.name]]
tau.3 <- mod.adoption.draws[[tau.3.name]]

tau.overall <- tau.1+tau.2+tau.3
tau.overall.025 <- quantile(tau.overall,0.025)
tau.overall.975 <- quantile(tau.overall,0.975)
vld.proto.below <- proto.reverted(mod.adoption.me.data.df, where='below', threshold.name='verylikelybad')

vld.proto.above <- proto.reverted(mod.adoption.me.data.df, where='above', threshold.name='verylikelybad')

ld.proto.below <- proto.reverted(mod.adoption.me.data.df, where='below', threshold.name='likelybad')

ld.proto.above <- proto.reverted(mod.adoption.me.data.df, where='above', threshold.name='likelybad')

md.proto.below <- proto.reverted(mod.adoption.me.data.df, where='below', threshold.name='maybebad')

md.proto.above <- proto.reverted(mod.adoption.me.data.df, where='above', threshold.name='maybebad')
threshold.delta <- signif(abs(md.proto.below[['d.nearest.threshold']]),2)

suppressMessages({
assertthat::are_equal(threshold.delta, signif(abs(ld.proto.below[['d.nearest.threshold']]),2))
assertthat::are_equal(threshold.delta, signif(abs(vld.proto.below[['d.nearest.threshold']]),2))
assertthat::are_equal(threshold.delta, signif(abs(md.proto.below[['d.nearest.threshold']]),2))
})
@ 

 
As shown in table \ref{tab:adoption.check}, we observe discontinuous increases in the likelihood of reversion at the ``maybe damaging'', and ``likely damaging'' thresholds, but at not at the ``very likely damaging'' threshold.  We find the greatest effect at the ``maybe damaging'' threshold  ($\overline{\tau_1} = \Sexpr{signif(mean(tau.1),2)}$,  $~\mathrm{CI}=\Sexpr{get.CI.str(tau.1,digits=3)}$).
Per our model and Figure \ref{fig:adoption.me}, the likelihood of reversion just below the ``maybe damaging'' threshold is only \Sexpr{proto.reverted.CI.str(md.proto.below, digits.1=2,digits.2=3)} so reverts of unflagged edits are relatively rare.  However, being flagged causes a dramatic increase in the reversion probability to \Sexpr{proto.reverted.CI.str(md.proto.above, digits=2)} for edits just above the threshold.  Flagging an edit at the ``maybe damaging'' level increases the odds it will be reverted by a factor of $\Sexpr{signif(exp(mean(tau.1)),2)}$ ($~\mathrm{CI}=\Sexpr{get.CI.str(tau.1,transform.f=exp)}$). 

Similarly, we find evidence that algorithmic flags at the ``likely damaging'' level causes a very large increase in the chances that an edit will be undone: an increases in odds by a factor of $\Sexpr{signif(exp(mean(tau.2)),2)}$ ($~\mathrm{CI}=\Sexpr{get.CI.str(tau.2,transform.f=exp)}$).  At the ``likely damaging'' threshold the likelihood an edit is reverted jumps from \Sexpr{proto.reverted.CI.str(ld.proto.below, digits.1=2,digits.2=3)} to \Sexpr{proto.reverted.CI.str(ld.proto.above, digits.1=2,digits.2=3)}.
  
However, we do not observe an increase in the likelihood of reversion at the ``very likely damaging'' level ($\overline{\tau_3} = \Sexpr{signif(mean(tau.3),2)}$,  $~\mathrm{CI}=\Sexpr{get.CI.str(tau.3)}$).  While this is surprising given the large increases observed at the other two thresholds, it makes sense given that only the ``likely damaging'' and ``very likely damaging'' levels but not at the ``maybe damaging'' level are enabled by default.  So if the ``likely damaging'' filters or flags are visible then probably so will  ``very likely damaging'' flags and filters. In our experience filtering at the ``very likely damaging'' threshold alone when patrolling recent changes on English Wikipedia was not very useful as few edits cross this threshold and those that do are frequently reverted by bots before a human editor can perform the revert.  Racing the bots to revert obvious damage seems like less useful and rewarding work compared to enabling the ``maybe damaging'' threshold to surface more ambiguous edits requiring human judgment to review.

From this analysis we conclude that the RCfilters system powered by ORES was put in to use by Wikipedians and has a detectable influence on which edits are reverted within 48 hours.   The large discontinuous increases in reversion we observe have important implications for design of sociotechnical systems that use algorithmic predictions to guide human attention which we discuss in \ref{sec:design.implications}.  We did not observe significant effects at the ``very likely damaging'' threshold and therefore we exclude this threshold from our subsequent hypothesis tests.

%   with  below this threshold Reversion below this threshold is rare  are rarely reverted, with a likelihood of

%   Similarly, being flagged at the ``likely damaging'' threshold additionally increases the odds an edit is reverted by a factor of $\Sexpr{signif(exp(mean(tau.2)),2)}$ ($95\%~\mathrm{CI}=\Sexpr{get.CI.str(tau.2,transform.f=exp)}$

% Indeed, by averaging over the sum of the three thresholds, we observe that being flagged generally increases the chances of reversion ($\overline{\sum_j \tau_j} = \Sexpr{signif(mean(tau.overall),2)}$, $95\%~\mathrm{CI} = \Sexpr{get.CI.str(tau.overall)}$).

% We observe the greatest effect for the third threshold, which corresponds to the red flag indicating that an edit is ``very likely damaging''  
% Our estimate indicates that flagging an edit at the ``very likely damaging'' level increases it's odds of being reverted by a factor of ).  An average edit with an ORES score only $\Sexpr{signif(abs(vld.proto.below[['d.nearest.threshold']]),2)}$ below the threshold triggering ``very likely damaging'' flags will be reverted with probability \Sexpr{proto.reverted.CI.str(vld.proto.below, digits=2)}, but right above the threshold the probability increases to \Sexpr{proto.reverted.CI.str(vld.proto.above)}.

% Being flagged at the orange, or ``likely damaging'' level ($\overline{\tau_2} = \Sexpr{mean(tau.2)}$, $95\%~\mathrm{CI}=\Sexpr{get.CI.str(tau.2)}$) causes an increase in the odds of being reverted by a factor of $\Sexpr{signif(exp(mean(tau.2)),2)}$ ($95\%~\mathrm{CI}=\Sexpr{get.CI.str(tau.2,transform.f=exp)}$), corresponding to a jump in reversion probability from \Sexpr{proto.reverted.CI.str(ld.proto.below)} to \Sexpr{proto.reverted.CI.str(ld.proto.above)}.  \TODO{Is it a problem if these CI overlap? They might overlap just because of uncertainty about the kinks, not just the discontinuity.}
% For the yellow, or ``maybe damaging'' level  ($\overline{\tau_3} = \Sexpr{signif(mean(tau.1),2)}$, $95\%~\mathrm{CI}=\Sexpr{get.CI.str(tau.1)}$) we do not estimate a statistically significant difference between the probability of being reverted above and below the cutoff.

\begin{figure}[t]
  \centering
\begin{subfigure}[t]{\textwidth}
  \centering
<<regplot.adoption, fig.height=2, fig.width=0.6*7, out.width="0.6\\textwidth",echo=FALSE, warning=F,message=F, error=F, result='asis'>>=
p <- my_mcmc_intervals(mod.adoption.draws,symbols=FALSE, tex=TRUE) + ggtitle("Effect of algorithmic flagging on sanctioning")

print(p)
@   
\caption{Plots of marginal posteriors for effects of flagging on reversion in the adoption check. \label{fig:adoption.posterior}}
\end{subfigure}
~
\begin{subfigure}[b]{\textwidth}
  \centering
  <<adoption.me.plot, echo=FALSE, fig.height=2.5, out.width='\\textwidth'>>=
  make.rdd.plot(mod.adoption.me.data.df, mod.adoption.bins.df, title="Prob reverted, all editors",digits=2)
  @
  \caption{Marginal effects plot showing model predicted relationship between ores score and reversion around the cutoffs \label{fig:adoption.me}}
\end{subfigure}
\caption{\label{tab:adoption.check} Results from adoption check show strong evidence of RCfilters use on Wikipedia with flagging at the maybe damaging or likely damaging thresholds roughly doubling the likelihood that an edit is reverted. \ref{fig:adoption.posterior} shows our inferred parameters with 95\% credible intervals and \ref{fig:adoption.me} plots the inferred relationship between ORES score and the probability of reversion around the threshold. }
\end{figure}

\subsection{Hypothesis tests}

As discussed above, we consider the effect of flagging over both the ``maybe damaging'' and ``likely damaging'' thresholds.  

We consider that our analysis supports \textbf{H1} in terms of the two kinds of identity-based signals visible in the moderation tools we consider:

In terms of our regression models we consider \textbf{H1} supported if:

$$\sum_j{\tau^{\mathbf{H1}, not\_IP}_j} = \tau^{\mathbf{H1},not\_IP}_{1} + \tau^{\mathbf{H1},not\_IP}_{2}  > \tau^{\textbf{H1}, IP}_{1} + \tau^{\textbf{H1}, IP}_{2} = \sum_j{\tau^{\mathbf{H1}, IP}_j}$$ 

and 

$$\sum_j{\tau^{\textbf{H1}, u.p}_j} = \tau^{\textbf{H1}, u.p.}_{1} + \tau^{\textbf{H1},u.p.}_{2}  > \tau^{\textbf{H1},no~u.p.}_{1} + \tau^{\textbf{H1},no\_u.p.}_{2} = \sum_j{\tau^{\textbf{H1}, no\_u.p.}_j}$$ 

Where we use the abbreviation ``no u.p.'' to indicate coefficients for edits by editors without user pages and ``u.p'' to stand for edits by editors having user pages.

Our analysis supports \textbf{H2a} if the total effect of being flagged on controversial reversion is greater than 0 for edits by IP editors and editors with red link user pages. 

$$\sum_j{\tau^{\mathbf{H2,IP}}_j} = \tau^{\mathbf{H2,IP}}_{1} + \tau^{\mathbf{H2,IP}}_{2} > 0 $$

and 

$$\sum_j{\tau^{\mathbf{H2,no\_u.p.}}_j} = \tau^{\mathbf{H2,no\_u.p.}}_{1} + \tau^{\mathbf{H2,no\_u.p.}}_{2} > 0 $$


\textbf{H3},  is supported if the total effect of being flagged on controversial reversion is less for actions by Non-IP editors is greater than for IP editors and  if this effect is greater for edits by users with a user page than those without, formally:

$$ \sum_j{\tau^{\mathrm{H3},not\_IP}_j} = \tau^{\mathbf{H3},not\_IP}_{1} + \tau^{\mathbf{H3},not\_IP}_{2}  > \tau^{\textbf{H3}, IP}_{1} + \tau^{\textbf{H3}, IP}_{2} = \sum_j{\tau^{\mathrm{H3},IP}_j}$$
and
$$\sum_j{\tau^{\mathrm{H3}, u.p}_j} = \tau^{\textbf{H3}, u.p.}_{1} + \tau^{\textbf{H3},u.p.}_{2}  > \tau^{\textbf{H3},no\_u.p.}_{1} + \tau^{\textbf{H3},no\_u.p.}_{2} = \sum_j{\tau^{\mathrm{H3}, no\_u.p.}_j}$$.

\section{Results}
<<set.h1.vars,echo=FALSE>>=
h1.tau.anon <- apply(matrix(c(h1.tau.1.anon,h1.tau.2.anon),ncol=2,byrow=FALSE),1,sum)
h1.tau.non.anon <- apply(matrix(c(h1.tau.1.non.anon,h1.tau.2.non.anon),ncol=2,byrow=FALSE),1,sum)
h1.tau.1.non.anon.sub.anon <- h1.tau.1.non.anon - h1.tau.1.anon
h1.tau.2.non.anon.sub.anon <- h1.tau.2.non.anon - h1.tau.2.anon
h1.tau.non.anon.sub.anon <- apply(matrix(c(h1.tau.non.anon, -1*h1.tau.anon),ncol=2,byrow=FALSE),1,sum)

h1.tau.user.page <- apply(matrix(c(h1.tau.1.user.page,h1.tau.2.user.page),ncol=2,byrow=FALSE),1,sum)
h1.tau.no.user.page <- apply(matrix(c(h1.tau.1.no.user.page,h1.tau.2.no.user.page),ncol=2,byrow=FALSE),1,sum)
h1.tau.1.user.page.sub.no.user.page <- h1.tau.1.user.page - h1.tau.1.no.user.page
h1.tau.2.user.page.sub.no.user.page <- h1.tau.2.user.page - h1.tau.2.no.user.page
h1.tau.user.page.sub.no.user.page <- apply(matrix(c(h1.tau.user.page, -1*h1.tau.no.user.page),ncol=2,byrow=FALSE),1,sum)

anon.ld.proto.below <- proto.reverted(mod.anon.reverted.me.data.df, where='below', threshold.name='likelybad')
anon.ld.proto.above <- proto.reverted(mod.anon.reverted.me.data.df, where='above', threshold.name='likelybad')
anon.md.proto.below <- proto.reverted(mod.anon.reverted.me.data.df, where='below', threshold.name='maybebad')
anon.md.proto.above <- proto.reverted(mod.anon.reverted.me.data.df, where='above', threshold.name='maybebad')
non.anon.ld.proto.below <- proto.reverted(mod.non.anon.reverted.me.data.df, where='below', threshold.name='likelybad')
non.anon.ld.proto.above <- proto.reverted(mod.non.anon.reverted.me.data.df, where='above', threshold.name='likelybad')
non.anon.md.proto.below <- proto.reverted(mod.non.anon.reverted.me.data.df, where='below', threshold.name='maybebad')
non.anon.md.proto.above <- proto.reverted(mod.non.anon.reverted.me.data.df, where='above', threshold.name='maybebad')

up.ld.proto.below <- proto.reverted(mod.user.page.reverted.me.data.df, where='below', threshold.name='likelybad')
up.ld.proto.above <- proto.reverted(mod.user.page.reverted.me.data.df, where='above', threshold.name='likelybad')
up.md.proto.below <- proto.reverted(mod.user.page.reverted.me.data.df, where='below', threshold.name='maybebad')
up.md.proto.above <- proto.reverted(mod.user.page.reverted.me.data.df, where='above', threshold.name='maybebad')
no.up.ld.proto.below <- proto.reverted(mod.no.user.page.reverted.me.data.df, where='below', threshold.name='likelybad')
no.up.ld.proto.above <- proto.reverted(mod.no.user.page.reverted.me.data.df, where='above', threshold.name='likelybad')
no.up.md.proto.below <- proto.reverted(mod.no.user.page.reverted.me.data.df, where='below', threshold.name='maybebad')
no.up.md.proto.above <- proto.reverted(mod.no.user.page.reverted.me.data.df, where='above', threshold.name='maybebad')

@ 
\begin{figure}[b]
  \centering
<<reverted.me.plot, echo=FALSE, fig.height=5,out.width='\\textwidth'>>=
make.comparison.me.plot(mod.anon.reverted.me.data.df,
                        mod.anon.reverted.bins.df,
                        'IP',
                        mod.non.anon.reverted.me.data.df,
                        mod.non.anon.reverted.bins.df,
                        'Not IP',
                        mod.no.user.page.reverted.me.data.df,
                        mod.no.user.page.reverted.bins.df,                        
                        "No user page",
                        mod.user.page.reverted.me.data.df, 
                        mod.user.page.reverted.bins.df, 
                        "User page",
                        plot.data=FALSE,
                        digits=2,
                        used.thresholds=c('Maybe damaging','Likely damaging')
                        )

@ 
  \caption{Marginal effects plot showing model predicted relationship between ores score and reversion around the cutoffs for registered and IP editors.\label{fig:h1.me}}
\end{figure}

\subsection{\textbf{H1}: Effect of flagging on sanctioning}

We turn now to our main results for our hypotheses, beginning with \textbf {H1:} that algorithmic flags will be relatively less influential in regulating over-profiled editors.  Figure \ref{fig:h1.regplot} shows point estimates and credible intervals for the causal effects of flagging on reversion and Figure \ref{fig:h1.me} shows marginal effects plots representing the modeled relationship between ORES score and reversion in the neighborhood of the thresholds for English Wikipedia. 

\begin{figure}[t]
\centering
% I have so much data and these marginal posteriors are so normal that there isn't much point in showing the intervals
% \begin{subfigure}[t]{0.75\textwidth}
% \centering
<<regplot.H1.anon,fig.asp=0.6,fig.width=5.5,out.width="75%",echo=FALSE,warning=F, message=F>>=

h1.mcmc.data <- data.table(
                           h1.tau.1.non.anon.sub.anon,
                           h1.tau.2.non.anon.sub.anon,
                           h1.tau.non.anon.sub.anon,
                           h1.tau.1.user.page.sub.no.user.page,                           
                           h1.tau.2.user.page.sub.no.user.page,
                           h1.tau.user.page.sub.no.user.page)

p <- big_reg_plot2(h1.mcmc.data, ip.facet.title="H1a", up.facet.title="H1b",ncol=1,n.legend.col=1) + ggtitle("Effects of algorithmic flagging on sanctioning")

print(p)
@       
\caption{Results for \textbf{H1} that flagging has a greater causal effect on reversion for editors that are not over-profiled than for editors that are.  Our analysis of IP editors shows that the effects of flagging for Non IP editors are greater than for IP editors at both the ``maybe damaging'' and the ``likely damaging'' thresholds and the difference over both thresholds is also positive.  On the other hand, flagging increases sanctioning, but not more for editors with user pages than for editors without. 
We place points over the X-axis corresponding to posterior means, which are the most likely parameter value. Lines show 95\% credible intervals for each parameter, indicating uncertainty in our inferences.  For each threshold, we also model parameters for each type of editor and then the difference between groups of editors below.  We also show overall effects obtained by summing posteriors over thresholds. We draw conclusions when the credible intervals do not contain 0, which is the Bayesian analog to testing a hypothesis with $p<0.05$. \label{fig:h1.regplot}}
\end{figure}                                                                          

We proposed that Wikipedia moderators reviewing edits will be likely to scrutinize edits by over-profiled users whether they are flagged or not, but that they be likely to scrutinize edits by under-profiled editors if they are flagged.
We find strong support for \textbf{H1a} which uses whether an edit is attributed to a registered editor or to an IP address. Per Figure \ref{fig:h1.me}, which shows the marginal effects plots for \textbf{H1}, edits to English with an ORES score just below the ``maybe damaging'' threshold jump in reversion probability from  \Sexpr{proto.reverted.CI.str(anon.md.proto.below, digits.1=2,digits.2=3)} to \Sexpr{proto.reverted.CI.str(anon.md.proto.above, digits.1=2,digits.2=3)} for IP editors. This corresponds to a $\Sexpr{signif(exp(mean(h1.tau.1.anon)),2)}$-factor $(~\mathrm{CI}=\Sexpr{get.CI.str(h1.tau.1.anon,transform.f=exp,format.percent=F)}$ increase in the odds of reversion.
But for non-IP editors we see an even bigger jump: a $\Sexpr{signif(exp(mean(h1.tau.1.non.anon)),2)}$-factor $(~\mathrm{CI}=\Sexpr{get.CI.str(h1.tau.1.non.anon,transform.f=exp,format.percent=F)}$ increase in reversion odds. This means receiving an ORES score just above the threshold increases the probability of reversion from \Sexpr{proto.reverted.CI.str(non.anon.md.proto.below, digits.1=2,digits.2=3)} to \Sexpr{proto.reverted.CI.str(non.anon.md.proto.above, digits.1=2,digits.2=3)} compared to an edit receiving a score just below the threshold.

Similarly, at the ``likely damaging'' threshold we find a $\Sexpr{signif(exp(mean(h1.tau.2.non.anon)),2)}$-factor $(~\mathrm{CI}=\Sexpr{get.CI.str(h1.tau.2.non.anon,transform.f=exp,format.percent=F)}$ increase in the odds of reversion for Non-IP edits which is greater than the $\Sexpr{signif(exp(mean(h1.tau.2.anon)),2)}$-factor $(~\mathrm{CI}=\Sexpr{get.CI.str(h1.tau.2.anon,transform.f=exp,format.percent=F)}$ increase in odds for IP edits. This corresponds to an increase in the probability that an edit to English Wikipedia will be reverted from \Sexpr{proto.reverted.CI.str(non.anon.md.proto.below, digits.1=2,digits.2=3)} to \Sexpr{proto.reverted.CI.str(non.anon.md.proto.above, digits.1=2,digits.2=3)} for Non-IP editors and an an increase from \Sexpr{proto.reverted.CI.str(anon.md.proto.below, digits.1=2,digits.2=3)} to \Sexpr{proto.reverted.CI.str(anon.md.proto.above, digits.1=2,digits.2=3)} for IP editors. 

Since we detect that edits by Non-IP editors are more sensitive to flagging at both thresholds, we also observe a greater overall effect for Non-IP editors $(\tau^{\mathrm{Non IP}}=\Sexpr{signif(mean(h1.tau.non.anon),2)}$ $(~\mathrm{CI}=\Sexpr{get.CI.str(h1.tau.non.anon,format.percent=F)}$) than for IP editors $(\tau^{\mathrm{IP}}=\Sexpr{signif(mean(h1.tau.anon),2)}$ $(~\mathrm{CI}=\Sexpr{get.CI.str(h1.tau.anon,format.percent=F)}$. Therefore we conclude that our analysis of edits by anonymous users supports \textbf{H1}.

On the other hand, our analysis of edits by users wtih red link user pages does not suggest that flagging has a greater effect for editors with user pages than those without.  While we do observe increases in the probability that an edit will be reverted when the edit's ORES scores cross the ``maybe damaging'' and ``likely damaging'' thresholds these changes are pretty similar between editors with and without user pages. For the ``maybe damaging'' threshold we find a jump in reversion probability from  \Sexpr{proto.reverted.CI.str(no.up.md.proto.below, digits.1=2,digits.2=3)} to \Sexpr{proto.reverted.CI.str(no.up.md.proto.above, digits.1=2,digits.2=3)} for editors without user pages and a jump from \Sexpr{proto.reverted.CI.str(up.md.proto.below, digits.1=2,digits.2=3)} to \Sexpr{proto.reverted.CI.str(up.md.proto.above, digits.1=2,digits.2=3)} for editors that do have user pages. Contrary to our proposed hypothesis, at the ``maybe damaging'' level the odds ratio of $\Sexpr{signif(exp(mean(h1.tau.1.no.user.page)),2)}$ $(~\mathrm{CI}=\Sexpr{get.CI.str(h1.tau.1.no.user.page,transform.f=exp,format.percent=F)})$ for editors that have created user pages is greater the odds ratio of $\Sexpr{signif(exp(mean(h1.tau.1.user.page)),2)}$ $(~\mathrm{CI}=\Sexpr{get.CI.str(h1.tau.1.user.page,transform.f=exp,format.percent=F)})$ for those that do not. 

We do not detect a statistical difference between editors with and without user pages at the ``likely damaging'' threshold.  The chances that an edit by an editor without a user page increase in probability from  \Sexpr{proto.reverted.CI.str(no.up.ld.proto.below, digits.1=2,digits.2=3)} to \Sexpr{proto.reverted.CI.str(no.up.ld.proto.above, digits.1=2,digits.2=3)} and for editors that do have user pages we find an increase from \Sexpr{proto.reverted.CI.str(up.ld.proto.below, digits.1=2,digits.2=3)} to \Sexpr{proto.reverted.CI.str(up.ld.proto.above, digits.1=2,digits.2=3)}.  These changes correspond to odds ratios of $\Sexpr{signif(exp(mean(h1.tau.2.no.user.page)),2)}$ $(~\mathrm{CI}=\Sexpr{get.CI.str(h1.tau.2.no.user.page,transform.f=exp,format.percent=F)}$) and $\Sexpr{signif(exp(mean(h1.tau.2.user.page)),2)}$ $(~\mathrm{CI}=\Sexpr{get.CI.str(h1.tau.2.user.page,transform.f=exp,format.percent=F)}$) respectively, which we cannot statistically distinguish.  Therefore we do not know from our models if there is a difference in flagging effect between editors with and without user pages at the ``likely damaging'' level.  This uncertainty is driven by the scarcity of edits around the ``likely damaging'' threshold for registered editors.

Similarly, we observe no overall statistical difference between edits by editors that have user pages $(\tau^{\mathrm{No~u.p.}}=\Sexpr{signif(mean(h1.tau.user.page),2)}$ $(~\mathrm{CI}=\Sexpr{get.CI.str(h1.tau.user.page,format.percent=F)})$ and edits by those that do not $(\tau^{\mathrm{IP}}=\Sexpr{signif(exp(mean(h1.tau.no.user.page)),2)}$ $(~\mathrm{CI}=\Sexpr{get.CI.str(h1.tau.no.user.page,transform.f=exp,format.percent=F)})$.  Therefore we conclude that over both our measures of over-profiling, our analysis provides only mixed support for \textbf{H1}.


\subsection{\textbf{H2}: Effect of flagging on controversial sanctioning}

% We tweak H2 so it's now about IP editors and editors without user pages
<<set.h2.vars,echo=FALSE>>=
h2.tau.1.anon <- mod.anon.controversial.draws[[tau.1.name]]
h2.tau.2.anon <- mod.anon.controversial.draws[[tau.2.name]]
h2.tau.3.anon <- mod.anon.controversial.draws[[tau.3.name]]

h2.tau.anon <- h2.tau.1.anon + h2.tau.2.anon

h2.anon.ld.proto.below <- proto.reverted(mod.anon.controversial.me.data.df, where='below', threshold.name='likelybad')
h2.anon.ld.proto.above <- proto.reverted(mod.anon.controversial.me.data.df, where='above', threshold.name='likelybad')
h2.anon.md.proto.below <- proto.reverted(mod.anon.controversial.me.data.df, where='below', threshold.name='maybebad')
h2.anon.md.proto.above <- proto.reverted(mod.anon.controversial.me.data.df, where='above', threshold.name='maybebad')

h2.tau.1.no.user.page <- mod.no.user.page.controversial.draws[[tau.1.name]]
h2.tau.2.no.user.page <- mod.no.user.page.controversial.draws[[tau.2.name]]
h2.tau.3.no.user.page <- mod.no.user.page.controversial.draws[[tau.3.name]]

h2.tau.no.user.page <- h2.tau.1.no.user.page + h2.tau.2.no.user.page

h2.no.user.page.ld.proto.below <- proto.reverted(mod.no.user.page.controversial.me.data.df, where='below', threshold.name='likelybad')
h2.no.user.page.ld.proto.above <- proto.reverted(mod.no.user.page.controversial.me.data.df, where='above', threshold.name='likelybad')
h2.no.user.page.md.proto.below <- proto.reverted(mod.no.user.page.controversial.me.data.df, where='below', threshold.name='maybebad')
h2.no.user.page.md.proto.above <- proto.reverted(mod.no.user.page.controversial.me.data.df, where='above', threshold.name='maybebad')

@ 

We now turn to our results for \textbf{H2a} and \textbf{H2b}.  We proposed two alternative hypotheses. First, reasoning that algorithmic flags might nudge moderators to issue sanctions, we proposed that edits being flagged will increase the chances of a controversial revert (meaning that a revert to an edit was undone by a third party) for over-profiled editors.  Contrary to this hypothesis, but consistent with our alternative hypothesis that better information will help moderators avoid violating second-order norms. We found that having an ORES score crossing the ``maybe damaging'' or ``likely damaging'' threshold decreases the chances that a revert is controversial for IP editors. We found no evidence for either \textbf{H2a} or \textbf{H2b} and we find no effect for editors without user pages.  Figure \ref{fig:h2.regplot} summarizes our parameter estimates and Figure \ref{fig:h2.me} is a marginal effects plots showing our modeled relationship between ORES score and the probability of a controversial sanction in the neighborhood of the thresholds.


\begin{figure}[t]
  \centering
\begin{subfigure}[t]{0.75\textwidth}
  \centering
<<regplot.controversial.anon, fig.asp=0.5, fig.width=4.5, out.width='75%', echo=FALSE, warning=F,message=F, error=F, result='asis'>>=
  p <- my_mcmc_intervals(mod.anon.controversial.draws,pars=c(tau.1.name,tau.2.name), symbols=FALSE, tex=TRUE,overall=TRUE) + ggtitle("Controversial sanctioning for IP editors")
  print(p)
@   
\caption{Plots of marginal posteriors for effects of flagging on whether sanctions are controversial.\label{fig:h2.regplot}}
\end{subfigure}
~
\begin{subfigure}[b]{\textwidth}
\centering  
<<me.plot.H2.anon, echo=FALSE, fig.height=2, out.width='\\textwidth', results='asis'>>=
make.rdd.plot(mod.anon.controversial.me.data.df, mod.all.controversial.bins.df, used.thresholds=c("maybebad","likelybad"), title="Prob. controversial, for reverts to anonymous edits",digits=3)
@ 
\caption[H2. me plot]{Marginal effects plots for models predicting whether a revert is controversial, for anonymous editors. \label{fig:h2.me}}
\end{subfigure}
\end{figure}

For reverts to IP edits receiving ORES scores just below the ``maybe damaging'' level, the probability that the revert is controversial is \Sexpr{proto.reverted.CI.str(h2.anon.md.proto.below)}.  But for reverts scoring just across the threshold this drops slightly to \Sexpr{proto.reverted.CI.str(h2.anon.md.proto.above)}, a decrease in odds by a factor of $\Sexpr{signif(exp(mean(h2.tau.1.anon)),2)}$ ($~\mathrm{CI}=\Sexpr{get.CI.str(h2.tau.1.anon,transform.f=exp)}$).  Similarly, being flagged at the ``likely damaging'' level decreases the odds that a revert is controversial by a factor of $\Sexpr{signif(exp(mean(h2.tau.2.anon)),2)}$ ($~\mathrm{CI}=\Sexpr{get.CI.str(h2.tau.2.anon,transform.f=exp)}$).  Just before the ``likely damaging'' threshold, a revert has a \Sexpr{proto.reverted.CI.str(h2.anon.ld.proto.below)} probability of being undone by a third party, but flagging decreases this to \Sexpr{proto.reverted.CI.str(h2.anon.ld.proto.above)}.  Algorithmic flagging has a negative effect over both thresholds $(\tau^{\mathrm{H2}}=\Sexpr{signif(mean(h2.tau.anon),2)}$ $(~\mathrm{CI}=\Sexpr{get.CI.str(h2.tau.anon,format.percent=F)}$).

\begin{figure}[t]
  \centering
\begin{subfigure}[t]{0.75\textwidth}
  \centering
<<regplot.controversial.no.user.page, fig.asp=0.5, fig.width=4.5, out.width='75%', echo=FALSE, warning=F,message=F, error=F, result='asis'>>=
  p <- my_mcmc_intervals(mod.no.user.page.controversial.draws,pars=c(tau.1.name,tau.2.name), symbols=FALSE, tex=TRUE,overall=TRUE) + ggtitle("Controversial sanctioning for edits with redlink user pages")
  print(p)
@   
\caption{Plots of marginal posteriors for effects of flagging on whether sanctions are controversial.\label{fig:h2.regplot}}
\end{subfigure}
~
\begin{subfigure}[b]{\textwidth}
\centering  
<<me.plot.H2.no.user.page, echo=FALSE, fig.height=2, out.width='\\textwidth', results='asis'>>=
make.rdd.plot(mod.no.user.page.controversial.me.data.df, mod.all.controversial.bins.df, used.thresholds=c("maybebad","likelybad"), title="Prob. controversial, for reverts to edits with redlink user pages",digits=3)
@ 
\caption[H2. me plot]{Marginal effects plots for models predicting whether a revert is controversial, editors without user pages. \label{fig:h2.me}}
\end{subfigure}
\end{figure}

We find no detectable change in the chances that a revert is controversial when the reverted edit has a red link user page. When an ORES score to an edit by a user without a user page is just below the ``maybe damaging'' level, the probability that a revert is controversial is \Sexpr{proto.reverted.CI.str(h2.no.user.page.md.proto.below)} and  \Sexpr{proto.reverted.CI.str(h2.no.user.page.md.proto.above)}, a change in odds of factor of $\Sexpr{signif(exp(mean(h2.tau.1.no.user.page)),2)}$ ($~\mathrm{CI}=\Sexpr{get.CI.str(h2.tau.1.no.user.page,transform.f=exp)}$).  Similarly, being flagged at the ``likely damaging'' level changes the odds that a revert is controversial by a factor of $\Sexpr{signif(exp(mean(h2.tau.2.no.user.page)),2)}$ ($~\mathrm{CI}=\Sexpr{get.CI.str(h2.tau.2.no.user.page,transform.f=exp)}$).  Just before the ``likely damaging'' threshold a revert has a \Sexpr{proto.reverted.CI.str(h2.no.user.page.ld.proto.below)} probability of being undone by a third party, and for flagged edits this is \Sexpr{proto.reverted.CI.str(h2.no.user.page.ld.proto.below)}.  We do not detect a statistically significant effect of algorithmic flagging over both thresholds $(\tau^{\mathrm{H2}}=\Sexpr{signif(mean(h2.tau.no.user.page),2)}$ $(~\mathrm{CI}=\Sexpr{get.CI.str(h2.tau.no.user.page,format.percent=F)}$).

\subsection{\textbf{H3}: Identity-based signals and effects of flagging on controversial sanctioning }

% Should we comment on power here?
We proposed in \textbf{H3}, following our interpretation of dual process models of decision that the effect of flagging on the likelihood of controversial sanction would be less positive for over-profiled users than for others.  Our analysis does not support this hypothesis.  As shown in \ref{fig:h3.reg.plot}, we find weak evidence that the effect for non-over profiled editors is greater at the ``maybe damaging'' threshold, but the opposite is true at the ``likely damaging'' threshold. Neither these estimates nor their sum are statistically significant at the 95\% level.  


\begin{figure}[h]
\centering
% I have so much data and these marginal posteriors are so normal that there isn't much point in showing the intervals
% \begin{subfigure}[t]{0.49\textwidth}
% \centering
<<regplot.H3.anon,fig.asp=0.6,fig.width=5.5,out.width="75%",echo=FALSE,warning=F, message=F,results='asis'>>=

h3.tau.anon <- apply(matrix(c(h3.tau.1.anon,h3.tau.2.anon),ncol=3,byrow=FALSE),1,sum)
h3.tau.non.anon <- apply(matrix(c(h3.tau.1.non.anon,h3.tau.2.non.anon),ncol=3,byrow=FALSE),1,sum)
h3.tau.non.anon.sub.anon <- apply(matrix(c(h3.tau.non.anon, -1*h3.tau.anon),ncol=2,byrow=FALSE),1,sum)

h3.tau.anon <- apply(matrix(c(h3.tau.1.anon,h3.tau.2.anon),ncol=3,byrow=FALSE),1,sum)
h3.tau.non.anon <- apply(matrix(c(h3.tau.1.non.anon,h3.tau.2.non.anon),ncol=3,byrow=FALSE),1,sum)
h3.tau.non.anon.sub.anon <- apply(matrix(c(h3.tau.non.anon, -1*h3.tau.anon),ncol=2,byrow=FALSE),1,sum)

h3.tau.user.page <- apply(matrix(c(h3.tau.1.user.page,h3.tau.2.user.page),ncol=3,byrow=FALSE),1,sum)
h3.tau.no.user.page <- apply(matrix(c(h3.tau.1.no.user.page,h3.tau.2.no.user.page),ncol=3,byrow=FALSE),1,sum)
h3.tau.user.page.sub.no.user.page <- apply(matrix(c(h3.tau.user.page, -1*h3.tau.no.user.page),ncol=2,byrow=FALSE),1,sum)


h3.mcmc.data <- data.table(
                           h3.tau.1.non.anon.sub.anon,
                           h3.tau.2.non.anon.sub.anon,
                           h3.tau.non.anon.sub.anon,
                           h3.tau.1.user.page.sub.no.user.page,                           
                           h3.tau.2.user.page.sub.no.user.page,
                           h3.tau.user.page.sub.no.user.page)

p <- big_reg_plot2(h3.mcmc.data,ip.facet.title='H3a',up.facet.title='H3b',ncol=1,n.legend.col=1) + ggtitle("Effects of flagging on controversial sanctioning")

print(p)
@       
\caption{Results for \textbf{H3} that flagging has a greater causal effect on controversial sanctioning for non-anonymous editors than for anonymous editors. Points show posterior medians and lines show 95\% credible intervals for each parameter. Dashed lines represent inferences based on aggregations of model parameters.\label{fig:h3.reg.plot}}
\end{figure}                                                                                      



% \begin{figure}[b]
%   \centering
% <<controversial.me.comparison.plot, echo=FALSE, fig.height=5,  out.width='\\textwidth'>>=
% make.comparison.me.plot(mod.anon.controversial.me.data.df,
%                         mod.anon.controversial.bins.df,
%                         'IP',
%                         mod.non.anon.controversial.me.data.df,
%                         mod.non.anon.controversial.bins.df,
%                         'Not IP',
%                         mod.no.user.page.controversial.me.data.df,
%                         mod.no.user.page.controversial.bins.df,  
%                         "No user page",
%                         mod.user.page.controversial.me.data.df, 
%                         mod.user.page.controversial.bins.df, 
%                         "User page",
%                         digits=3,
%                         used.thresholds=c("Maybe damaging", "Likely damaging")
%                         )


% @ 
  
%   \caption{Marginal effects plot for models predicting whether a revert is controversial}
%   \label{fig:me.controversial.comp}
% \end{figure}

\section{Discussion}
\subsection{Flagging, over-profiling, and sanctioning}

\textbf{H1} proposed that flagging will have a greater effect for editors that are not over-profiled than for over-profiled participants because moderators pay attention to over-profiled participants even when they are not flagged, but actions by participants that are not over-profiled will attract moderator attention when they are flagged. Our analysis of edits by IP editors supports this hypothesis, but our analysis of edits with red link user pages does not.

Of the two identity-based signals we consider, IP-editing attracts far more attention from academics and community members in discussions of Wikipedia vandalism.  Sources such as Broughton \cite{broughton_wikipedia_2008} suggesting that red link user pages are signs useful to vandalism patrollers are older and may not reflect widespread practices of contemporary Wikipedians. 

We interpret our findings for \textbf{H1} as evidence that algorithmic flags can improve treatment of over-profiled users by substituting for identity-based signals in routing moderator attention.  Yet, our results for red-link user pages suggest unanticipated scope conditions for what sorts of identity-based signals are used for profiling or when algorithmic flags might reinforce identity-based signals rather than substituting for them. 

\subsection{Flagging and controversial sanctioning}

We test two competing hypotheses about how algorithmic flags would effect fairness of norm enforcement reflected by sanctions against second-order-norm violations.  While we interpreted dual-process theories as suggestive that algorithmic flags might influence Wikipedia moderators as salient signals and thereby increase second-order norm violations, we instead find that flagging decreases the incidence of controversial sanctions overall. We take this as evidence supporting applications of quantitative rationalization to online community governance. Additional information from algorithmic flags appear to help moderators more accurately issue sanctions.  This is additional evidence that the RCfilters system helped moderators find and sanction damaging edits.  An increase in sanctions compliant with second-order norms will naturally lead to an decrease in the proportion of sanctions that do not violate second-order norms.

\subsection{Over-profiling and controversial sanctioning}

In a second test applying dual-process theories to understand potential interaction between algorithmic flags and identity-based signals we proposed that algorithmic flags would have a more positive effect on controversial sanctioning for users that do not display signals associated with over-profiling compared to users like IP editors and editors with red-link user pages.  We did not find evidence supporting this hypothesis for either type of editor characteristic.  This suggests that our interpretation of dual process theories is of limited use for understanding how algorithmic flags and identity based signals interact. 

\subsection{Design implications \label{sec:design.implications}}

% Halfak says to cite de laat in this paragraph. 
System designers should move beyond considering only the biases inherent in the algorithm and also consider the systemic and down stream effects of flagging their community member's actions.  While quality control is an important function in open production communities like Wikipedia, supporting newcomers and encouraging contribution is also essential.  Past work has shown that, in general, increased quality control efforts correspond to a decrease in newcomer engagement and the hypothesized mechanism is an increased scrutiny directed towards newcomers\cite{teblunthuis_revisiting_2018}.  Similarly, while blocking anonymous edits lead to a decrease in reverted edits on Wikia wikis, it also lead to a decrease in positive contributions.  While it may be intuitive to think about the edits that get sanctioned as obviously bad vandalism, many of the edits flagged by the "maybe bad" threshold are authored by well-meaning newcomers and anonymous editors\cite{halfaker_rise_2013}.  There's a potentially high cost to sanctioning these well meaning but borderline contributions. System designers should track changes in the rate of sanctions to sensitive groups of community members in order to assure that such well meaning contributors aren't being driven away by over-sanctioning. 

Systems designers should also consider the fairness of threshold-based flagging for interfaces for content quality analysis.  While thresholds allowed us explore the effects of flagging on sanctioning behavior in this study, they did so because they arbitrarily flagged edits right above the thresholds but not those right below.    The step-wise threshold approach applied by RCFilters brings disproportionately more attention to contributions that register just above the threshold and disproportionately less attention to contributions that register just below the threshold.  Our results make it clear that this drives sanctioning behavior to also be disproportionate unflagged but damaging edits are concentrated right below the threshold and flagged non-damaging edits right above it

A more effective use of quality control support models might apply scrutinizing attention to contributions in proportion to the likelihood that the contributions are in need of a sanction.  For example, Huggle, an alternative counter-vandalism tool for Wikipedia\footnote{See discussion in \cite{halfaker_snuggle:_2014}}, shows users a list of edits sorted by the likelihood that they are damaging.  Reviewers are encouraged to review the highest likelihood edits first and only move onto lower likelihood edits once those reviews are complete.  Such a user-experience has the potential to increase efficiency and fairness by better concentrating moderator attention where it can have the greatest benefits and away from individuals who be subject false positive classifications at discrete thresholds.

\subsection{Limitations}
Readers should consider the following limitations of our analysis:
\subsubsection{Causality:} 
We believe that our regression discontinuity design provides relatively strong evidence of causal relationships between algorithmic flagging and sanctioning.  That said, causal interpretation of our estimates depends on untestable assumptions that we can model the secular relationship between ores scores and our outcomes, that editors do not manipulate their edits around thresholds and that other discontinuities triggered by ORES scores do not confound our analysis.  We argue that these assumptions are believable, but they are strong compared to those of a randomized controlled experiment.  Our analysis provides strengths that an experiment would not including ecological validity and non-intervention.  Furthermore, the limitations stemming from regression discontinuity assumptions are relatively minor compared to those facing our comparison of edits by editors having different visible identity-characteristics.

While our study design affords causal inference for effects of flagging, we it does not provide causal evidence for the interactions between identity-based signals and flagging.   Our theory proposes that the presence of identity-based signals cause moderators to make some sanctioning actions instead of others, but our evidence only allows us to describe differences between IP editors, editors with no user page, and other editors.  It does not show that the presence or absence of identity-based signals in moderation tools explains the observed differences.  A promising direction for future work is to conduct a randomized controlled trial that both varies identity-based signals and algorithmic flags in online community moderation.   

\subsubsection{Generalizability:}

While our theories of interactions between identity-based signals and algorithmic flags is general, we study a single system, deployed on multiple Wikipedia projects.  These projects are not representative of online communities in general or even of Wikipedia language editions.  We analyze the broadest possible sample in an effort to improve generalizability beyond English Wikipedia alone.  Wikipedia language communities adopted ORES according to their perceived needs and their ability to label training data.  We do not claim that our findings generalize beyond the specific pool of communities that we study. 

\subsubsection{Alternative Explanations:}

Finally, our analysis cannot rule out plausible alternative explanations for our findings related to systematic differences between edits with or without identity-based signals.  For example, if damaging edits by IP editors, or editors without user pages are more difficult for the ORES model to detect, that could drive our findings for \textbf{H1} as sanctioning would be less driven by algorithmic flagging for such editors. Such a scenario seems to suggest that over-profiled editors are sophisticated relative to other editors, which is doubtful. Yet other unknown systematic differences between editors cannot be ruled out.   

% maybe newcomers with user pages are more suspect?



\subsection{Conclusion}

As algorithmic flagging is increasingly adopted in online community governance, it is important to study the consequences of these systems for collaboration for fairness to users who already face barriers to participation.  
Critics of machine learning trace how algorithms can encode discriminatory patterns in human behavior. In response, some machine learning researchers propose adaptations to traditional modeling building practice to limit and control algorithmic bias.  But when tools for predictive governance are introduced into a sociotechnical system, they may become incorporated in ways that may not be anticipated from an analysis of the tool alone.  We consider how identity-based signals of contributor quality may be used along side algorithmic flags in routing moderator attention.  Will algorithmic flags support increased fairness or increase over-profiling?  

We investigate how the RCfilters/ORES was put into use by Wikipedians by analyzing the effects of being flagged by the algorithm using a regression discontinuity design. The regression discontinuity analysis demonstrates that the tool was put into use, and affords testing hypotheses about how flagging shapes the fairness of moderator actions to over-profiled users.  

Flagged edits are on a more level playing field between anonymous and non-anonymous editors in that flagging has a greater effect on sanctioning for non-anonymous edits than for anonymous edits.  In the sense that flagging decreases controversial sanctions of anonymous editors, it can improve fairness.   

When we consider red user-page links, an identity-based signal associated with newcomer status and thought to be useful for Wikipedia moderators routing their attention, we did not find clear evidence.  This suggests that future work look to what kinds of identity-based signals are used in practice and to explain how different kinds of information may be used alongside algorithmic flags.




% critiques of algorithmic fairness or discrimination
% machine learning practitioners pursue methods for building algorithms   Field studies of such systems that are deployed and used in the wild, as we do in this study of the RCfilters/ORES system on Wikipedia.  Our research design based on regression discontinuity causal  

% Based on the logic of ``over-profiling,'' statistical discrimination, and salient signals, we proposed that Wikipedia moderators would rely more on algorithmic flags to guide them to discover damaging edits by users that are not already over-profiled.  Instead we found little difference in the effect of flagging on the likelihood of reversion between registered and IP editors.

% What explains this surprising result?  One possibility is that Wikipedians do not actually ``over-profile'' IP editors at all. If Wikipedians already scrutinize edits by IP editors and by registered users equally, then we would not expect to find a difference in how flagging effects reversion in ways associated with editor type.  This explanation is dubious since Wikipedians are thought to be highly suspicious of anonymous editors.

% However, if the availability of an algorithmic flag obviates the need for statistical discrimination against ``over-profiled'' editors, then Wikipedians may use only signals from the algorithmic flagging system instead of using algorithmic flags alongside identity-based characteristics.  This explanation is encouraging for it suggests that introducing algorithmic predictions into governance systems can reduce statistical discrimination.


\bibliographystyle{ACM-Reference-Format}
\bibliography{OresAudit.bib}

% \setcounter{biburlnumpenalty}{9001}
% \printbibliography[title = {References}, heading=secbib]

\end{document}

% LOCAL_WORDS: decile
